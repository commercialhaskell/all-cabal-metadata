homepage: https://streamly.composewell.com
changelog-type: markdown
hash: 845646ff2eb8b74e99204367e89a42bacc11cb162cee7c67c2905d8367a0a440
test-bench-deps: {}
maintainer: streamly@composewell.com
synopsis: Dataflow programming and declarative concurrency
changelog: "# Changelog\n\n<!-- See rendered changelog at https://streamly.composewell.com
  -->\n\n## 0.8.1 (Nov 2021)\n\nSee docs/API-changelog.txt for new APIs introduced.\n\n###
  Bug Fixes\n\n* Several bug fixes in the Array module:\n    * Fix writeN fold eating
  away one element when applied multiple times\n      [#1258](https://github.com/composewell/streamly/issues/1258).\n
  \   * Fix potentially writing beyond allocated memory when shrinking. Likely\n      cause
  of [#944](https://github.com/composewell/streamly/issues/944).\n    * Fix potentially
  writing beyond allocated memory when writing the last\n      element. Likely cause
  of \n      [#944](https://github.com/composewell/streamly/issues/944).\n    * Fix
  missing pointer touch could potentially cause use of freed memory.\n    * Fix unnecessary
  additional allocation due to a bug\n* Fix a bug in classifySessionsBy, see \n  [PR
  #1311](https://github.com/composewell/streamly/pull/1311). The bug\n  could cause
  premature ejection of a session when input events with the\n  same key are split
  into multiple sessions.\n\n### Notable Internal API Changes\n\n* `tapAsync` from
  `Streamly.Internal.Data.Stream.Parallel` has been moved to\n  `Streamly.Internal.Data.Stream.IsStream`
  and renamed to `tapAsyncK`.\n* `Fold2` has now been renamed to `Refold` and the
  corresponding `Fold2`\n  combinators have been either renamed or removed.\n\n##
  0.8.0 (Jun 2021)\n\nSee [API Changelog](docs/API-changelog.txt) for a complete list
  of signature\nchanges and new APIs introduced.\n\n### Breaking changes\n\n* `Streamly.Prelude`\n
  \   * `fold`: this function may now terminate early without consuming\n      the
  entire stream. For example, `fold Fold.head stream` would\n      now terminate immediately
  after consuming the head element from\n      `stream`. This may result in change
  of behavior in existing programs\n      if the program relies on the evaluation
  of the full stream.\n* `Streamly.Data.Unicode.Stream`\n    * The following APIs
  no longer throw errors on invalid input, use new\n      APIs suffixed with a prime
  for strict behavior:\n        * decodeUtf8\n        * encodeLatin1\n        * encodeUtf8\n*
  `Streamly.Data.Fold`:\n    * Several instances have been moved to the `Streamly.Data.Fold.Tee`\n
  \     module, please use the `Tee` type to adapt to the changes.\n\n### Bug Fixes\n\n*
  Concurrent Streams: The monadic state for the stream is now propagated across\n
  \ threads. Please refer to\n  [#369](https://github.com/composewell/streamly/issues/369)
  for more info.\n* `Streamly.Prelude`:\n    * `bracket`, `handle`, and `finally`
  now also work correctly on streams\n      that aren't fully drained. Also, the resource
  acquisition and release is\n      atomic with respect to async exceptions.\n    *
  `iterate`, `iterateM` now consume O(1) space instead of O(n).\n    * `fromFoldableM`
  is fixed to be concurrent.\n* `Streamly.Network.Inet.TCP`: `accept` and `connect`
  APIs now close the socket\n  if an exception is thrown.\n* `Streamly.Network.Socket`:
  `accept` now closes the socket if an exception is\n  thrown.\n\n### Enhancements\n\n*
  See [API Changelog](docs/API-changelog.txt) for a complete list of new\n  modules
  and APIs introduced.\n* The Fold type is now more powerful, the new termination
  behavior allows\n  to express basic parsing of streams using folds.\n* Many new
  Fold and Unfold APIs are added.\n* A new module for console IO APIs is added.\n*
  Experimental modules for the following are added:\n    * Parsing\n    * Deserialization\n
  \   * File system event handling (fsnotify/inotify)\n    * Folds for streams of
  arrays\n* Experimental `use-c-malloc` build flag to use the c library `malloc` for\n
  \ array allocations. This could be useful to avoid pinned memory fragmentation.\n\n\n###
  Notable Internal/Pre-release API Changes\n\nBreaking changes:\n\n* The `Fold` type
  has changed to accommodate terminating folds.\n* Rename: `Streamly.Internal.Prelude`
  => `Streamly.Internal.Data.Stream.IsStream`\n* Several other internal modules have
  been renamed and re-factored.\n\nBug fixes:\n\n* A bug was fixed in the conversion
  of `MicroSecond64` and `MilliSecond64`\n  (commit e5119626)\n* Bug fix: `classifySessionsBy`
  now flushes sessions at the end and terminates.\n\n### Miscellaneous\n\n* Drop support
  for GHC 7.10.3.\n* The examples in this package are moved to a new github repo\n
  \ [streamly-examples](https://github.com/composewell/streamly-examples)\n\n## 0.7.3
  (February 2021)\n\n### Build Issues\n\n* Fix build issues with primitive package
  version >= 0.7.1.\n* Fix build issues on armv7.\n\n## 0.7.2 (April 2020)\n\n###
  Bug Fixes\n\n* Fix a bug in the `Applicative` and `Functor` instances of the `Fold`\n
  \ data type.\n\n### Build Issues\n\n* Fix a bug that occasionally caused a build
  failure on windows when\n  used with `stack` or `stack ghci`.\n* Now builds on 32-bit
  machines.\n* Now builds with `primitive` package version >= 0.5.4 && <= 0.6.4.0\n*
  Now builds with newer `QuickCheck` package version >= 2.14 && < 2.15.\n* Now builds
  with GHC 8.10.\n\n## 0.7.1 (February 2020)\n\n### Bug Fixes\n\n* Fix a bug that
  caused `findIndices` to return wrong indices in some\n  cases.\n* Fix a bug in `tap`,
  `chunksOf` that caused memory consumption to\n  increase in some cases.\n* Fix a
  space leak in concurrent streams (`async`, `wAsync`, and `ahead`) that\n  caused
  memory consumption to increase with the number of elements in the\n  stream, especially
  when built with `-threaded` and used with `-N` RTS option.\n  The issue occurs only
  in cases when a worker thread happens to be used\n  continuously for a long time.\n*
  Fix scheduling of WAsyncT stream style to be in round-robin fashion.\n* Now builds
  with `containers` package version < 0.5.8.\n* Now builds with `network` package
  version >= 3.0.0.0 && < 3.1.0.0.\n\n### Behavior change\n\n* Combinators in `Streamly.Network.Inet.TCP`
  no longer use TCP `NoDelay` and\n  `ReuseAddr` socket options by default. These
  options can now be specified\n  using appropriate combinators.\n\n### Performance\n\n*
  Now uses `fusion-plugin` package for predictable stream fusion optimizations\n*
  Significant improvement in performance of concurrent stream operations.\n* Improved
  space and time performance of `Foldable` instance.\n\n## 0.7.0 (November 2019)\n\n###
  Breaking changes\n\n* Change the signature of `foldrM` to ensure that it is lazy\n*
  Change the signature of `iterateM` to ensure that it is lazy.\n* `scanx` would now
  require an additional `Monad m` constraint.\n\n### Behavior change\n\n* Earlier
  `ParallelT` was unaffected by `maxBuffer` directive, now `maxBuffer`\n  can limit
  the buffer of a `ParallelT` stream as well. When the buffer becomes\n  full, the
  producer threads block.\n* `ParallelT` streams no longer have an unlimited buffer
  by default. Now the\n  buffer for parallel streams is limited to 1500 by default,
  the same as other\n  concurrent stream types.\n\n### Deprecations\n\n* In `Streamly.Prelude`:\n
  \   * `runStream` has been replaced by `drain`\n    * `runN` has been replaced by
  `drainN`\n    * `runWhile` has been replaced by `drainWhile`\n    * `fromHandle`
  has been deprecated. Please use\n      `Streamly.FileSystem.Handle.read`, `Streamly.Data.Unicode.Stream.decodeUtf8`
  and\n      `splitOnSuffix` with `Streamly.Data.Fold.toList` to split the\n       stream
  to a stream of `String` separated by a newline.\n    * `toHandle` has been deprecated.
  Please use `intersperse` and `concatUnfold` to\n      add newlines to a stream,
  `Streamly.Data.Unicode.Stream.encodeUtf8` for encoding and\n      `Streamly.FileSystem.Handle.write`
  for writing to a file handle.\n    * Deprecate `scanx`, `foldx`, `foldxM`, `foldr1`\n
  \   * Remove deprecated APIs `foldl`, `foldlM`\n    * Replace deprecated API `scan`
  with a new signature, to scan using Fold.\n\n* In `Streamly` module:\n    * `runStream`
  has been deprecated, please use `Streamly.Prelude.drain`\n\n* Remove deprecated
  module `Streamly.Time` (moved to Streamly.Internal.Data.Time)\n* Remove module `Streamly.Internal`
  (functionality moved to the Internal hierarchy)\n\n### Bug Fixes\n\n* Fix a bug
  that caused `uniq` function to yield the same element twice.\n* Fix a bug that caused
  \"thread blocked indefinitely in an MVar operation\"\n  exception in a parallel
  stream.\n* Fix unbounded memory usage (leak) in `parallel` combinator. The bug manifests\n
  \ when large streams are combined using `parallel`.\n\n### Major Enhancements\n\nThis
  release contains a lot of new features and major enhancements.  For more\ndetails
  on the new features described below please see the haddock docs of the\nmodules
  on hackage.\n\n#### Exception Handling\n\nSee `Streamly.Prelude` for new exception
  handling combinators like `before`,\n`after`, `bracket`, `onException`, `finally`,
  `handle` etc.\n\n#### Composable Folds\n\n`Streamly.Data.Fold` module provides composable
  folds (stream consumers). Folds\nallow splitting, grouping, partitioning, unzipping
  and nesting a stream onto\nmultiple folds without breaking the stream. Combinators
  are provided for\ntemporal and spatial window based fold operations, for example,
  to support\nfolding and aggregating data for timeout or inactivity based sessions.\n\n####
  Composable Unfolds\n\n`Streamly.Data.Unfold` module provides composable stream generators.
  Unfolds allow\nhigh performance merging/flattening/combining of stream generators.\n\n####
  Streaming File IO\n\n`Streamly.FileSystem.Handle` provides handle based streaming
  file IO\noperations.\n\n#### Streaming Network IO\n\n* `Streamly.Network.Socket`
  provides socket based streaming network IO\noperations.\n\n* `Streamly.Network.Inet.TCP`
  provides combinators to build Inet/TCP\nclients and servers.\n\n#### Concurrent
  concatMap\n\nThe new `concatMapWith` in `Streamly.Prelude` combinator performs a\n`concatMap`
  using a supplied merge/concat strategy. This is a very\npowerful combinator as you
  can, for example, concat streams\nconcurrently using this.\n\n### Other Enhancements\n\n*
  Add the following new features/modules:\n  * _Unicode Strings_: `Streamly.Data.Unicode.Stream`
  module provides\n    encoding/decoding of character streams and other character
  stream\n    operations.\n  * _Arrays_: `Streamly.Memory.Array` module provides arrays
  for efficient\n    in-memory buffering and efficient interfacing with IO.\n\n* Add
  the following to `Streamly.Prelude`:\n    * `unfold`, `fold`, `scan` and `postscan`\n
  \   * `concatUnfold` to concat a stream after unfolding each element\n    * `intervalsOf`
  and `chunksOf`\n    * `splitOn`, `splitOnSuffix`, `splitWithSuffix`, and `wordsBy`\n
  \   * `groups`, `groupsBy` and `groupsByRolling`\n    * `postscanl'` and `postscanlM'`\n
  \   * `intersperse` intersperse an element in between consecutive elements in\n
  \     stream\n    * `trace` combinator maps a monadic function on a stream just
  for side\n      effects\n    * `tap` redirects a copy of the stream to a `Fold`\n\n##
  0.6.1 (March 2019)\n\n### Bug Fixes\n\n* Fix a bug that caused `maxThreads` directive
  to be ignored when rate control\n  was not used.\n\n### Enhancements\n\n* Add GHCJS
  support\n* Remove dependency on \"clock\" package\n\n## 0.6.0 (December 2018)\n\n###
  Breaking changes\n\n* `Monad` constraint may be needed on some of the existing APIs
  (`findIndices`\n  and `elemIndices`).\n\n### Enhancements\n\n* Add the following
  functions to Streamly.Prelude:\n    * Generation: `replicate`, `fromIndices`, `fromIndicesM`\n
  \   * Enumeration: `Enumerable` type class, `enumerateFrom`, `enumerateFromTo`,\n
  \     `enumerateFromThen`, `enumerateFromThenTo`, `enumerate`, `enumerateTo`\n    *
  Running: `runN`, `runWhile`\n    * Folds: `(!!)`, `maximumBy`, `minimumBy`, `the`\n
  \   * Scans: `scanl1'`, `scanl1M'\n    * Filters: `uniq`, `insertBy`, `deleteBy`,
  `findM`\n    * Multi-stream: `eqBy`, `cmpBy`, `mergeBy`, `mergeByM`, `mergeAsyncBy`,\n
  \     `mergeAsyncByM`, `isPrefixOf`, `isSubsequenceOf`, `stripPrefix`,\n      `concatMap`,
  `concatMapM`, `indexed`, `indexedR`\n* Following instances were added for `SerialT
  m`, `WSerialT m` and\n  `ZipSerialM m`:\n  * When `m` ~ `Identity`: IsList, Eq,
  Ord, Show, Read, IsString, NFData,\n    NFData1, Traversable\n  * When `m` is `Foldable`:
  Foldable\n* Performance improvements\n* Add benchmarks to measure composed and iterated
  operations\n\n## 0.5.2 (October 2018)\n\n### Bug Fixes\n\n* Cleanup any pending
  threads when an exception occurs.\n* Fixed a livelock in ahead style streams. The
  problem manifests sometimes when\n  multiple streams are merged together in ahead
  style and one of them is a nil\n  stream.\n* As per expected concurrency semantics
  each forked concurrent task must run\n  with the monadic state captured at the fork
  point.  This release fixes a bug,\n  which, in some cases caused an incorrect monadic
  state to be used for a\n  concurrent action, leading to unexpected behavior when
  concurrent streams are\n  used in a stateful monad e.g. `StateT`. Particularly,
  this bug cannot affect\n  `ReaderT`.\n\n## 0.5.1 (September 2018)\n\n* Performance
  improvements, especially space consumption, for concurrent\n  streams\n\n## 0.5.0
  (September 2018)\n\n### Bug Fixes\n\n* Leftover threads are now cleaned up as soon
  as the consumer is garbage\n  collected.\n* Fix a bug in concurrent function application
  that in certain cases would\n  unnecessarily share the concurrency state resulting
  in incorrect output\n  stream.\n* Fix passing of state across `parallel`, `async`,
  `wAsync`, `ahead`, `serial`,\n  `wSerial` combinators. Without this fix combinators
  that rely on state\n  passing e.g.  `maxThreads` and `maxBuffer` won't work across
  these\n  combinators.\n\n### Enhancements\n\n* Added rate limiting combinators `rate`,
  `avgRate`, `minRate`, `maxRate` and\n  `constRate` to control the yield rate of
  a stream.\n* Add `foldl1'`, `foldr1`, `intersperseM`, `find`, `lookup`, `and`, `or`,\n
  \ `findIndices`, `findIndex`, `elemIndices`, `elemIndex`, `init` to Prelude\n\n###
  Deprecations\n\n* The `Streamly.Time` module is now deprecated, its functionality
  is subsumed\n  by the new rate limiting combinators.\n\n## 0.4.1 (July 2018)\n\n###
  Bug Fixes\n\n* foldxM was not fully strict, fixed.\n\n## 0.4.0 (July 2018)\n\n###
  Breaking changes\n\n* Signatures of `zipWithM` and `zipAsyncWithM` have changed\n*
  Some functions in prelude now require an additional `Monad` constraint on\n  the
  underlying type of the stream.\n\n### Deprecations\n\n* `once` has been deprecated
  and renamed to `yieldM`\n\n### Enhancements\n\n* Add concurrency control primitives
  `maxThreads` and `maxBuffer`.\n* Concurrency of a stream with bounded concurrency
  when used with `take` is now\n  limited by the number of elements demanded by `take`.\n*
  Significant performance improvements utilizing stream fusion optimizations.\n* Add
  `yield` to construct a singleton stream from a pure value\n* Add `repeat` to generate
  an infinite stream by repeating a pure value\n* Add `fromList` and `fromListM` to
  generate streams from lists, faster than\n  `fromFoldable` and `fromFoldableM`\n*
  Add `map` as a synonym of fmap\n* Add `scanlM'`, the monadic version of scanl'\n*
  Add `takeWhileM` and `dropWhileM`\n* Add `filterM`\n\n## 0.3.0 (June 2018)\n\n###
  Breaking changes\n\n* Some prelude functions, to whom concurrency capability has
  been added, will\n  now require a `MonadAsync` constraint.\n\n### Bug Fixes\n\n*
  Fixed a race due to which, in a rare case, we might block indefinitely on\n  an
  MVar due to a lost wakeup.\n* Fixed an issue in adaptive concurrency. The issue
  caused us to stop creating\n  more worker threads in some cases due to a race. This
  bug would not cause any\n  functional issue but may reduce concurrency in some cases.\n\n###
  Enhancements\n* Added a concurrent lookahead stream type `Ahead`\n* Added `fromFoldableM`
  API that creates a stream from a container of monadic\n  actions\n* Monadic stream
  generation functions `consM`, `|:`, `unfoldrM`, `replicateM`,\n  `repeatM`, `iterateM`
  and `fromFoldableM` can now generate streams\n  concurrently when used with concurrent
  stream types.\n* Monad transformation functions `mapM` and `sequence` can now map
  actions\n  concurrently when used at appropriate stream types.\n* Added concurrent
  function application operators to run stages of a\n  stream processing function
  application pipeline concurrently.\n* Added `mapMaybe` and `mapMaybeM`.\n\n## 0.2.1
  (June 2018)\n\n### Bug Fixes\n* Fixed a bug that caused some transformation ops
  to return incorrect results\n  when used with concurrent streams. The affected ops
  are `take`, `filter`,\n  `takeWhile`, `drop`, `dropWhile`, and `reverse`.\n\n##
  0.2.0 (May 2018)\n\n### Breaking changes\n* Changed the semantics of the Semigroup
  instance for `InterleavedT`, `AsyncT`\n  and `ParallelT`. The new semantics are
  as follows:\n  * For `InterleavedT`, `<>` operation interleaves two streams\n  *
  For `AsyncT`, `<>` now concurrently merges two streams in a left biased\n    manner
  using demand based concurrency.\n  * For `ParallelT`, the `<>` operation now concurrently
  meges the two streams\n    in a fairly parallel manner.\n\n  To adapt to the new
  changes, replace `<>` with `serial` wherever it is used\n  for stream types other
  than `StreamT`.\n\n* Remove the `Alternative` instance.  To adapt to this change
  replace any usage\n  of `<|>` with `parallel` and `empty` with `nil`.\n* Stream
  type now defaults to the `SerialT` type unless explicitly specified\n  using a type
  combinator or a monomorphic type.  This change reduces puzzling\n  type errors for
  beginners. It includes the following two changes:\n  * Change the type of all stream
  elimination functions to use `SerialT`\n    instead of a polymorphic type. This
  makes sure that the stream type is\n    always fixed at all exits.\n  * Change the
  type combinators (e.g. `parallely`) to only fix the argument\n    stream type and
  the output stream type remains polymorphic.\n\n  Stream types may have to be changed
  or type combinators may have to be added\n  or removed to adapt to this change.\n*
  Change the type of `foldrM` to make it consistent with `foldrM` in base.\n* `async`
  is renamed to `mkAsync` and `async` is now a new API with a different\n  meaning.\n*
  `ZipAsync` is renamed to `ZipAsyncM` and `ZipAsync` is now ZipAsyncM\n  specialized
  to the IO Monad.\n* Remove the `MonadError` instance as it was not working correctly
  for\n  parallel compositions. Use `MonadThrow` instead for error propagation.\n*
  Remove Num/Fractional/Floating instances as they are not very useful. Use\n  `fmap`
  and `liftA2` instead.\n\n### Deprecations\n* Deprecate and rename the following
  symbols:\n    * `Streaming` to `IsStream`\n    * `runStreaming` to `runStream`\n
  \   * `StreamT` to `SerialT`\n    * `InterleavedT` to `WSerialT`\n    * `ZipStream`
  to `ZipSerialM`\n    * `ZipAsync` to `ZipAsyncM`\n    * `interleaving` to `wSerially`\n
  \   * `zipping` to `zipSerially`\n    * `zippingAsync` to `zipAsyncly`\n    * `<=>`
  to `wSerial`\n    * `<|` to `async`\n    * `each` to `fromFoldable`\n    * `scan`
  to `scanx`\n    * `foldl` to `foldx`\n    * `foldlM` to `foldxM`\n* Deprecate the
  following symbols for future removal:\n    * `runStreamT`\n    * `runInterleavedT`\n
  \   * `runAsyncT`\n    * `runParallelT`\n    * `runZipStream`\n    * `runZipAsync`\n\n###
  Enhancements\n* Add the following functions:\n    * `consM` and `|:` operator to
  construct streams from monadic actions\n    * `once` to create a singleton stream
  from a monadic action\n    * `repeatM` to construct a stream by repeating a monadic
  action\n    * `scanl'` strict left scan\n    * `foldl'` strict left fold\n    *
  `foldlM'` strict left fold with a monadic fold function\n    * `serial` run two
  streams serially one after the other\n    * `async` run two streams asynchronously\n
  \   * `parallel` run two streams in parallel (replaces `<|>`)\n    * `WAsyncT` stream
  type for BFS version of `AsyncT` composition\n* Add simpler stream types that are
  specialized to the IO monad\n* Put a bound (1500) on the output buffer used for
  asynchronous tasks\n* Put a limit (1500) on the number of threads used for Async
  and WAsync types\n\n## 0.1.2 (March 2018)\n\n### Enhancements\n* Add `iterate`,
  `iterateM` stream operations\n\n### Bug Fixes\n* Fixed a bug that caused unexpected
  behavior when `pure` was used to inject\n  values in Applicative composition of
  `ZipStream` and `ZipAsync` types.\n\n## 0.1.1 (March 2018)\n\n### Enhancements\n*
  Make `cons` right associative and provide an operator form `.:` for it\n* Add `null`,
  `tail`, `reverse`, `replicateM`, `scan` stream operations\n* Improve performance
  of some stream operations (`foldl`, `dropWhile`)\n\n### Bug Fixes\n* Fix the `product`
  operation. Earlier, it always returned 0 due to a bug\n* Fix the `last` operation,
  which returned `Nothing` for singleton streams\n\n## 0.1.0 (December 2017)\n\n*
  Initial release\n"
basic-deps:
  exceptions: '>=0.8 && <0.11'
  heaps: '>=0.3 && <0.5'
  base: '>=4.9 && <5'
  monad-control: '>=1.0 && <2'
  filepath: '>=1.2.0.0 && <1.4.3.0'
  lockfree-queue: '>=0.2.3 && <0.3'
  network: '>=2.6 && <4'
  containers: '>=0.5 && <0.7'
  ghc-prim: '>=0.2 && <0.9'
  unicode-data: '>=0.1 && <0.3'
  fusion-plugin-types: '>=0.1 && <0.2'
  atomic-primops: '>=0.8 && <0.9'
  mtl: '>=2.2 && <3'
  transformers-base: '>=0.4 && <0.5'
  transformers: '>=0.4 && <0.7'
  deepseq: '>=1.4.1 && <1.5'
  primitive: '>=0.5.4 && <0.8'
  directory: '>=1.2.2 && <1.4'
all-versions:
- 0.1.0
- 0.1.1
- 0.1.2
- 0.2.0
- 0.2.1
- 0.3.0
- 0.4.0
- 0.4.1
- 0.5.0
- 0.5.1
- 0.5.2
- 0.6.0
- 0.6.1
- 0.7.0
- 0.7.1
- 0.7.2
- 0.7.3
- 0.8.0
- 0.8.1
author: Composewell Technologies
latest: 0.8.1
description-type: markdown
description: |
  # [Streamly][]: Idiomatic Haskell with the Performance of C

  [![Gitter chat](https://badges.gitter.im/composewell/gitter.svg)](https://gitter.im/composewell/streamly)
  [![Hackage](https://img.shields.io/hackage/v/streamly.svg?style=flat)](https://hackage.haskell.org/package/streamly)

  Streamly is a Haskell library that provides the building blocks to build
  safe, scalable, modular and high performance software.  Streamly offers:

  * The type safety of Haskell.
  * The performance of C programs.
  * Powerful abstractions for structuring your code.
  * Idiomatic functional programming.
  * Declarative concurrency for the seamless use of multiprocessing hardware.

  ## About This Document

  This guide introduces programming with [Streamly][] using a few practical
  examples:

  *  We will start with a simple program that [counts the number of words
     in a text](#modular-word-counting). We will then transform this program
     into a [concurrent](#concurrent-word-counting) program that can efficiently
     use multiprocessing hardware.
  *  Next, we will create a [concurrent network
     server](#a-concurrent-network-server). We then show
     how to write a network server that [merges multiple
     streams](#merging-incoming-streams) concurrently.
  *  Our third example shows how to list a directory tree concurrently,
     by reading [multiple directories in
     parallel](#listing-directories-recursivelyconcurrently).
  *  Finally, we will look at how to [rate limit](#rate-limiting) stream
     processing.

  The guide then looks at how Streamly achieves its
  [performance](#performance).  It [concludes](#notes) with a brief
  discussion about Streamly's design philosophy, and with suggestions for
  further reading.

  ## Getting Started

  ### Installing Streamly

  If you wish to follow along with this guide, you will need to have
  [Streamly][] installed.

  Please see the [Getting Started With The Streamly Package](docs/getting-started.md)
  guide for instructions on how to install [Streamly][].

  If you wish to run benchmarks, please be sure to build your
  application using the instructions in the [Build Guide](docs/building.md).

  ### An overview of the types used in these examples

  As an expository device, we have indicated the types at the intermediate
  stages of stream computations as comments in the examples below.
  The meaning of these types are:

  * A `SerialT IO a` is a serial stream of values of type `a` in the IO Monad.
  * An `AsyncT IO a` is a concurrent (asynchronous) stream of values of type
    `a` in the IO Monad.
  * An `Unfold IO a b` is a representation of a function that converts a seed
    value of type `a` into a stream of values of type `b` in the IO Monad.
  * A `Fold IO a b` is a representation of a function that converts a stream of
    type `a` to a final accumulator of type `b` in the IO Monad.

  ### A Note on Module Naming

  Some of the examples below use modules from the `Internal` Streamly package
  hierarchy.  These are not really internal to the library.  We classify
  `Streamly` modules into two categories:

  * _Released modules and APIs_: These modules and APIs are
    stable. Significant changes to these modules and APIs will cause
    Streamly's version number to change according to the package versioning
    policy.
  * _Pre-release modules and APIs_: These modules and APIs have not been
    formally released yet.  They may change in the near future, and such
    changes will not necessarily be reflected in Streamly's package
    version number.  As yet unreleased modules and APIs reside in the
    `Internal` namespace.

  Please use a minor release upper bound to adhere to the Haskell PVP when
  using the pre-release (internal) modules.

  ## The Examples

  ### Modular Word Counting

  A `Fold` in Streamly is a composable stream consumer.  For our first
  example, we will use `Fold`s to count the number of bytes, words and lines
  present in a file.  We will then compose individual `Fold`s together to
  count words, bytes and lines at the same time.

  Please see the file [WordCountModular.hs][] for the complete example
  program, including the imports that we have omitted here.

  #### Count Bytes (wc -c)

  We start with a code fragment that counts the number of bytes in a file:

  ```haskell
  import qualified Streamly.Data.Fold as Fold
  import qualified Streamly.Internal.FileSystem.File as File
  import qualified Streamly.Prelude as Stream

  wcb :: String -> IO Int
  wcb file =
      File.toBytes file        -- SerialT IO Word8
    & Stream.fold Fold.length  -- IO Int
  ```

  ### Count Lines (wc -l)

  The next code fragment shows how to count the number of lines in a file:

  ```haskell
  -- ASCII character 10 is a newline.
  countl :: Int -> Word8 -> Int
  countl n ch = if ch == 10 then n + 1 else n

  -- The fold accepts a stream of `Word8` and returns a line count (`Int`).
  nlines :: Monad m => Fold m Word8 Int
  nlines = Fold.foldl' countl 0

  wcl :: String -> IO Int
  wcl file =
      File.toBytes file  -- SerialT IO Word8
    & Stream.fold nlines -- IO Int
  ```

  ### Count Words (wc -w)

  Our final code fragment counts the number of whitespace-separated words
  in a stream:

  ```haskell
  countw :: (Int, Bool) -> Word8 -> (Int, Bool)
  countw (n, wasSpace) ch =
      if isSpace $ chr $ fromIntegral ch
      then (n, True)
      else (if wasSpace then n + 1 else n, False)

  -- The fold accepts a stream of `Word8` and returns a word count (`Int`).
  nwords :: Monad m => Fold m Word8 Int
  nwords = fst <$> Fold.foldl' countw (0, True)

  wcw :: String -> IO Int
  wcw file =
      File.toBytes file   -- SerialT IO Word8
    & Stream.fold nwords  -- IO Int
  ```

  ### Counting Bytes, Words and Lines Together

  By using the `Tee` combinator we can compose the three folds that count
  bytes, lines and words individually into a single fold that counts all
  three at once.  The applicative instance of `Tee` distributes its input
  to all the supplied folds (`Fold.length`, `nlines`, and `nwords`) and
  then combines the outputs from the folds using the supplied combiner
  function (`(,,)`).

  ```haskell
  import qualified Streamly.Internal.Data.Fold.Tee as Tee

  -- The fold accepts a stream of `Word8` and returns the three counts.
  countAll :: Fold IO Word8 (Int, Int, Int)
  countAll = Tee.toFold $ (,,) <$> Tee Fold.length <*> Tee nlines <*> Tee nwords

  wc :: String -> IO (Int, Int, Int)
  wc file =
      File.toBytes file    -- SerialT IO Word8
    & Stream.fold countAll -- IO (Int, Int, Int)
  ```

  This example demonstrates the excellent modularity offered by
  [Streamly][]'s simple and concise API.  Experienced Haskellers will
  notice that we have not used bytestrings&mdash;we instead used a stream of
  `Word8` values, simplifying our program.

  ### The Performance of Word Counting

  We compare two equivalent implementations: one using [Streamly][],
  and the other using C.

  The performance of the [Streamly word counting
  implementation][WordCount.hs] is:

  ```
  $ time WordCount-hs gutenberg-500MB.txt
  11242220 97050938 574714449 gutenberg-500MB.txt

  real    0m1.825s
  user    0m1.697s
  sys     0m0.128s
  ```

  The performance of an equivalent [wc implementation in C][WordCount.c] is:

  ```
  $ time WordCount-c gutenberg-500MB.txt
  11242220 97050938 574714449 gutenberg-500MB.txt

  real    0m2.100s
  user    0m1.935s
  sys     0m0.165s
  ```

  ### Concurrent Word Counting

  In our next example we show how the task of counting words, lines,
  and bytes could be done in parallel on multiprocessor hardware.

  To count words in parallel we first divide the stream into chunks
  (arrays), do the counting within each chunk, and then add all the
  counts across chunks.  We use the same code as above except that we use
  arrays for our input data.

  Please see the file [WordCountParallel.hs][] for the complete working
  code for this example, including the imports that we have omitted below.

  The `countArray` function counts the line, word, char counts in one chunk:

  ```haskell
  import qualified Streamly.Data.Array.Foreign as Array

  countArray :: Array Word8 -> IO Counts
  countArray arr =
        Stream.unfold Array.read arr            -- SerialT IO Word8
      & Stream.decodeLatin1                     -- SerialT IO Char
      & Stream.foldl' count (Counts 0 0 0 True) -- IO Counts
  ```

  Here the function `count` and the `Counts` data type are defined in the
  `WordCount` helper module defined in [WordCount.hs][].

  When combining the counts in two contiguous chunks, we need to check
  whether the first element of the next chunk is a whitespace character
  in order to determine if the same word continues in the next chunk or
  whether the chunk starts with a new word. The `partialCounts` function
  adds a `Bool` flag to `Counts` returned by `countArray` to indicate
  whether the first character in the chunk is a space.

  ```haskell
  partialCounts :: Array Word8 -> IO (Bool, Counts)
  partialCounts arr = do
      let r = Array.getIndex arr 0
      case r of
          Just x -> do
              counts <- countArray arr
              return (isSpace (chr (fromIntegral x)), counts)
          Nothing -> return (False, Counts 0 0 0 True)
  ```

  `addCounts` then adds the counts from two consecutive chunks:

  ```haskell
  addCounts :: (Bool, Counts) -> (Bool, Counts) -> (Bool, Counts)
  addCounts (sp1, Counts l1 w1 c1 ws1) (sp2, Counts l2 w2 c2 ws2) =
      let wcount =
              if not ws1 && not sp2 -- No space between two chunks.
              then w1 + w2 - 1
              else w1 + w2
       in (sp1, Counts (l1 + l2) wcount (c1 + c2) ws2)
  ```

  To count in parallel we now only need to divide the stream into arrays,
  apply our counting function to each array, and then combine the counts
  from each chunk.

  ```haskell
  wc :: String -> IO (Bool, Counts)
  wc file = do
        Stream.unfold File.readChunks file -- AheadT IO (Array Word8)
      & Stream.mapM partialCounts          -- AheadT IO (Bool, Counts)
      & Stream.maxThreads numCapabilities  -- AheadT IO (Bool, Counts)
      & Stream.fromAhead                   -- SerialT IO (Bool, Counts)
      & Stream.foldl' addCounts (False, Counts 0 0 0 True) -- IO (Bool, Counts)
  ```

  Please note that the only difference between a concurrent and a
  non-concurrent program lies in the use of the `Stream.fromAhead`
  combinator.  If we remove the call to `Stream.fromAhead`, we would
  still have a perfectly valid and performant serial program. Notice
  how succinctly and idiomatically we have expressed the concurrent word
  counting problem.

  A benchmark with 2 CPUs:

  ```
  $ time WordCount-hs-parallel gutenberg-500MB.txt
  11242220 97050938 574714449 gutenberg-500MB.txt

  real    0m1.284s
  user    0m1.952s
  sys     0m0.140s
  ```

  These example programs have assumed ASCII encoded input data.  For UTF-8
  streams, we have a [concurrent wc implementation][WordCountUTF8.hs]
  with UTF-8 decoding.  This concurrent implementation performs as well
  as the standard `wc` program in serial benchmarks. In concurrent mode
  [Streamly][]'s implementation can utilise multiple processing cores if
  these are present, and can thereby run much faster than the standard
  binary.

  Streamly provides concurrency facilities similar
  to [OpenMP](https://en.wikipedia.org/wiki/OpenMP) and
  [Cilk](https://en.wikipedia.org/wiki/Cilk) but with a more declarative
  style of expression.  With Streamly you can write concurrent programs
  with ease, with support for different types of concurrent scheduling.

  ### A Concurrent Network Server

  We now move to a slightly more complicated example: we simulate a
  dictionary lookup server which can serve word meanings to multiple
  clients concurrently.  This example demonstrates the use of the concurrent
  `mapM` combinator.

  Please see the file [WordServer.hs][] for the complete code for this
  example, including the imports that we have omitted below.

  ```haskell
  import qualified Streamly.Data.Fold as Fold
  import qualified Streamly.Network.Inet.TCP as TCP
  import qualified Streamly.Network.Socket as Socket
  import qualified Streamly.Unicode.Stream as Unicode

  -- Simulate network/db query by adding a delay.
  fetch :: String -> IO (String, String)
  fetch w = threadDelay 1000000 >> return (w,w)

  -- Read lines of whitespace separated list of words from a socket, fetch the
  -- meanings of each word concurrently and return the meanings separated by
  -- newlines, in same order as the words were received. Repeat until the
  -- connection is closed.
  lookupWords :: Socket -> IO ()
  lookupWords sk =
        Stream.unfold Socket.read sk               -- SerialT IO Word8
      & Unicode.decodeLatin1                       -- SerialT IO Char
      & Stream.wordsBy isSpace Fold.toList         -- SerialT IO String
      & Stream.fromSerial                          -- AheadT  IO String
      & Stream.mapM fetch                          -- AheadT  IO (String, String)
      & Stream.fromAhead                           -- SerialT IO (String, String)
      & Stream.map show                            -- SerialT IO String
      & Stream.intersperse "\n"                    -- SerialT IO String
      & Unicode.encodeStrings Unicode.encodeLatin1 -- SerialT IO (Array Word8)
      & Stream.fold (Socket.writeChunks sk)        -- IO ()

  serve :: Socket -> IO ()
  serve sk = finally (lookupWords sk) (close sk)

  -- | Run a server on port 8091. Accept and handle connections concurrently. The
  -- connection handler is "serve" (i.e. lookupWords).  You can use "telnet" or
  -- "nc" as a client to try it out.
  main :: IO ()
  main =
        Stream.unfold TCP.acceptOnPort 8091 -- SerialT IO Socket
      & Stream.fromSerial                   -- AsyncT IO ()
      & Stream.mapM serve                   -- AsyncT IO ()
      & Stream.fromAsync                    -- SerialT IO ()
      & Stream.drain                        -- IO ()
  ```

  ### Merging Incoming Streams

  In the next example, we show how to merge logs coming from multiple
  nodes in your network.  These logs are merged at line boundaries and
  the merged logs are written to a file or to a network destination.
  This example uses the `concatMapWith` combinator to merge multiple
  streams concurrently.

  Please see the file [MergeServer.hs][] for the complete working code,
  including the imports that we have omitted below.

  ```haskell
  import qualified Streamly.Data.Unfold as Unfold
  import qualified Streamly.Network.Socket as Socket

  -- | Read a line stream from a socket.
  -- Note: lines are buffered, and we could add a limit to the
  -- buffering for safety.
  readLines :: Socket -> SerialT IO (Array Char)
  readLines sk =
      Stream.unfold Socket.read sk                 -- SerialT IO Word8
    & Unicode.decodeLatin1                         -- SerialT IO Char
    & Stream.splitWithSuffix (== '\n') Array.write -- SerialT IO String

  recv :: Socket -> SerialT IO (Array Char)
  recv sk = Stream.finally (liftIO $ close sk) (readLines sk)

  -- | Starts a server at port 8091 listening for lines with space separated
  -- words. Multiple clients can connect to the server and send streams of lines.
  -- The server handles all the connections concurrently, merges the incoming
  -- streams at line boundaries and writes the merged stream to a file.
  server :: Handle -> IO ()
  server file =
        Stream.unfold TCP.acceptOnPort 8090        -- SerialT IO Socket
      & Stream.concatMapWith Stream.parallel recv  -- SerialT IO (Array Char)
      & Stream.unfoldMany Array.read               -- SerialT IO Char
      & Unicode.encodeLatin1                       -- SerialT IO Word8
      & Stream.fold (Handle.write file)            -- IO ()

  main :: IO ()
  main = withFile "output.txt" AppendMode server
  ```

  ### Listing Directories Recursively/Concurrently

  Our next example lists a directory tree recursively, reading
  multiple directories concurrently.

  This example uses the tree traversing combinator `iterateMapLeftsWith`.
  This combinator maps a stream generator on the `Left` values in its
  input stream (directory names in this case), feeding the resulting `Left`
  values back to the input, while it lets the `Right` values (file names
  in this case) pass through to the output. The `Stream.ahead` stream
  joining combinator then makes it iterate on the directories concurrently.

  Please see the file [ListDir.hs][] for the complete working code,
  including the imports that we have omitted below.

  ```haskell
  import Streamly.Internal.Data.Stream.IsStream (iterateMapLeftsWith)

  import qualified Streamly.Prelude as Stream
  import qualified Streamly.Internal.FileSystem.Dir as Dir (toEither)

  -- Lists a directory as a stream of (Either Dir File).
  listDir :: String -> SerialT IO (Either String String)
  listDir dir =
        Dir.toEither dir               -- SerialT IO (Either String String)
      & Stream.map (bimap mkAbs mkAbs) -- SerialT IO (Either String String)

      where mkAbs x = dir ++ "/" ++ x

  -- | List the current directory recursively using concurrent processing.
  main :: IO ()
  main = do
      hSetBuffering stdout LineBuffering
      let start = Stream.fromPure (Left ".")
      Stream.iterateMapLeftsWith Stream.ahead listDir start
          & Stream.mapM_ print
  ```

  ### Rate Limiting

  For bounded concurrent streams, a stream yield rate can be specified
  easily.  For example, to print "tick" once every second you can simply
  write:

  ```haskell
  main :: IO ()
  main =
        Stream.repeatM (pure "tick")  -- AsyncT IO String
      & Stream.timestamped            -- AsyncT IO (AbsTime, String)
      & Stream.avgRate 1              -- AsyncT IO (AbsTime, String)
      & Stream.fromAsync              -- SerialT IO (AbsTime, String)
      & Stream.mapM_ print            -- IO ()
  ```

  Please see the file [Rate.hs][] for the complete working code.

  The concurrency of the stream is automatically controlled to match the
  specified rate. [Streamly][]'s rate control works precisely even at
  throughputs as high as millions of yields per second.

  For more sophisticated rate control needs please see the Streamly [reference
  documentation][Streamly].

  ### Reactive Programming

  Streamly supports reactive (time domain) programming because of its
  support for declarative concurrency. Please see the `Streamly.Prelude`
  module for time-specific combinators like `intervalsOf`, and
  folds like `takeInterval` in `Streamly.Internal.Data.Fold`.
  Please also see the pre-release sampling combinators in the
  `Streamly.Internal.Data.Stream.IsStream.Top` module for `throttle` and
  `debounce` like operations.

  The examples [AcidRain.hs][] and [CirclingSquare.hs][] demonstrate
  reactive programming using [Streamly][].

  ### More Examples

  If you would like to view more examples, please visit the [Streamly
  Examples][streamly-examples] web page.

  ### Further Reading

  * [Streaming Benchmarks][streaming-benchmarks]
  * [Concurrency Benchmarks][concurrency-benchmarks]
  * Functional Conf 2019 [Video](https://www.youtube.com/watch?v=uzsqgdMMgtk) | [Slides](https://www.slideshare.net/HarendraKumar10/streamly-concurrent-data-flow-programming)
  * [Other Guides](docs/)
  * [Streamly Homepage][Streamly]

  ## Performance

  As you have seen in the word count example above, [Streamly][] offers
  highly modular abstractions for building programs while also offering
  the performance close to an equivalent (imperative) C program.

  Streamly offers excellent performance even for byte-at-a-time stream
  operations using efficient abstractions like `Unfold`s and terminating
  `Fold`s.  Byte-at-a-time stream operations can simplify programming
  because the developer does not have to deal explicitly with chunking
  and re-combining data.

  Streamly exploits GHC's stream fusion optimizations (`case-of-case` and
  `spec-constr`) aggressively to achieve C-like speed, while also offering
  highly modular abstractions to developers.

  [Streamly][] will usually perform very well without any
  compiler plugins.  However, we have fixed some deficiencies
  that we had noticed in GHC's optimizer using a [compiler
  plugin](https://github.com/composewell/fusion-plugin).  We hope to fold
  these optimizations into GHC in the future; until then we recommend that
  you use this plugin for applications that are performance sensitive.

  ### Benchmarks

  We measured several Haskell streaming implementations
  using various micro-benchmarks. Please see the [streaming
  benchmarks][streaming-benchmarks] page for a detailed comparison of
  Streamly against other streaming libraries.

  Our results show that [Streamly][] is the fastest effectful streaming
  implementation on almost all the measured microbenchmarks. In many cases
  it runs up to 100x faster, and in some cases even 1000x faster than
  some of the tested alternatives. In some composite operation benchmarks
  [Streamly][] turns out to be significantly faster than Haskell's list
  implementation.

  *Note*: If you can write a program in some other way or with some other
  language that runs significantly faster than what [Streamly][] offers,
  please let us know and we will improve.

  ## Notes

  Streamly comes equipped with a very powerful set of abstractions to
  accomplish many kinds of programming tasks: it provides support for
  programming with streams and arrays, for reading and writing from the
  file system and from the network, for time domain programming (reactive
  programming), and for reacting to file system events using `fsnotify`.

  Please view [Streamly's documentation][Streamly] for more information
  about Streamly's features.

  ### Concurrency

  Streamly uses lock-free synchronization for achieving concurrent
  operation with low overheads.  The number of tasks performed concurrently
  are determined automatically based on the rate at which a consumer
  is consuming the results. In other words, you do not need to manage
  thread pools or decide how many threads to use for a particular task.
  For CPU-bound tasks Streamly will try to keep the number of threads
  close to the number of CPUs available; for IO-bound tasks it will utilize
  more threads.

  The parallelism available during program execution can be utilized with
  very little overhead even where the task size is very
  small, because Streamly will automatically switch between
  serial or batched execution of tasks on the same CPU depending
  on whichever is more efficient.  Please see our [concurrency
  benchmarks][concurrency-benchmarks] for more detailed performance
  measurements, and for a comparison with the `async` package.

  ### Design Goals

  Our goals for [Streamly][] from the very beginning have been:

  1. To achieve simplicity by unifying abstractions.
  2. To offer high performance.

  These goals are hard to achieve simultaneously because they are usually
  inversely related.  We have spent many years trying to get the abstractions
  right without compromising performance.

  `Unfold` is an example of an abstraction that we have created to achieve
  high performance when mapping streams on streams.  `Unfold` allows stream
  generation to be optimized well by the compiler through stream fusion.
  A `Fold` with termination capability is another example which modularizes
  stream elimination operations through stream fusion.  Terminating folds
  can perform many simple parsing tasks that do not require backtracking.
  In Streamly, `Parser`s are a natural extension to terminating `Fold`s;
  `Parser`s add the ability to backtrack to `Fold`s.  Unification leads
  to simpler abstractions and lower cognitive overheads while also not
  compromising performance.

  ## Credits

  The following authors/libraries have influenced or inspired this library in a
  significant way:

    * Roman Leshchinskiy ([vector](http://hackage.haskell.org/package/vector))
    * Gabriella Gonzalez ([foldl](https://hackage.haskell.org/package/foldl))
    * Alberto G. Corona ([transient](https://hackage.haskell.org/package/transient))

  Please see the [`credits`](./credits/README.md) directory for a full
  list of contributors, credits and licenses.

  ## Licensing

  Streamly is an [open source](https://github.com/composewell/streamly)
  project available under a liberal [BSD-3-Clause license][LICENSE]

  ## Contributing to Streamly

  As an open project we welcome contributions:

  * [Streamly Contributor's Guide][CONTRIBUTING.md]
  * [Contact the streamly development team](mailto:streamly@composewell.com)

  ## Getting Support

  Professional support is available for [Streamly][]: please contact
  [support@composewell.com](mailto:support@composewell.com).

  You can also join our [community chat
  channel](https://gitter.im/composewell/streamly) on Gitter.

  <!--
  Link References.
  -->

  [Streamly]: https://streamly.composewell.com/
  [streamly-examples]: https://github.com/composewell/streamly-examples
  [streaming-benchmarks]: https://github.com/composewell/streaming-benchmarks
  [concurrency-benchmarks]: https://github.com/composewell/concurrency-benchmarks

  <!--
  Keep all the unstable links here so that they can be updated to stable
  links (for online docs) before we release.
  -->

  <!-- examples -->
  [WordCountModular.hs]: https://github.com/composewell/streamly-examples/blob/master/examples/WordCountModular.hs
  [WordCount.hs]: https://github.com/composewell/streamly-examples/blob/master/examples/WordCount.hs
  [WordCount.c]: https://github.com/composewell/streamly-examples/blob/master/examples/WordCount.c
  [WordCountParallel.hs]: https://github.com/composewell/streamly-examples/blob/master/examples/WordCountParallel.hs
  [WordCountUTF8.hs]: https://github.com/composewell/streamly-examples/blob/master/examples/WordCountUTF8.hs
  [WordServer.hs]: https://github.com/composewell/streamly-examples/blob/master/examples/WordServer.hs
  [MergeServer.hs]: https://github.com/composewell/streamly-examples/blob/master/examples/MergeServer.hs
  [ListDir.hs]: https://github.com/composewell/streamly-examples/blob/master/examples/ListDir.hs
  [Rate.hs]: https://github.com/composewell/streamly-examples/blob/master/examples/Rate.hs
  [AcidRain.hs]: https://github.com/composewell/streamly-examples/tree/master/examples/AcidRain.hs
  [CirclingSquare.hs]: https://github.com/composewell/streamly-examples/tree/master/examples/CirclingSquare.hs

  <!-- local files -->
  [LICENSE]: LICENSE
  [CONTRIBUTING.md]: CONTRIBUTING.md
  [docs]: docs/
license-name: BSD-3-Clause
