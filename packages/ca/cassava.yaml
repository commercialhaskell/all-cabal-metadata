all-versions:
- 0.1.0.0
- 0.1.0.1
- 0.2.0.0
- 0.2.1.0
- 0.2.1.1
- 0.2.1.2
- 0.2.2.0
- 0.3.0.0
- 0.3.0.1
- 0.4.0.0
- 0.4.1.0
- 0.4.2.0
- 0.4.2.1
- 0.4.2.2
- 0.4.2.3
- 0.4.2.4
- 0.4.3.0
- 0.4.3.1
- 0.4.4.0
- 0.4.5.0
- 0.4.5.1
- 0.5.0.0
- 0.5.1.0
- 0.5.2.0
- 0.5.3.0
- 0.5.3.1
- 0.5.3.2
- 0.5.4.0
author: Johan Tibell
basic-deps:
  Only: '>=0.1 && <0.1.1'
  array: '>=0.4 && <0.6'
  attoparsec: '>=0.11.3.0 && <0.15'
  base: '>=4.9 && <5'
  bytestring: '>=0.10.4 && <0.13'
  containers: '>=0.4.2 && <1'
  deepseq: '>=1.1 && <1.6'
  hashable: <2
  scientific: '>=0.3.4.7 && <0.4'
  text: <2.2
  text-short: '>=0.1 && <0.2'
  transformers: '>=0.2 && <0.7'
  unordered-containers: <0.3
  vector: '>=0.8 && <0.14'
changelog: |
  ## Version 0.5.4.0

   * Add `decodeWithP` and `decodeByNameWithP` to `Streaming` interface ([PR #237](https://github.com/haskell-hvr/cassava/pull/237)).
   * Build tested with GHC 8.0 - 9.12.2.
   * Functionality tested with GHC 8.4 - 9.12.2.

  ## Version 0.5.3.2

   * Proper exception on hanging doublequote ([PR #222](https://github.com/haskell-hvr/cassava/pull/222)).
   * Allow latest `hashable`.
   * Build tested with GHC 8.0 - 9.10.1.
   * Functionality tested with GHC 8.4 - 9.10.1.

  ## Version 0.5.3.1

   * Remove support for GHC 7.
   * Remove cabal flag `bytestring--LT-0_10_4` and support for `bytestring < 0.10.4`.
   * Tested with GHC 8.0 - 9.10 alpha3

  ## Version 0.5.3.0 revision 2

   * Allow `bytestring-0.12`
   * Tested with GHC 7.4 - 9.6.2

  ## Version 0.5.3.0 revision 1

   * Allow `base-4.18`
   * Tested with GHC 7.4 - 9.6.1 alpha

  ## Version 0.5.3.0

   * Improve error messages for `lookup` and NamedRecord parsers (#197)
   * Fix bug (infinite loop) in `FromField Const` instance (#185)
   * Turn flag `bytestring--LT-0_10_4` off by default (#183)
   * Doc: Add cassava usage example of reading/writing to file (#97)
   * Update to latest version of dependencies (#190, #193, #199)
   * Tested with GHC 7.4 - 9.4 (#184, #204)

  ## Version 0.5.2.0

   * Add `FromField`/`ToField` instances for `Identity` and `Const` (#158)
   * New `typeclass`-less decoding functions `decodeWithP` and `decodeByNameWithP` (#67,#167)
   * Support for final phase of MFP / base-4.13

  ## Version 0.5.1.0

   * Add `FromField`/`ToField` instance for `Natural` (#141,#142)
   * Add `FromField`/`ToField` instances for `Scientific` (#143,#144)
   * Add support for modifying Generics-based instances (adding
     `Options`, `defaultOptions`, `fieldLabelModifier`,
     `genericParseRecord`, `genericToRecord`, `genericToNamedRecord`,
     `genericHeaderOrder`) (#139,#140)
   * Documentation improvements

  ## Version 0.5.0.0

  ### Semantic changes

   * Don't unecessarily quote spaces with `QuoteMinimal` (#118,#122,#86)
   * Fix semantics of `foldl'` (#102)
   * Fix field error diagnostics being mapped to `endOfInput` in `Parser` monad. (#99)
   * Honor `encIncludeHeader` in incremental API (#136)

  ### Other changes

   * Support GHC 8.2.1
   * Use factored-out `Only` package
   * Add `FromField`/`ToField` instance for `ShortText`
   * Add `MonadFail` and `Semigroup` instance for `Parser`
   * Add `Semigroup` instance for incremental CSV API `Builder` & `NamedBuilder`
   * Port to `ByteString` builder & drop dependency on `blaze-builder`

  ## Version 0.4.5.1

   * Restore GHC 7.4 support (#124)

  ## Version 0.4.5.0

   * Support for GHC 8.0 added; support for GHC 7.4 dropped

   * Fix defect in `Foldable(foldr)` implementation failing to skip
     unconvertable records (#102)

   * Documentation fixes

   * Maintainer changed

  ## Version 0.4.4.0

   * Added record instances for larger tuples.

   * Support attoparsec 0.13.

   * Add field instances for short bytestrings.

  ## Version 0.4.3.0

   * Documentation overhaul with more examples.

   * Add Data.Csv.Builder, a low-level bytestring builder API.

   * Add a high-level builder API to Data.Csv.Incremental.

   * Generalize the default FromNamedRecord/ToNamedRecord instances.

   * Improved support for deriving instances using GHC.Generics.

   * Added some control over quoting.

  ## Version 0.4.2.4

   * Support attoparsec 0.13.

  ## Version 0.4.2.3

   * Support GHC 7.10.

  ## Version 0.4.2.2

   * Support blaze-builder 0.4.

   * Make sure inlining doesn't prevent rules from firing.

   * Fix incorrect INLINE pragmas.

  ## Version 0.4.2.1

   * Support deepseq-1.4.

  ## Version 0.4.2.0

   * Minor performance improvements.

   * Add 8 and 9 tuple instances for From/ToRecord.

   * Support text-1.2.

  ## Version 0.4.1.0

   * Ignore whitespace when converting numeric fields.

   * Accept \r as a line terminator.

   * Support attoparsec-0.12.
changelog-type: markdown
description: |
  [![Hackage](https://img.shields.io/hackage/v/cassava.svg?label=Hackage&color=informational)](https://hackage.haskell.org/package/cassava)
  [![Cabal build](https://github.com/haskell-hvr/cassava/workflows/Haskell-CI/badge.svg)](https://github.com/haskell-hvr/cassava/actions)

  # `cassava`: A CSV parsing and encoding library

  **Please refer to the [package description](https://hackage.haskell.org/package/cassava#description) for an overview of `cassava`.**

  ## Usage example

  Here's the two second crash course in using the library. Given a CSV file with this content:

  ```csv
  John Doe,50000
  Jane Doe,60000
  ```

  here's how you'd process it record-by-record:

  ```haskell
  {-# LANGUAGE ScopedTypeVariables #-}

  import qualified Data.ByteString.Lazy as BL
  import Data.Csv
  import qualified Data.Vector as V

  main :: IO ()
  main = do
      csvData <- BL.readFile "salaries.csv"
      case decode NoHeader csvData of
          Left err -> putStrLn err
          Right v -> V.forM_ v $ \ (name, salary :: Int) ->
              putStrLn $ name ++ " earns " ++ show salary ++ " dollars"
  ```

  If you want to parse a file that includes a header, like this one

  ```csv
  name,salary
  John Doe,50000
  Jane Doe,60000
  ```

  use [`decodeByName`](https://hackage.haskell.org/package/cassava/docs/Data-Csv.html#v:decodeByName):

  ```haskell
  {-# LANGUAGE OverloadedStrings #-}

  import Control.Applicative
  import qualified Data.ByteString.Lazy as BL
  import Data.Csv
  import qualified Data.Vector as V

  data Person = Person
      { name   :: !String
      , salary :: !Int
      }

  instance FromNamedRecord Person where
      parseNamedRecord r = Person <$> r .: "name" <*> r .: "salary"

  main :: IO ()
  main = do
      csvData <- BL.readFile "salaries.csv"
      case decodeByName csvData of
          Left err -> putStrLn err
          Right (_, v) -> V.forM_ v $ \ p ->
              putStrLn $ name p ++ " earns " ++ show (salary p) ++ " dollars"
  ```

  You can find more code examples in the [`examples/` folder](https://github.com/hvr/cassava/tree/master/examples) as well as smaller usage examples in the [`Data.Csv` module documentation](https://hackage.haskell.org/package/cassava/docs/Data-Csv.html).

  ## Project Goals for `cassava`

  There's no end to what people consider CSV data. Most programs don't
  follow [RFC4180](https://tools.ietf.org/html/rfc4180) so one has to
  make a judgment call which contributions to accept.  Consequently, not
  everything gets accepted, because then we'd end up with a (slow)
  general purpose parsing library. There are plenty of those. The goal
  is to roughly accept what the Python
  [`csv`](https://docs.python.org/3/library/csv.html) module accepts.

  The Python `csv` module (which is implemented in C) is also considered
  the base-line for performance.  Adding options (e.g. the above
  mentioned parsing "flexibility") will have to be a trade off against
  performance. There's been complaints about performance in the past,
  therefore, if in doubt performance wins over features.

  Last but not least, it's important to keep the dependency footprint
  light, as each additional dependency incurs costs and risks in terms
  of additional maintenance overhead and loss of flexibility. So adding
  a new package dependency should only be done if that dependency is
  known to be a reliable package and there's a clear benefit which
  outweights the cost.

  ## Further reading

  The primary API documentation for `cassava` is its Haddock documentation which can be found at http://hackage.haskell.org/package/cassava/docs/Data-Csv.html

  Below are listed additional recommended third-party blogposts and tutorials

   - [CSV encoding and decoding in Haskell with Cassava](https://www.stackbuilders.com/tutorials/haskell/csv-encoding-decoding/)
description-type: markdown
hash: 7d1cc4bcceca83f1220db147ccbd3a5e58ed1cb8513c26ec02fbadcb058cf5f0
homepage: https://github.com/haskell-hvr/cassava
latest: 0.5.4.0
license-name: BSD-3-Clause
maintainer: https://github.com/haskell-hvr/cassava
synopsis: A CSV parsing and encoding library
test-bench-deps:
  HUnit: <1.7
  QuickCheck: '>=2.14 && <3'
  attoparsec: '>=0'
  base: '>=4.11 && <5'
  bytestring: '>=0'
  cassava: '>=0'
  hashable: '>=0'
  quickcheck-instances: '>=0.3.12 && <0.4'
  scientific: '>=0'
  test-framework: '>=0.8 && <0.9'
  test-framework-hunit: '>=0.3 && <0.4'
  test-framework-quickcheck2: '>=0.3 && <0.4'
  text: '>=0'
  unordered-containers: '>=0'
  vector: '>=0'
