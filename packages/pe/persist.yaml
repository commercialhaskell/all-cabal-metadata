all-versions:
- '0.1'
- 0.1.1.0
- 0.1.1.1
- 0.1.1.2
- 0.1.1.3
- 0.1.1.4
- 0.1.1.5
- 1.0.0.0
author: |-
  Daniel Mendler <mail@daniel-mendler.de>,
  Michael Sloan <sloan@fpcomplete.com>,
  FP Complete,
  Lennart Kolmodin <kolmodin@dtek.chalmers.se>,
  Galois Inc.,
  Lemmih <lemmih@gmail.com>,
  Bas van Dijk <v.dijk.bas@gmail.com>,
  Kyle Butt <kyle@iteratee.net>
basic-deps:
  base: '>=4.16 && <5'
  bytestring: '>=0.11.1 && <1'
  containers: '>=0.6.5.1 && <0.9'
  text: '>=1.2.5 && <2.2'
changelog: "## Changes from 0.1.1.5 to 1.0.0.0 ##\n\n  * Use DerivingVia to reduce
  repetititon for base instances\n    \n    Add three helper type classes: HasEndianness,
  SerializeAs,\n    ReinterpretAs, and two newtypes: ViaSerializeAs and ViaReinterpretAs.\n
  \   HasEndianness allows us to create persist instances for `BigEndian a`\n    and
  `LittleEndian a` without repetititon. SerializeAs allows anyone to\n    supply a
  cast function in both directions and then re-use either a\n    HasEndianness instance
  or a Persist instance. See the source file for\n    example uses.\n\n  * Add unsafe
  versions of put/get and helpers\n    \n    Allow for Persist instances to reserve
  specific sizes ahead of time and\n    then elide the checks while serializing. As
  an implemented example, we\n    allocate space for the length of a ByteString and
  the bytes in the\n    ByteString together. For characters as well, we allocate the
  space all\n    at once. This technique would be useful for arrays of fixed size\n
  \   elements, like unboxed vectors.\n\n  * Add getPrefix and unsafeGetPrefix\n    \n
  \   It often occurs when deserializing a custom format that we need to\n    recursively
  deserialize a subcomponent from the next `n` bytes. The\n    naive way to do this
  would be to get a `ByteString` for those n bytes,\n    and then run the appropriate
  `Get` on that `ByteString`. To simplify\n    this process, introduce `getPrefix`
  and `unsafeGetPrefix`. These\n    manipulate the `Get` internals to expose only
  the appropriate bytes to\n    the sub-deserializer. This avoids repeated copying,
  and even repeated\n    `ByteString` construction.\n\n  * Add length backpatching\n
  \   \n    There are many serialized formats where a serialized sub-value is\n    prefixed
  by its length. The simplest way to find the serialized length\n    of a value is
  to serialize it and then to measure the length. For\n    formats that use a fixed
  number of bytes for this length, we can avoid\n    copying and also walking a data
  structure twice: Once to find the size,\n    and again to write the data with the
  known size. It is straightforward\n    to use this. Reserve the size, serialize
  the sub-value, then resolve the\n    size, with the correct endianness and inclusivity.\n
  \   \n    We do this by reserving the bytes ahead of time and returning a\n    `PutSize`
  handle that can be used later to fill in the result.\n    Currently, the library
  handles computing the appropriate size, which can\n    be filled in either Big or
  Little endian, and either inclusive or\n    exclusive of the size itself. DHCPv6
  packets use this style of\n    length-value encoding. As does Cassandra's encoding
  format, and MongoDB\n    as well. MongoDB uses this same backpatching optimization.\n\n
  \ * Switch to NoFieldSelectors and OverloadedRecordDot\n    \n    GHC has supported
  OverloadedRecordDot and NoFieldSelectors since 9.2\n    Switch away from prefixed
  names and instead just use names and record\n    dot syntax. There are a handful
  of now-ambiguous updates. They were\n    resolved using a small lambda and RecordWildCards/NamedFieldPuns.
  This\n    is a breaking change, but there are several and we are planning on a new\n
  \   major release.\n\n  * Bump version bounds to match ghc 9.2\n    \n    Bump tested-with
  to match the versions of ghc 9.2--9.12 that were\n    manually tested. Bump versions
  of any libraries included with ghc to\n    match the minimum version included with
  ghc 9.2\n\n  * Add Kyle Butt as maintainer and author\n\n  * Borrow improved error
  message handling from Store\n\n  * Improve interaction with bytestring library\n
  \   \n    Avoid a copy for a strict ByteString with only one chunk by calling\n
  \   `reallocBytes` and then building the ByteString directly by recording a\n    finalizer.\n
  \   \n    Add support for running a put and producing a lazy ByteString. Turn each\n
  \   chunk into a strict ByteString with the appropriate finalizer, and then\n    use
  `foldlM` to reverse the chunks and build the lazy ByteString at the\n    same time.\n\n
  \ * Add resolveSize helpers that write a computed size\n    \n    This makes list
  serialization much faster. It makes almost all of the\n    serialization benchmarks
  competitive with store. The remaining gap is\n    due to reinterpretCast. Copying
  a Double to tmp and then again to the\n    buffer is slower. But this is a simple
  win that puts the new reservation\n    code to good use.\n"
changelog-type: markdown
description: |-
  A binary serialization library with focus on performance similar to store and
  cereal. Includes utilities to match externally specified data formats.
description-type: haddock
hash: 192e7df7dc9d640306d4209ca836b9cbbfaaa81c251f308b72e780c713740c12
homepage: https://github.com/minad/persist
latest: 1.0.0.0
license-name: BSD-3-Clause
maintainer: Kyle Butt <kyle@iteratee.net>
synopsis: Minimal serialization library with focus on performance
test-bench-deps:
  QuickCheck: '>=2.16.0 && <2.17'
  base: '>=0'
  bytestring: '>=0'
  persist: '>=0'
  test-framework: '>=0.8.2 && <0.9'
  test-framework-quickcheck2: '>=0.3.0 && <0.4'
  text: '>=0'
