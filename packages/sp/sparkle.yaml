homepage: http://github.com/tweag/sparkle#readme
changelog-type: markdown
hash: 3cb98e514e33b7f9a8d86255c5d38132b3355e7ad3a8354ebc1222e03be64d01
test-bench-deps: {}
maintainer: m@tweag.io
synopsis: Distributed Apache Spark applications in Haskell
changelog: ! "# Change Log\n\nAll notable changes to this project will be documented
  in this file.\n\nThe format is based on [Keep a Changelog](http://keepachangelog.com/).\n\n##
  [0.6] - 2017-07-16\n\n### Added\n\n* Support shipping anonymous objects that appear
  in inline-java\n  quasiquotes. You'll need to configure your app to use the Kryo\n
  \ serializer for this to work. See FAQ in README. This fixes #104.\n\n### Changed\n\n*
  Move `parallelize`, `textFile` and `binaryRecords` to `Context`\n  module.\n* Functions
  such as `sample` now use the [choice][hackage-choice]\n  library to describe the
  semantics of boolean arguments in their\n  types.\n* Use inline-java for RDD bindings
  under the hood.\n\n## [0.5] - 2017-02-21\n\n### Added\n\n* Bind to expm1\n* Add
  bindings to dayofmonth, current_timestamp and current_date.\n* Add support for the
  dataframe condition expressions\n* Add bindings to withColumnRenamed, columns, printSchema,
  Column.expr.\n* Bind DataFrame distinct.\n* Add bindings for log and log1p for Columns.\n*
  Add binding to Column.cast.\n* Add bindings getList and array for columns.\n* Add
  bindings: schema for rows, Metadata type, javaRDD, range, Row\n  getters and constructors,
  StrucType constructors, createDataFrame,\n  more DataType bindings.\n\n### Changed\n\n*
  Prevent Haskell exceptions from escaping apply.\n* Update sparkle to work with latest
  jni which uses ForeignPtr\n  for java references.\n* Move StructType and friends
  to modules StructField, DataType and Metadata.\n* Rename createRow, rowGet, rowSize,
  joinPairRDD to have the same names\n  as the java methods.\n\n## [0.4]\n\n### Added\n\n*
  Support for reading/writing Parquet files.\n* More `RDD` method bindings: `repartition`,
  `treeAggregate`,\n  `binaryRecords`, `aggregateByKey`, `mapPartitions`, `mapPartitionsWithIndex`.\n*
  More complete `DataFrame` support.\n* Intero support.\n* `stack ghci` support.\n*
  Support Template Haskell splices and `ANN` annotations that use\n  sparkle code.\n\n###
  Changed \n\n### Fixed\n\n* More reliable initialization of embedded shared library.\n*
  Cleanup temporary files properly.\n\n## [0.3] - 2016-12-27\n\n### Added\n\n* Dockerfile
  to build sparkle.\n* Compatibility with singletons-2.2.\n* Add the identity `Reify`/`Reflect`
  instances.\n* Change JNI bindings to use new `JNI.String` type, instead of\n  `ByteString`.
  This new type guarantees the invariants required by\n  the JNI API (null-termination
  in particular).\n\n### Changed\n\n* Remove `Reify`/`Reflect` instances for `Int`.
  Only instances for\n  sized types remain.\n\n### Fixed\n\n* Fix type in `Reify Int`
  making it incorrect.\n\n## [0.2.0] - 2016-12-13\n\n### Added\n\n* New binding: `getOrCreateSQLContext`.\n\n###
  Changed\n\n* `getOrCreate` renamed to `getOrCreateSparkContext`.\n\n## [0.1.0.1]
  - 2016-06-12\n\n### Added\n\n* More bindings to more `call*Method` JNI functions.\n\n###
  Changed\n\n* Use `getOrCreate` to get `SparkContext`.\n\n## [0.1.0] - 2016-04-25\n\n*
  Initial release\n"
basic-deps:
  streaming: ! '>=0.1'
  zip-archive: ! '>=0.2'
  bytestring: ! '>=0.10'
  base: ! '>=4.10.1.0 && <5'
  jni: ! '>=0.5.0 && <0.6'
  text: ! '>=1.2'
  filepath: ! '>=1.4'
  process: ! '>=1.2'
  jvm: ! '>=0.4.0.1 && <0.5'
  singletons: ! '>=2.0'
  distributed-closure: ! '>=0.3'
  binary: ! '>=0.7'
  regex-tdfa: ! '>=1.2'
  sparkle: -any
  jvm-streaming: ! '>=0.2'
  inline-java: ! '>=0.7.0 && <0.8'
  choice: ! '>=0.1'
  vector: ! '>=0.11'
all-versions:
- '0.1'
- '0.1.0.1'
- '0.2'
- '0.3'
- '0.4'
- '0.4.0.1'
- '0.4.0.2'
- '0.5.0.1'
- '0.6'
- '0.7'
author: Tweag I/O
latest: '0.7'
description-type: markdown
description: ! "# sparkle: Apache Spark applications in Haskell\n\n[![CircleCI](https://circleci.com/gh/tweag/sparkle.svg?style=svg)](https://circleci.com/gh/tweag/sparkle)\n\n*sparkle
  [spär′kəl]:* a library for writing resilient analytics\napplications in Haskell
  that scale to thousands of nodes, using\n[Spark][spark] and the rest of the Apache
  ecosystem under the hood.\nSee [this blog post][hello-sparkle] for the details.\n\n**This
  is an early tech preview, not production ready.**\n\n[spark]: http://spark.apache.org/\n[hello-sparkle]:
  http://www.tweag.io/posts/2016-02-25-hello-sparkle.html\n\n## Getting started\n\nThe
  tl;dr using the `hello` app as an example on your local machine:\n\n```\n$ stack
  build hello\n$ stack exec -- sparkle package sparkle-example-hello\n$ stack exec
  -- spark-submit --master 'local[1]' sparkle-example-hello.jar\n```\n\n## How to
  use\n\nTo run a Spark application the process is as follows:\n\n1. **create** an
  application in the `apps/` folder, in-repo or as\n   a submodule;\n1. **add** your
  app to `stack.yaml`;\n1. **build** the app;\n1. **package** your app into a deployable
  JAR container;\n1. **submit** it to a local or cluster deployment of Spark.\n\n**If
  you run into issues, read the Troubleshooting section below\n  first.**\n\n### Build\n\n####
  Linux\n\n**Requirements**\n\n* the [Stack][stack] build tool (version 1.2 or above);\n*
  either, the [Nix][nix] package manager,\n* or, OpenJDK, Gradle and Spark (version
  1.6) installed from your distro.\n\nTo build:\n\n```\n$ stack build\n```\n\nYou
  can optionally get Stack to download Spark and Gradle in a local\nsandbox (using
  [Nix][nix]) for good build results reproducibility.\n**This is the recommended way
  to build sparkle.** Alternatively,\nyou'll need these installed through your OS
  distribution's package\nmanager for the next steps (and you'll need to tell Stack
  how to find\nthe JVM header files and shared libraries).\n\nTo use Nix, set the
  following in your `~/.stack/config.yaml` (or pass\n`--nix` to all Stack commands,
  see the [Stack manual][stack-nix] for\nmore):\n\n```yaml\nnix:\n  enable: true\n```\n\n####
  Other platforms\n\nsparkle is not directly supported on non-Linux operating systems
  (e.g.\nMac OS X or Windows). But you can use Docker to run sparkle natively\ninside
  a container on those platforms. First,\n\n```\n$ stack docker pull\n```\n\nThen,
  just add `--docker` as an argument to *all* Stack commands, e.g.\n\n```\n$ stack
  --docker build\n```\n\nBy default, Stack uses the [tweag/sparkle][docker-build-img]
  build and\ntest Docker image, which includes everything that Nix does as in the\nLinux
  section. See the [Stack manual][stack-docker] for how to modify\nthe Docker settings.\n\n###
  Package\n\nTo package your app as a JAR directly consumable by Spark:\n\n```\n$
  stack exec -- sparkle package <app-executable-name>\n```\n\n### Submit\n\nFinally,
  to run your application, for example locally:\n\n```\n$ stack exec -- spark-submit
  --master 'local[1]' <app-executable-name>.jar\n```\n\nThe `<app-executable-name>`
  is any executable name as given in the\n`.cabal` file for your app. See apps in
  the [apps/](apps/) folder for\nexamples.\n\nSee [here][spark-submit] for other options,
  including launching\na [whole cluster from scratch on EC2][spark-ec2]. This\n[blog
  post][tweag-blog-haskell-paas] shows you how to get started on\nthe [Databricks
  hosted platform][databricks] and on\n[Amazon's Elastic MapReduce][aws-emr].\n\n[docker-build-img]:
  https://hub.docker.com/r/tweag/sparkle/\n[stack]: https://github.com/commercialhaskell/stack\n[stack-docker]:
  https://docs.haskellstack.org/en/stable/docker_integration/\n[stack-nix]: https://docs.haskellstack.org/en/stable/nix_integration/#configuration\n[spark-submit]:
  http://spark.apache.org/docs/1.6.2/submitting-applications.html\n[spark-ec2]: http://spark.apache.org/docs/1.6.2/ec2-scripts.html\n[nix]:
  http://nixos.org/nix\n[tweag-blog-haskell-paas]: http://www.tweag.io/posts/2016-06-20-haskell-compute-paas-with-sparkle.html\n[databricks]:
  https://databricks.com/\n[aws-emr]: https://aws.amazon.com/emr/\n\n## How it works\n\nsparkle
  is a tool for creating self-contained Spark applications in\nHaskell. Spark applications
  are typically distributed as JAR files, so\nthat's what sparkle creates. We embed
  Haskell native object code as\ncompiled by GHC in these JAR files, along with any
  shared library\nrequired by this object code to run. Spark dynamically loads this\nobject
  code into its address space at runtime and interacts with it\nvia the Java Native
  Interface (JNI).\n\n## Troubleshooting\n\n### `jvm` library or header files not
  found\n\nYou'll need to tell Stack where to find your local JVM installation.\nSomething
  like the following in your `~/.stack/config.yaml` should do\nthe trick, but check
  that the paths match up what's on your system:\n\n```\nextra-include-dirs: [/usr/lib/jvm/java-7-openjdk-amd64/include]\nextra-lib-dirs:
  [/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64/server]\n```\n\nOr use `--nix`:
  since it won't use your globally installed JDK, it\nwill have no trouble finding
  its own locally installed one.\n\n### Can't build sparkle on OS X\n\nOS X is not
  a supported platform for now. There are several issues to\nmake sparkle work on
  OS X, tracked\n[in this ticket](https://github.com/tweag/sparkle/issues/12).\n\n###
  Gradle <= 2.12 incompatible with JDK 9\n\nIf you're using JDK 9, note that you'll
  need to either downgrade to\nJDK 8 or update your Gradle version, since Gradle versions
  up to and\nincluding 2.12 are not compatible with JDK 9.\n\n### Anonymous classes
  in inline-java quasiquotes fail to deserialize\n\nWhen using inline-java, it is
  recommended to use the Kryo serializer,\nwhich is currently not the default in Spark
  but is faster anyways. If\nyou don't use the Kryo serializer, objects of anonymous
  class, which\narise e.g. when using Java 8 function literals,\n\n```haskell\nfoo
  :: RDD Int -> IO (RDD Bool)\nfoo rdd = [java| $rdd.map((Integer x) -> x.equals(0))
  |]\n```\n\nwon't be deserialized properly in multi-node setups. To avoid this\nproblem,
  switch to the Kryo serializer by setting the following\nconfiguration properties
  in your `SparkConf`:\n\n```haskell\ndo conf <- newSparkConf \"some spark app\"\n
  \  confSet conf \"spark.serializer\" \"org.apache.spark.serializer.KryoSerializer\"\n
  \  confSet conf \"spark.kryo.registrator\" \"io.tweag.sparkle.kryo.InlineJavaRegistrator\"\n```\n\nSee
  [#104](https://github.com/tweag/sparkle/issues/104) for more\ndetails.\n\n## License\n\nCopyright
  (c) 2015-2016 EURL Tweag.\n\nAll rights reserved.\n\nsparkle is free software, and
  may be redistributed under the terms\nspecified in the [LICENSE](LICENSE) file.\n\n##
  Sponsors\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n[![Tweag I/O](http://i.imgur.com/0HK8X4y.png)](http://tweag.io)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n[![LeapYear](http://i.imgur.com/t9VxRHn.png)](http://leapyear.io)\n\nsparkle
  is maintained by [Tweag I/O](http://tweag.io/).\n\nHave questions? Need help? Tweet
  at\n[@tweagio](http://twitter.com/tweagio).\n"
license-name: BSD3
