all-versions:
- 0.2.0
author: Sascha Grunert
basic-deps:
  base: '>=4.7 && <5'
  random: '>=0'
  split: '>=0'
changelog: ''
changelog-type: ''
description: "# Î·n\n## A tiny neural network \U0001F9E0\n\nThis small neural network
  is based on the\n[backpropagation](https://en.wikipedia.org/wiki/Backpropagation)
  algorithm.\n\n## Usage\n\nA minimal usage example would look like this:\n\n```haskell\nimport
  AI.Nn (new\n             ,predict\n             ,train)\n\nmain :: IO ()\nmain =
  do\n  {- Creates a new network with two inputs,\n     two hidden layers and one
  output -}\n  network <- new [2, 2, 1]\n\n  {- Train the network for a common logical
  AND,\n     until the maximum error of 0.01 is reached -}\n  let trainedNetwork =
  train 0.01 network [([0, 0], [0])\n                                          ,([0,
  1], [0])\n                                          ,([1, 0], [0])\n                                          ,([1,
  1], [1])]\n\n  {- Predict the learned values -}\n  let r00 = predict trainedNetwork
  [0, 0]\n  let r01 = predict trainedNetwork [0, 1]\n  let r10 = predict trainedNetwork
  [1, 0]\n  let r11 = predict trainedNetwork [1, 1]\n\n  {- Print the results -}\n
  \ putStrLn $ printf \"0 0 -> %.2f\" (head r00)\n  putStrLn $ printf \"0 1 -> %.2f\"
  (head r01)\n  putStrLn $ printf \"1 0 -> %.2f\" (head r10)\n  putStrLn $ printf
  \"1 1 -> %.2f\" (head r11)\n```\n\nThe result should be something like:\n\n```console\n0
  0 -> -0.02\n0 1 -> -0.02\n1 0 -> -0.01\n1 1 -> 1.00\n```\n\n## Hacking\nTo start
  hacking simply clone this repository and make sure that\n[stack](https://docs.haskellstack.org/en/stable/README/)
  is installed. Then\nsimply hack around and build the project with:\n\n```console\n>
  stack build --file-watch\n```\n\n## Contributing\nYou want to contribute to this
  project? Wow, thanks! So please just fork it and\nsend me a pull request.\n"
description-type: markdown
hash: 47a88f468afbfb43a6c5e75629555bc6d09e9850a12b82b5d9efa84cb0d14a5e
homepage: https://github.com/saschagrunert/nn#readme
latest: 0.2.0
license-name: MIT
maintainer: mail@saschagrunert.de
synopsis: A tiny neural network
test-bench-deps:
  base: '>=4.7 && <5'
  nn: '>=0'
  tasty: '>=0'
  tasty-hspec: '>=0'
  tasty-quickcheck: '>=0'
