all-versions:
- 1.0.0
- 1.0.1
- 1.1.0
- 1.1.1
- 1.2.0
- 2.0.0
author: Gabriella Gonzalez
basic-deps:
  aeson: '>=0'
  base: '>=4.15.0.0 && <5'
  bytestring: '>=0'
  containers: '>=0'
  filepath: '>=0'
  http-api-data: '>=0'
  http-client: '>=0'
  http-client-tls: '>=0'
  http-types: '>=0'
  openai: '>=0'
  servant: '>=0'
  servant-client: '>=0'
  servant-multipart-api: '>=0'
  servant-multipart-client: '>=0'
  text: '>=0'
  time: '>=0'
  unordered-containers: '>=0'
  vector: '>=0'
changelog: |
  2.0.0:

  - **BREAKING CHANGE**: Renamed `Item_InputMessage` to `Item_Input_Message` in `OpenAI.V1.Responses` for consistency with new constructors

    The `InputItem` data type in the Responses API has been updated to use a consistent naming scheme. If you are using the Responses API, you will need to update your code:

    ```haskell
    -- Before (v1.2.0):
    Item_InputMessage { role = ..., content = ..., status = ... }

    -- After (v2.0.0):
    Item_Input_Message { role = ..., content = ..., status = ... }
    ```

  - Add support for function tool calls in Responses API
    - New `InputItem` constructors: `Item_Input_Function_Call`, `Item_Input_Function_Call_Output`, `Item_Input_Item_Reference`
    - Add flattened tool JSON representation for Responses API compatibility
    - Export status constants: `statusIncomplete`, `statusCompleted`
    - Export tool choice constants: `toolChoiceNoneText`, `toolChoiceAutoText`, `toolChoiceRequiredText`

  - Code quality improvements:
    - Optimize `isFunctionField` using `HashSet` for O(1) lookups
    - Simplify `unflattenChoice` with guards
    - Extract magic strings as named constants

  1.2.0:

  - [`/v1/responses`: Add support for Responses API](https://platform.openai.com/docs/api-reference/responses)
  - Add `Tool_Web_Search`

  1.1.1:

  - [Remove timeout on default `ClientEnv`](https://github.com/MercuryTechnologies/openai/pull/55)
  - [`/v1/chat/completions`: Add support for Search](https://github.com/MercuryTechnologies/openai/pull/57)
  - [Fix CreateSpeech JSON instances, add new voices and optional instructions field](https://github.com/MercuryTechnologies/openai/pull/58)
  - [Correct `ToJSON` of `FileSearchResources`](https://github.com/MercuryTechnologies/openai/pull/49)
  - [New example app for tool-calling and chat-loop](https://github.com/MercuryTechnologies/openai/pull/60)

  1.1.0:

  - BREAKING CHANGE: Fix details representations for various types [[#44](https://github.com/MercuryTechnologies/openai/pull/44)] [[#45](https://github.com/MercuryTechnologies/openai/pull/45)] [[#50](https://github.com/MercuryTechnologies/openai/pull/50)] [[#51](https://github.com/MercuryTechnologies/openai/pull/51)]

    A few details-related fields were fixed to match the behavior of the OpenAI
    API.

  - Add `FromJSON`/`ToJSON` instances for all types [[#42](https://github.com/MercuryTechnologies/openai/pull/42)] [[#47](https://github.com/MercuryTechnologies/openai/pull/47)]

  - [Add support for `reasoning_effort` parameter in chat completions](https://github.com/MercuryTechnologies/openai/pull/48)

  1.0.1:

  - Include `README`
  - Include usage example

  1.0.0:

  - Initial release
changelog-type: markdown
description: |
  # `openai`

  This provides a binding to OpenAI's API using `servant`

  ## Example usage

  ```haskell
  {-# LANGUAGE DuplicateRecordFields #-}
  {-# LANGUAGE NamedFieldPuns        #-}
  {-# LANGUAGE OverloadedStrings     #-}
  {-# LANGUAGE OverloadedLists       #-}

  module Main where

  import Data.Foldable (traverse_)
  import OpenAI.V1
  import OpenAI.V1.Chat.Completions

  import qualified Data.Text as Text
  import qualified Data.Text.IO as Text.IO
  import qualified System.Environment as Environment

  main :: IO ()
  main = do
      key <- Environment.getEnv "OPENAI_KEY"

      clientEnv <- getClientEnv "https://api.openai.com"

      let Methods{ createChatCompletion } = makeMethods clientEnv (Text.pack key)

      text <- Text.IO.getLine

      ChatCompletionObject{ choices } <- createChatCompletion _CreateChatCompletion
          { messages = [ User{ content = [ Text{ text } ], name = Nothing } ]
          , model = "gpt-4o-mini"
          }

      let display Choice{ message } = Text.IO.putStrLn (messageToContent message)

      traverse_ display choices
  ```

  ### Responses API (JSON)

  ```haskell
  {-# LANGUAGE DuplicateRecordFields #-}
  {-# LANGUAGE NamedFieldPuns        #-}
  {-# LANGUAGE OverloadedStrings     #-}

  import qualified Data.Text as Text
  import qualified OpenAI.V1 as V1
  import qualified OpenAI.V1.Responses as Responses

  main :: IO ()
  main = do
      key <- System.Environment.getEnv "OPENAI_KEY"

      env <- V1.getClientEnv "https://api.openai.com"
      let V1.Methods{ createResponse } = V1.makeMethods env (Text.pack key) Nothing Nothing

      let req = Responses._CreateResponse
              { Responses.model = "gpt-5"
              , Responses.input = Just (Responses.Input
                  [ Responses.Item_Input_Message
                      { Responses.role = Responses.User
                      , Responses.content = [ Responses.Input_Text{ Responses.text = "Say hello in one sentence." } ]
                      , Responses.status = Nothing
                      }
                  ])
              }

      res <- createResponse req
      print res
  ```


  ## Setup

  ### Using Nix with Flakes (Recommended)

  This project uses Nix with flakes for development environment setup.

  1. Ensure you have Nix with flakes enabled
  2. Copy the sample environment file and configure your OpenAI API key:

  ```bash
  # Copy the sample environment file
  cp .envrc.sample .envrc
  ```

  3. Edit the `.envrc` file and replace the placeholder API key with your actual key

  4. Use direnv to automatically load the development environment:

  ```bash
  # Install direnv if you haven't already
  # macOS: brew install direnv
  # Linux: your-package-manager install direnv

  # Enable direnv hook in your shell
  eval "$(direnv hook bash)" # or zsh, fish, etc.

  # Clone the repository and enter the directory
  git clone https://github.com/MercuryTechnologies/openai.git
  cd openai

  # Allow direnv (this will automatically load the environment)
  direnv allow
  ```

  ### Manual Setup

  Without Nix:

  ```bash
  # Clone the repository
  git clone https://github.com/MercuryTechnologies/openai.git
  cd openai

  # Build with cabal
  cabal build
  ```

  ## Environment Variables

  Set your OpenAI API key as an environment variable:

  ```bash
  # Option 1: Set directly in your shell
  export OPENAI_KEY="your-openai-api-key"

  # Option 2: Using .envrc with direnv (recommended)
  (umask 077; cp .envrc.sample .envrc)
  # Edit .envrc to add your API key
  direnv allow
  ```

  The API key is needed for running the test suite and example program.

  ## Testing

  Run the test suite:

  ```bash
  cabal test
  ```

  The test suite is in the `tasty/` directory with test data located in `tasty/data/`.

  ## Running the Example

  ```bash
  # Make sure your API key is set (either via .envrc or export)
  # If using direnv with proper .envrc setup, this happens automatically

  # Build and run the example
  cabal run openai-example
  ```
description-type: markdown
hash: b5103b10ddbb9e4dcafdeb22c3deafb0608153240e2ad905713feb0dcdb0892e
homepage: ''
latest: 2.0.0
license-name: BSD-3-Clause
maintainer: GenuineGabriella@gmail.com
synopsis: Servant bindings to OpenAI
test-bench-deps:
  aeson: '>=0'
  base: '>=0'
  http-client: '>=0'
  http-client-tls: '>=0'
  openai: '>=0'
  servant-client: '>=0'
  tasty: '>=0'
  tasty-hunit: '>=0'
  text: '>=0'
