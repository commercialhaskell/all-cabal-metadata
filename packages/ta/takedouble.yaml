homepage: https://github.com/shapr/takedouble
changelog-type: markdown
hash: a134d5db1b70382f8406020c1a6c76227c99e7f0805d5d9b1982f0479f733cdc
test-bench-deps:
  extra: -any
  unix: -any
  base: '>=4.11 && <5'
  filepath: -any
  hedgehog: -any
  takedouble: -any
  temporary: -any
  directory: -any
maintainer: Shae Erisson
synopsis: duplicate file finder
changelog: |
  # Revision history for takedouble

  ## 0.1.0.0 -- 2022-06-22

  * First version. Released on an unsuspecting world.
    In this first episode, files are lazily compared by filesize, then by first and last 4 kilobytes, and then by entire file contents.
    Duplicates are reported as a list of results.
basic-deps:
  bytestring: -any
  extra: -any
  unix: -any
  base: '>=4.11 && <5'
  filepath: -any
  takedouble: -any
  directory: -any
all-versions:
- 0.0.1.1
author: Shae Erisson
latest: 0.0.1.1
description-type: markdown
description: |
  # takedouble
  TakeDouble is a duplicate file finder that reads and checks the filesize and first 4k and last 4k of a file and only then checks the full file to find duplicates.

  # How do I make it go?
  You can use nix or cabal to build this.

  `cabal build` should produce a binary. (use [ghcup](https://www.haskell.org/ghcup/) to install cabal and the latest GHC version).

  After that, `takedouble <dirname>` so you could use `takedouble ~/` for example.

  # Is it Fast?

  On my ThinkPad with six Xeon cores, 128GB RAM, and a 1TB Samsung 970 Pro NVMe (via PCIe 3.0), I can check 34393 uncached files in 6.4 seconds.
  A second run on the same directory takes 2.8 seconds due to file metainfo cached in memory.
license-name: BSD-3-Clause
