homepage: https://github.com/shapr/takedouble
changelog-type: markdown
hash: 7e3acd24f2d27ab2ecee0f7f0b34c2e9240f6cc3a8670307549660707d5a7566
test-bench-deps:
  extra: -any
  unix: -any
  base: '>=4.11 && <5'
  filepath: -any
  hedgehog: -any
  takedouble: -any
  temporary: -any
  directory: -any
maintainer: Shae Erisson
synopsis: duplicate file finder
changelog: |
  # Revision history for takedouble

  ## 0.1.0.0 -- 2022-06-22

  * First version. Released on an unsuspecting world.
    In this first episode, files are lazily compared by filesize, then by first and last 4 kilobytes, and then by entire file contents.
    Duplicates are reported as a list of results.
basic-deps:
  bytestring: -any
  extra: -any
  unix: -any
  base: '>=4.11 && <5'
  filepath: -any
  takedouble: -any
  filepattern: -any
  directory: -any
all-versions:
- 0.0.1.1
- 0.0.2.0
author: Shae Erisson
latest: 0.0.2.0
description-type: markdown
description: |
  # takedouble
  TakeDouble is a duplicate file finder that reads and checks the filesize and first 4k and last 4k of a file and only then checks the full file to find duplicates.

  # How do I make it go?
  You can use nix or cabal to build this.

  `cabal build` should produce a binary. (use [ghcup](https://www.haskell.org/ghcup/) to install cabal and the latest GHC version).

  After that, `takedouble <dirname>` so you could use `takedouble ~/` for example.

  If there are common files you'd like to exclude (such as .git directories) you can pass a glob to exclude any matching patterns from the output.

  For example

  `takedouble <dirname> "**/.git/**"`

  # Is it Fast?

  On my ThinkPad with six Xeon cores, 128GB RAM, and a 1TB Samsung 970 Pro NVMe (via PCIe 3.0), I can check 34393 uncached files in 6.4 seconds.
  A second run on the same directory takes 2.8 seconds due to file metainfo cached in memory.
license-name: BSD-3-Clause
