all-versions:
- '0.1'
author: Bodigrim
basic-deps:
  base: '>=4.11 && <5'
  containers: '>=0.5.11 && <0.8'
  deepseq: '>=1.4 && <1.6'
  infinite-list: '>=0.1 && <0.2'
  regression-simple: '>=0.2.1 && <0.3'
  tasty: '>=1.4 && <1.6'
  tasty-bench: '>=0.3.4 && <0.5'
changelog: |
  ## 0.1

  * Initial release.
changelog-type: markdown
description: |
  # tasty-bench-fit

  Benchmark a given function for variable input sizes and find out its time complexity.

  ```haskell
  > fit $ mkFitConfig (\x -> sum [1..x]) (10, 10000)
  1.2153e-8 * x
  > fit $ mkFitConfig (\x -> Data.List.nub [1..x]) (10, 10000)
  2.8369e-9 * x ^ 2
  > fit $ mkFitConfig (\x -> Data.List.sort $ take (fromIntegral x) $ iterate (\n -> n * 6364136223846793005 + 1) (1 :: Int)) (10, 100000)
  5.2990e-8 * x * log x
  ```

  One can usually get reliable results for functions, which do not allocate much: like in-place vector sort or fused list operations like `sum [1..x]`.

  Unfortunately, fitting functions, which allocate a lot, is likely to be disappointing: GC kicks in irregularly depending on nursery and heap sizes and often skews observations beyond any recognition. Consider running such measurements with `-O0` or in `ghci` prompt. This is how the usage example above was generated. Without optimizations your program allocates much more and triggers GC regularly, somewhat evening out its effect.
description-type: markdown
hash: 7577c597abad1ab33e5ae18ec978d4b4cfa34d6f3c87b5640f9701d46f34edc7
homepage: https://github.com/Bodigrim/tasty-bench-fit
latest: '0.1'
license-name: MIT
maintainer: andrew.lelechenko@gmail.com
synopsis: Determine time complexity of a given function
test-bench-deps:
  base: '>=0'
  containers: <0.7
  tasty: '>=0'
  tasty-bench: '>=0'
  tasty-bench-fit: '>=0'
  tasty-expected-failure: <0.13
  tasty-quickcheck: <0.11
