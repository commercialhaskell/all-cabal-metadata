homepage: https://github.com/ndmitchell/tagsoup#readme
changelog-type: text
hash: 07520669c752af2deea09499ff518e0a1dee44f1f3a40e273f41223b46252126
test-bench-deps:
  bytestring: -any
  base: ==4.*
  time: -any
  process: -any
  deepseq: ! '>=1.1'
  QuickCheck: ! '>=2.4'
  tagsoup: -any
  directory: -any
maintainer: Neil Mitchell <ndmitchell@gmail.com>
synopsis: Parsing and extracting information from (possibly malformed) HTML/XML documents
changelog: ! "Changelog for TagSoup\n\n0.14.7, released 2018-09-18\n    #75, escape
  single quote (') characters as &#39;\n    #72, update some dead URLs\n0.14.6, released
  2018-02-27\n    Fix up source positions on bogus <! markers\n0.14.5, released 2018-02-27\n
  \   Fix up source positions on bogus <? markers\n0.14.4, released 2018-02-26\n    #71,
  fix up source positions on bogus comments\n0.14.3, released 2018-01-21\n    #70,
  fix up source positions on comments\n0.14.2, released 2017-11-30\n    #66, make
  sure positions are correct for lone & characters\n0.14.1, released 2017-02-25\n
  \   #63, add maybeAttrib\n0.14, released 2016-06-11\n    #14, eliminate Text.HTML.Download\n0.13.10,
  released 2016-05-06\n    #51, improve the Haddock documentation\n    #52, fix some
  > 16bit HTML entities\n0.13.9, released 2016-03-15\n    #50, fix a space leak\n
  \   #36, fix the demo examples\n    #35, make IsString a superclass of StringLike\n
  \   #33, make flattenTree O(n) instead of O(n^2)\n0.13.8, released 2016-01-10\n
  \   #30, add parse/render functions directly to the Tree module\n0.13.7, released
  2016-01-09\n    #32, make sure upper case &#X works in lookupEntity\n0.13.6, released
  2015-12-23\n    #28, some named entities require a trailing semicolon (e.g. mid)\n0.13.5,
  released 2015-10-30\n    #26, rename the test program to test-tagsoup\n0.13.4, released
  2015-10-26\n    #24, add isTagComment function\n    Update the copyright year\n0.13.3,
  released 2014-10-01\n    Work on GHC 7.9\n0.13.2, released 2014-08-08\n    Remove
  all package upper bounds\n    Allow QuickCheck-2.6\n0.13.1, released 2014-01-20\n
  \   #562, treat <script> specially as per HTML5\n0.13, released 2013-08-06\n    #616,
  extend to all HTML5 entities\n    Optimise lookupNamedEntity\n    Replace escapeXMLChar
  with escapeXML\n    Change all Entity functions to return String, not Int or Char\n0.12.7,
  released 2012-08-19\n    Allow deepseq-1.3\n    Allow QuickCheck-2.5\n    Support
  bytestring-0.10 (NFData instances added)\n0.12.6, released 2011-12-11\n    #515,
  don't crash on malformed characters (use ? instead)\n0.12.5, released 2011-11-17\n
  \   Add optRawTag to Render, to ensure script's don't get escaped\n0.12.4, released
  2011-11-05\n    #487, fix the behaviour of ~== for TagComment and others\n0.12.3,
  released 2011-08-12\n    GHC 7.2 compatibility\n0.12.2, released 2011-06-12\n    Add
  StringLike instances for Text\n0.12.1, released 2011-06-01\n    Add parseOptionsEntities
  and improve documentation\n0.12, released 2010-12-12\n    Upgrade to QuickCheck
  2.4.*\n    Export toTagRep\n    Make the -download flag off by default\n    Eliminate
  HTTP dependency\n    Eliminate mtl dependency\n0.11.1, released 2010-10-04\n    Support
  --flags=-download to eliminate the network dependency\n0.11, released 2010-09-12\n
  \   #326, <a \"foo\"> is no longer treated as an attribute\n    Add Eq/Ord instances
  to Tree\n    Don't mark Text.HTML.TagSoup.Tree as preliminary\n    #325, \\r should
  be treated as whitespace\n0.10.1, released 2010-07-22\n    #322, don't change '
  to &apos; in render (do what the docs say)\n0.10, released 2010-05-26\n    Improve
  the cabal file, make the test program off by default\n    Expose Text.HTML.TagSoup.Match
  again (was hidden accidentally)\n0.9, released 2010-05-09\n    #294, let optEntityData
  see if there was a ';' (CHANGES API)\n    Numeric/hex entities in attributes were
  misparsed\n    #149, add escapeHTML function\n0.8, released 2010-01-07\n    Parser
  now based on HTML5 specification\n    Tag is now parameterised by the string type\n0.6,
  released 2008-04-23\n    Addition of Text.HTML.TagSoup.Tree and Text.HTML.TagSoup.Render\n
  \   Text.HTML.TagSoup.Parser.Options renamed to ParseOptions\n    Text.HTML.TagSoup.Parser.options
  renamed to parseOptions\n0.4, released 2008-01-14\n    Changelog started\n"
basic-deps:
  bytestring: -any
  base: ==4.*
  text: -any
  containers: -any
all-versions:
- '0.1'
- '0.4'
- '0.6'
- '0.8'
- '0.9'
- '0.10'
- '0.10.1'
- '0.11'
- '0.11.1'
- '0.12'
- '0.12.1'
- '0.12.2'
- '0.12.3'
- '0.12.4'
- '0.12.5'
- '0.12.6'
- '0.12.7'
- '0.12.8'
- '0.13'
- '0.13.1'
- '0.13.2'
- '0.13.3'
- '0.13.4'
- '0.13.5'
- '0.13.6'
- '0.13.7'
- '0.13.8'
- '0.13.9'
- '0.13.10'
- '0.14'
- '0.14.1'
- '0.14.2'
- '0.14.3'
- '0.14.4'
- '0.14.5'
- '0.14.6'
- '0.14.7'
author: Neil Mitchell <ndmitchell@gmail.com>
latest: '0.14.7'
description-type: markdown
description: ! "# TagSoup [![Hackage version](https://img.shields.io/hackage/v/tagsoup.svg?label=Hackage)](https://hackage.haskell.org/package/tagsoup)
  [![Stackage version](https://www.stackage.org/package/tagsoup/badge/nightly?label=Stackage)](https://www.stackage.org/package/tagsoup)
  [![Linux Build Status](https://img.shields.io/travis/ndmitchell/tagsoup/master.svg?label=Linux%20build)](https://travis-ci.org/ndmitchell/tagsoup)
  [![Windows Build Status](https://img.shields.io/appveyor/ci/ndmitchell/tagsoup/master.svg?label=Windows%20build)](https://ci.appveyor.com/project/ndmitchell/tagsoup)\n\nTagSoup
  is a library for parsing HTML/XML. It supports the HTML 5 specification, and can
  be used to parse either well-formed XML, or unstructured and malformed HTML from
  the web. The library also provides useful functions to extract information from
  an HTML document, making it ideal for screen-scraping.\n\nThe library provides a
  basic data type for a list of unstructured tags, a parser to convert HTML into this
  tag type, and useful functions and combinators for finding and extracting information.
  This document gives two particular examples of scraping information from the web,
  while a few more may be found in the [Sample](https://github.com/ndmitchell/tagsoup/blob/master/test/TagSoup/Sample.hs)
  file from the source repository. The examples we give are:\n\n* Obtaining the last
  modified date of the Haskell wiki\n* Obtaining a list of Simon Peyton Jones' latest
  papers\n* A brief overview of some other examples\n\nThe intial version of this
  library was written in Javascript and has been used for various commercial projects
  involving screen scraping. In the examples general hints on screen scraping are
  included, learnt from bitter experience. It should be noted that if you depend on
  data which someone else may change at any given time, you may be in for a shock!\n\nThis
  library was written without knowledge of the Java version of [TagSoup](https://github.com/jukka/tagsoup).
  They have made a very different design decision: to ensure default attributes are
  present and to properly nest parsed tags. We do not do this - tags are merely a
  list devoid of nesting information.\n\n\n#### Acknowledgements\n\nThanks to Mike
  Dodds for persuading me to write this up as a library. Thanks to many people for
  debugging and code contributions, including: Gleb Alexeev, Ketil Malde, Conrad Parker,
  Henning Thielemann, Dino Morelli, Emily Mitchell, Gwern Branwen.\n\n\n## Potential
  Bugs\n\nThere are two things that may go wrong with these examples:\n\n* _The Websites
  being scraped may change._ There is nothing I can do about this, but if you suspect
  this is the case let me know, and I'll update the examples and tutorials. I have
  already done so several times, it's only a few minutes work.\n* _The `openURL` method
  may not work._ This happens quite regularly, and depending on your server, proxies
  and direction of the wind, they may not work. The solution is to use `wget` to download
  the page locally, then use `readFile` instead. Hopefully a decent Haskell HTTP library
  will emerge, and that can be used instead.\n\n\n## Last modified date of Haskell
  wiki\n\nOur goal is to develop a program that displays the date that the wiki at\n[`wiki.haskell.org`](http://wiki.haskell.org/Haskell)
  was last modified. This\nexample covers all the basics in designing a basic web-scraping
  application.\n\n### Finding the Page\n\nWe first need to find where the information
  is displayed and in what format.\nTaking a look at the [front web page](http://wiki.haskell.org/Haskell),
  when\nnot logged in, we see:\n\n```html\n<ul id=\"f-list\">\n  <li id=\"lastmod\">
  This page was last modified on 9 September 2013, at 22:38.</li>\n  <li id=\"copyright\">Recent
  content is available under <a href=\"/HaskellWiki:Copyrights\" title=\"HaskellWiki:Copyrights\">a
  simple permissive license</a>.</li>\n  <li id=\"privacy\"><a href=\"/HaskellWiki:Privacy_policy\"
  title=\"HaskellWiki:Privacy policy\">Privacy policy</a></li>\n  <li id=\"about\"><a
  href=\"/HaskellWiki:About\" title=\"HaskellWiki:About\">About HaskellWiki</a></li>\n
  \ <li id=\"disclaimer\"><a href=\"/HaskellWiki:General_disclaimer\" title=\"HaskellWiki:General
  disclaimer\">Disclaimers</a></li>\n</ul>\n```\n\nSo, we see that the last modified
  date is available. This leads us to rule 1:\n\n**Rule 1:** Scrape from what the
  page returns, not what a browser renders, or what view-source gives.\n\nSome web
  servers will serve different content depending on the user agent, some browsers
  will have scripting modify their displayed HTML, some pages will display differently
  depending on your cookies. Before you can start to figure out how to start scraping,
  first decide what the input to your program will be. There are two ways to get the
  page as it will appear to your program.\n\n#### Using the HTTP package\n\nWe can
  write a simple HTTP downloader with using the [HTTP package](http://hackage.haskell.org/package/HTTP):\n\n```haskell\nmodule
  Main where\n\nimport Network.HTTP\n\nopenURL :: String -> IO String\nopenURL x =
  getResponseBody =<< simpleHTTP (getRequest x)\n\nmain :: IO ()\nmain = do\n    src
  <- openURL \"http://wiki.haskell.org/Haskell\"\n    writeFile \"temp.htm\" src\n```\n\nNow
  open `temp.htm`, find the fragment of HTML containing the hit count, and examine
  it.\n\n#### Using the `tagsoup` Program\n\nTagSoup installs both as a library and
  a program. The program contains all the\nexamples mentioned on this page, along
  with a few other useful functions. In\norder to download a URL to a file:\n\n```bash\n$
  tagsoup grab http://wiki.haskell.org/Haskell > temp.htm\n```\n\n### Finding the
  Information\n\nNow we examine both the fragment that contains our snippet of information,
  and\nthe wider page. What does the fragment have that nothing else has? What\nalgorithm
  would we use to obtain that particular element? How can we still\nreturn the element
  as the content changes? What if the design changes? But\nwait, before going any
  further:\n\n**Rule 2:** Do not be robust to design changes, do not even consider
  the possibility when writing the code.\n\nIf the user changes their website, they
  will do so in unpredictable ways. They may move the page, they may put the information
  somewhere else, they may remove the information entirely. If you want something
  robust talk to the site owner, or buy the data from someone. If you try and think
  about design changes, you will complicate your design, and it still won't work.
  It is better to write an extraction method quickly, and happily rewrite it when
  things change.\n\nSo now, let's consider the fragment from above. It is useful to
  find a tag\nwhich is unique just above your snippet - something with a nice `id`
  or `class`\nattribute - something which is unlikely to occur multiple times. In
  the above\nexample, an `id` with value  `lastmod` seems perfect.\n\n```haskell\nmodule
  Main where\n\nimport Data.Char\nimport Network.HTTP\nimport Text.HTML.TagSoup\n\nopenURL
  :: String -> IO String\nopenURL x = getResponseBody =<< simpleHTTP (getRequest x)\n\nhaskellLastModifiedDateTime
  :: IO ()\nhaskellLastModifiedDateTime = do\n    src <- openURL \"http://wiki.haskell.org/Haskell\"\n
  \   let lastModifiedDateTime = fromFooter $ parseTags src\n    putStrLn $ \"wiki.haskell.org
  was last modified on \" ++ lastModifiedDateTime\n    where fromFooter = unwords
  . drop 6 . words . innerText . take 2 . dropWhile (~/= \"<li id=lastmod>\")\n\nmain
  :: IO ()\nmain = haskellLastModifiedDateTime\n```\n\nNow we start writing the code!
  The first thing to do is open the required URL, then we parse the code into a list
  of `Tag`s with `parseTags`. The `fromFooter` function does the interesting thing,
  and can be read right to left:\n\n* First we throw away everything (`dropWhile`)
  until we get to an `li` tag\n  containing `id=lastmod`. The `(~==)` and `(~/=)`
  operators are different from\nstandard equality and inequality since they allow
  additional attributes to be\npresent. We write `\"<li id=lastmod>\"` as syntactic
  sugar for `TagOpen \"li\"\n[(\"id\",\"lastmod\")]`. If we just wanted any open tag
  with the given `id`\nattribute we could have written `(~== TagOpen \"\" [(\"id\",\"lastmod\")])`
  and this\nwould have matched.  Any empty strings in the second element of the match
  are\nconsidered as wildcards.\n* Next we take two elements: the `<li>` tag and the
  text node immediately\n  following.\n* We call the `innerText` function to get all
  the text values from inside,\n  which will just be the text node following the `lastmod`.\n*
  We split the string into a series of words and drop the first six, i.e. the\n  words
  `This`, `page`, `was`, `last`, `modified` and `on`\n* We reassemble the remaining
  words into the resulting string `9 September\n  2013, at 22:38.`\n\nThis code may
  seem slightly messy, and indeed it is - often that is the nature of extracting information
  from a tag soup.\n\n**Rule 3:** TagSoup is for extracting information where structure
  has been lost, use more structured information if it is available.\n\n\n## Simon's
  Papers\n\nOur next very important task is to extract a list of all Simon Peyton
  Jones' recent research papers off his [home page](http://research.microsoft.com/en-us/people/simonpj/).
  The largest change to the previous example is that now we desire a list of papers,
  rather than just a single result.\n\nAs before we first start by writing a simple
  program that downloads the appropriate page, and look for common patterns. This
  time we want to look for all patterns which occur every time a paper is mentioned,
  but no where else. The other difference from last time is that previous we grabbed
  an automatically generated piece of information - this time the information is entered
  in a more freeform way by a human.\n\nFirst we spot that the page helpfully has
  named anchors, there is a current work anchor, and after that is one for Haskell.
  We can extract all the information between them with a simple `take`/`drop` pair:\n\n```haskell\ntakeWhile
  (~/= \"<a name=haskell>\") $\ndrop 5 $ dropWhile (~/= \"<a name=current>\") tags\n```\n\nThis
  code drops until you get to the \"current\" section, then takes until you get to
  the \"haskell\" section, ensuring we only look at the important bit of the page.
  Next we want to find all hyperlinks within this section:\n\n```haskell\nmap f $
  sections (~== \"<A>\") $ ...\n```\n\nRemember that the function to select all tags
  with name \"A\" could have been written as `(~== TagOpen \"A\" [])`, or alternatively
  `isTagOpenName \"A\"`. Afterwards we map each item with an `f` function. This function
  needs to take the tags starting just after the link, and find the text inside the
  link.\n\n```haskell\nf = dequote . unwords . words . fromTagText . head . filter
  isTagText\n```\n\nHere the complexity of interfacing to human written markup comes
  through. Some of the links are in italic, some are not - the `filter` drops all
  those that are not, until we find a pure text node. The `unwords . words` deletes
  all multiple spaces, replaces tabs and newlines with spaces and trims the front
  and back - a neat trick when dealing with text which has spacing at the source code
  but not when displayed. The final thing to take account of is that some papers are
  given with quotes around the name, some are not - dequote will remove the quotes
  if they exist.\n\nFor completeness, we now present the entire example:\n\n```haskell\nmodule
  Main where\n\nimport Network.HTTP\nimport Text.HTML.TagSoup\n\nopenURL :: String
  -> IO String\nopenURL x = getResponseBody =<< simpleHTTP (getRequest x)\n\nspjPapers
  :: IO ()\nspjPapers = do\n        tags <- parseTags <$> openURL \"http://research.microsoft.com/en-us/people/simonpj/\"\n
  \       let links = map f $ sections (~== \"<A>\") $\n                    takeWhile
  (~/= \"<a name=haskell>\") $\n                    drop 5 $ dropWhile (~/= \"<a name=current>\")
  tags\n        putStr $ unlines links\n    where\n        f :: [Tag String] -> String\n
  \       f = dequote . unwords . words . fromTagText . head . filter isTagText\n\n
  \       dequote ('\\\"':xs) | last xs == '\\\"' = init xs\n        dequote x = x\n\nmain
  :: IO ()\nmain = spjPapers\n```\n\n## Other Examples\n\nSeveral more examples are
  given in the Example file, including obtaining the (short) list of papers from my
  site, getting the current time and a basic XML validator. All can be invoked using
  the `tagsoup` executable program. All use very much the same style as presented
  here - writing screen scrapers follow a standard pattern. We present the code from
  two for enjoyment only.\n\n### My Papers\n\n```haskell\nmodule Main where\n\nimport
  Network.HTTP\nimport Text.HTML.TagSoup\n\nopenURL :: String -> IO String\nopenURL
  x = getResponseBody =<< simpleHTTP (getRequest x)\n\nndmPapers :: IO ()\nndmPapers
  = do\n        tags <- parseTags <$> openURL \"http://community.haskell.org/~ndm/downloads/\"\n
  \       let papers = map f $ sections (~== \"<li class=paper>\") tags\n        putStr
  $ unlines papers\n    where\n        f :: [Tag String] -> String\n        f xs =
  fromTagText (xs !! 2)\n\nmain :: IO ()\nmain = ndmPapers\n```\n\n### UK Time\n\n```haskell\nmodule
  Main where\n\nimport Network.HTTP\nimport Text.HTML.TagSoup\n\nopenURL :: String
  -> IO String\nopenURL x = getResponseBody =<< simpleHTTP (getRequest x)\n\ncurrentTime
  :: IO ()\ncurrentTime = do\n    tags <- parseTags <$> openURL \"http://www.timeanddate.com/worldclock/uk/london\"\n
  \   let time = fromTagText (dropWhile (~/= \"<span id=ct>\") tags !! 1)\n    putStrLn
  time\n\nmain :: IO ()\nmain = currentTime\n```\n\n## Other Examples\nIn [Sample.hs](https://github.com/ndmitchell/tagsoup/blob/master/test/TagSoup/Sample.hs)\nthe
  following additional examples are listed:\n\n- Google Tech News\n- Package list
  form Hackage\n- Print names of story contributors on sequence.complete.org\n- Parse
  rows of a table\n\n\n## Related Projects\n\n* [TagSoup for Java](https://github.com/jukka/tagsoup)
  - an independently written malformed HTML parser for Java.\n* [HXT: Haskell XML
  Toolbox](http://www.fh-wedel.de/~si/HXmlToolbox/) - a more comprehensive XML parser,
  giving the option of using TagSoup as a lexer.\n* [Other Related Work](http://www.fh-wedel.de/~si/HXmlToolbox/#rel)
  - as described on the HXT pages.\n* [Using TagSoup with Parsec](http://therning.org/magnus/posts/2008-08-08-367-tagsoup-meet-parsec.html)
  - a nice combination of Haskell libraries.\n* [tagsoup-parsec](https://hackage.haskell.org/package/tagsoup-parsec)
  - a library for easily using TagSoup as a token type in Parsec.\n* [tagsoup-megaparsec](https://hackage.haskell.org/package/tagsoup-megaparsec)
  - a library for easily using TagSoup as a token type in Megaparsec.\n* [WraXML](https://hackage.haskell.org/packages/archive/wraxml/latest/doc/html/Text-XML-WraXML-Tree-TagSoup.html)
  - construct a lazy tree from TagSoup lexemes.\n"
license-name: BSD3
