all-versions:
- 0.0.1.0
- 0.0.1.1
- 0.0.2.0
- 0.0.3.0
- 0.0.4.0
author: phoityne.hs@gmail.com
basic-deps:
  base: ^>=4.21.0.0
  optparse-applicative: '>=0'
  pms-application-service: '>=0'
  pms-domain-model: '>=0'
  pms-domain-service: '>=0'
  pms-infra-cmdrun: '>=0'
  pms-infra-procspawn: '>=0'
  pms-infra-watch: '>=0'
  pms-infrastructure: '>=0'
  pms-ui-notification: '>=0'
  pms-ui-request: '>=0'
  pms-ui-response: '>=0'
  safe-exceptions: '>=0'
changelog: |
  # Revision history for pty-mcp-server

  ## 0.0.4.0 -- 2025-06-29

  * Added prompts interface.

  ## 0.0.3.0 -- 2025-06-22

  * Added file change monitoring for tools-list.json.

  ## 0.0.2.0 -- 2025-06-15

  * Updated the README.

  ## 0.0.1.0 -- 2025-05-31

  * First version.
changelog-type: markdown
description: "# pty-mcp-server\n\n----\n## ⚠️ Caution\n\n**Do not grant unrestricted
  control to AI.**  \nUnsupervised use or misuse may lead to unintended consequences.
  \ \nAll AI systems must remain strictly under human oversight and control.  \nUse
  responsibly, with full awareness and at your own risk.  \n\n----\n## \U0001F4D8
  Overview\n\n**`pty-mcp-server`** is a Haskell implementation of an **MCP (Model
  Context Protocol) server** that enables AI agents to dynamically acquire and control
  **PTY (pseudo-terminal) sessions**, allowing interaction with real system environments
  through terminal-based interfaces.\n\nThe server communicates exclusively via **standard
  input/output (stdio)**, ensuring simple and secure integration with MCP clients.
  Through this interface, AI agents can execute commands, retrieve system states,
  and apply configurations—just as a human operator would through a terminal.\n\n---\n\n###
  \U0001F3AF Purpose\n\n- Provide AI agents with **TTY-based control capabilities**
  \ \n- Enable **automated configuration, inspection, and operation** using CLI tools
  \ \n- Facilitate **AI-driven workflows for system development, diagnostics, and
  remote interaction**  \n- Allow AI agents to access and operate on **systems beyond
  the reach of static scripts or APIs**  \n- Support **Infrastructure as Code (IaC)**
  scenarios requiring **interactive or stateful terminal workflows**  \n- Assist in
  **system integration** across heterogeneous environments and legacy systems  \n-
  Empower **AI agents to support DevOps, IaC, and integration pipelines** by operating
  tools that require human-like terminal interaction  \n\n---\n\n### \U0001F527 Example
  Use Cases\n\n- **Dynamic execution of CLI tools** that require a PTY environment
  \ \n  (e.g., embedded systems over serial or SSH-based terminals)  \n- **REPL automation**:
  driving GHCi or other CLI-based interactive interpreters  \n- **Interactive debugging**
  of Haskell applications or shell-based workflows  \n- **System diagnostics** through
  scripted or interactive bash sessions  \n- **Remote server management** using SSH
  \ \n- **Hands-on system operation** where CLI behavior cannot be emulated via non-interactive
  scripting  \n- **Network device interaction**: configuring routers, switches, or
  appliances via console  \n- **AI-assisted IaC workflows**: executing Terraform,
  Ansible, or shell-based deploy scripts that involve prompts, state reconciliation,
  or real-time input  \n- **AI-driven system integration testing** across multiple
  environments and CLI tools  \n- **Legacy system automation** where GUI/API is unavailable
  and only terminal interaction is supported  \n\n\n---\n\n## User Guide (Usage and
  Setup)\n\n### Features\n\n`pty-mcp-server` provides the following built-in tools
  for powerful and flexible automation:\n\n#### Available Tools\n- **`pty-connect`**
  \ \n  Launches any command through a PTY interface with optional arguments.  \n
  \ Great for general-purpose terminal automation.\n\n- **`pty-terminate`**  \n  Forcefully
  terminates an active pseudo-terminal (PTY) connection.\n\n- **`pty-message`**  \n
  \ Sends input to an existing PTY session (e.g., `df -k`) without needing full context
  of the current terminal state.  \n  Abstracts interaction in a programmable way.\n\n-
  **`pty-bash`**  \n  Starts an interactive Bash shell (`/bin/bash -i -l`) in a pseudo-terminal.
  \ \n  Empowers AI to execute shell commands like a real user.\n\n- **`pty-ssh`**
  \ \n  Opens a remote SSH session via PTY, enabling access to remote systems.  \n
  \ Accepts user/host and SSH flags as arguments.\n\n- **`pty-cabal`**  \n  Launches
  a cabal repl session within a specified project directory, loading a target Haskell
  file.  \n  Supports argument passing and live code interaction.\n\n- **`pty-stack`**
  \ \n  Launches a stack repl session within a specified project directory, loading
  a target Haskell file.  \n  Supports argument passing and live code interaction.\n\n-
  **`pty-ghci`**  \n  Launches a GHCi session within a specified project directory,
  loading a target Haskell file.  \n  Supports argument passing and live code interaction.\n\n-
  **`proc-spawn`**  \n  Spawns an external process using the specified arguments and
  enables interactive communication via standard input and output. Unlike PTY-based
  execution, this communicates directly with the process using the runProcess function
  without allocating a pseudo-terminal. Suitable for non-TUI, stdin/stdout-based interactive
  programs.\n\n- **`proc-terminate`**  \n  Forcefully terminates a running process
  created via runProcess.\n\n- **`proc-message`**  \n  Sends structured text-based
  instructions or commands to a subprocess started with runProcess. It provides a
  programmable interface for interacting with the process via standard input.\n\n-
  **`Scriptable CLI Integration`**  \n  The `pty-mcp-server` supports execution of
  shell scripts associated with registered tools defined in `tools-list.json`. Each
  tool must be registered by name, and a corresponding shell script (`.sh`) should
  exist in the configured `tools/` directory.\n\n  This design supports AI-driven
  workflows by exposing tool interfaces through a predictable scripting mechanism.
  The AI can issue tool invocations by name, and the server transparently manages
  execution and interaction.  \n  To add a new tool:\n    1. Create a shell script
  named `your-tool.sh` in the `tools/` directory.\n    2. Add an entry in `tools-list.json`
  with the name `\"your-tool\"` and appropriate metadata.\n    3. No need to recompile
  or modify the server — tools are dynamically resolved by name.\n\n  This separation
  of tool definitions (`tools-list.json`) and implementation (`tools/your-tool.sh`)
  ensures clean decoupling and simplifies extensibility.\n\n> **Note:**  \n> Commands
  starting with `pty-` are not supported on Windows. These tools rely on POSIX-style
  pseudo terminals (PTY), which are not natively available in the Windows environment.\n\n\n###
  Running with Podman or Docker\n\nYou can build and run `pty-mcp-server` using either
  **Podman** or **Docker**.\n\n**Note:** When running pty-mcp-server inside a Docker
  container, after establishing a pty connection, you will be operating within the
  container environment. This should be taken into account when interacting with the
  server.\n\n#### 1. Build the image\n\nClone the repository and navigate to the `docker`
  directory:\n\n```bash\n$ git clone https://github.com/phoityne/pty-mcp-server.git\n$
  cd pty-mcp-server/docker\n$ podman build . -t pty-mcp-server-image\n$\n```\nRef
  : [build.sh](https://github.com/phoityne/pty-mcp-server/blob/main/docker/build.sh)\n\n####
  2. Run the container\nRun the server inside a container:\n\n```bash\n$ podman run
  --rm -i \\\n--name pty-mcp-server-container \\\n-v /path/to/dir:/path/to/dir \\\n--hostname
  pms-docker-container \\\npty-mcp-server-image \\\n-y /path/to/dir/config.yaml\n$\n```\nRef
  : [run.sh](https://github.com/phoityne/pty-mcp-server/blob/main/docker/run.sh)\n\nBelow
  is an example of how to configure `mcp.json` to run the MCP server within VSCode:\n```json\n{\n
  \ \"servers\": {\n    \"pty-mcp-server\": {\n      \"type\": \"stdio\",\n      \"command\":
  \"/path/to/run.sh\",\n      \"args\": []\n      /*\n      \"command\": \"podman\",\n
  \     \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"--name\", \"pty-mcp-server-container\",\n
  \       \"-v\", \"/path/to/dir:/path/to/dir\",\n        \"--hostname\", \"pms-docker-container\",\n
  \       \"pty-mcp-server-image\",\n        \"-y\", \"/path/to/dir/config.yaml\"\n
  \     ]\n      */\n    }\n  }\n}\n```\n\n### Binary Installation\n\nIf you prefer
  to build it yourself, make sure the following requirements are met: \n- GHC >= 9.12
  \ \n\nYou can install `pty-mcp-server` using `cabal`:\n\n```bash\n$ cabal install
  pty-mcp-server\n```\n\n### Binary Execution\n\nThe `pty-mcp-server` application
  is executed from the command line.\n\n#### Usage\n\n```sh\n$ pty-mcp-server -y config.yaml\n```\nWhile
  the server can be launched directly from the command line, it is typically started
  and managed by development tools that integrate an MCP client—such as Visual Studio
  Code. These tools utilize the server to enable interactive and automated command
  execution via PTY sessions.\n\n#### VSCode Integration: `.vscode/mcp.json`\n\nTo
  streamline development and server invocation from within Visual Studio Code, the
  project supports a `.vscode/mcp.json` configuration file.\n\nThis file defines how
  the `pty-mcp-server` should be launched in a development environment. Example configuration:\n\n```json\n{\n
  \ \"servers\": {\n    \"pty-mcp-server\": {\n      \"type\": \"stdio\",\n      \"command\":
  \"pty-mcp-server\",\n      \"args\": [\"-y\", \"/path/to/your/config.yaml\"]\n    }\n
  \ }\n}\n```\n\n### config.yaml Configuration ([ref](https://github.com/phoityne/pty-mcp-server/blob/main/configs/pty-mcp-server.yaml))\n-
  `logDir`:  \n  The directory path where log files will be saved. This includes standard
  output/error logs and logs from script executions.\n\n- `logLevel`:  \n  Sets the
  logging level. Examples include `\"Debug\"`, `\"Info\"`, and `\"Error\"`.\n\n- `toolsDir`:
  \ \n  Directory containing script files (shell scripts named after tool names, e.g.,
  `ping.sh`). If a script matching the tool name exists here, it will be executed
  when the tool is called.  \n  This directory must also contain the `tools-list.json`
  file, which defines the available public tools and their metadata.\n\n- `prompts`:
  \ \n  A list of prompt strings used to detect interactive command prompts. This
  allows the AI to identify when a command is awaiting input. Examples include `\"ghci>\"`,
  `\"]$\"`, `\"password:\"`, etc.\n\n  \n---\n\n## Demonstrations\n\n### Demo: Watch
  AI Create and Launch a Web App from Scratch\n![Demo web service construct](https://raw.githubusercontent.com/phoityne/pty-mcp-server/main/docs/demo_web.gif)
  \ \nRef : [Web Service Construction Agent Prompt](https://github.com/phoityne/pty-mcp-server/blob/main/assets/prompts/web-service-prompt.md)\n\n\n1.
  \U0001F4CC [Scene 1: Overview & MCP Configuration]  \nIn this demo, we’ll show how
  an AI agent builds and runs a web service inside a Docker container using the `pty-mcp-server`.
  \ \nFirst, we configure `mcp.json` to launch the MCP server using a shell script.
  \ \nThis script starts the Docker container where our PTY-based interaction will
  take place.\n2. \U0001F433 [Scene 2: Docker Launch Configuration]  \nThe `run.sh`
  script includes volume mounts, hostname settings, and opens **port 8080**.  \nThis
  allows the container to expose a web service to the host system.\n\n3. \U0001F680
  [Scene 3: Starting the MCP Server]  \nNow, the container is launched, and the `pty-mcp-server`
  is running inside it,  \nready to handle AI-driven requests through a pseudo-terminal.\n\n4.
  \U0001F916 [Scene 4: Connecting the AI Agent]  \nWe open the chat interface and
  send a prompt designed for a web service builder agent.  \nThe AI connects to the
  container’s Bash session via PTY and begins its preparation.\n\n5. \U0001F6E0️ [Scene
  5: Initial Setup Commands]  \nFollowing the prompt, the AI starts by:  \n    - Creating
  a project folder  \n    - Moving into the working directory\n\n6. \U0001F4E5 [Scene
  6: AI Ready to Receive Instructions]  \nOnce the environment is ready, we instruct
  the AI to build a “Hello, world” web service.  \nFrom here, the AI begins its autonomous
  construction process.\n\n7. ⚙️ [Scene 7: AI Executes Web Setup Commands]  \nThe
  AI proposes a series of terminal commands.  \nAs the user, we review and approve
  them one by one.  \nSteps include:\n    - Checking for Python\n    - Installing
  Flask\n    - Writing the source code (`app.py`) to serve “Hello, world”\n    - Running
  the Flask server\n    - Testing via `curl http://localhost:8080` inside the container\n\n8.
  \U0001F310 [Scene 8: Verifying from Outside the Container]  \nTo confirm external
  accessibility, we access the service from the host via **port 8080**.  \n✅ As expected,
  the response is: **“Hello, world”**\n\n9. \U0001F9FE [Scene 9: Reviewing the Execution
  History]  \nFinally, we review the AI's actions step by step:\n    - Initialized
  the Bash session and created the working directory  \n    - Set up the Python environment
  \ \n    - Generated the Flask-based `app.py`  \n    - Launched the web server and
  validated its operation\n\n10. \U0001F3C1 [Scene 10: Conclusion]  \nThis demonstrates
  how AI, combined with the **PTY-MCP-Server** and **Docker**,  \ncan automate real
  development tasks — **interactively**, **intelligently**, and **reproducibly**.\n\n\n###
  Demo: Docker Execution and Host SSH Access\n![Demo Docker](https://raw.githubusercontent.com/phoityne/pty-mcp-server/main/docs/demo_docker.gif)\n\n1.
  MCP Configuration with Docke  \nThis is the mcp.json file. It defines the MCP server
  startup configuration. In this case, the pty-mcp-server will be launched using a
  shell script: run.sh. This script uses Podman to start the container.\n2. Starting
  the MCP Server  \nHere is the run.sh script. It launches the Docker container using
  podman run, with the correct volume mount, hostname, and image tag. Once executed,
  the MCP server starts inside the container.\n3. Tool List  \nNext, the list of tools
  exposed to the client is defined in tools-list.json.\nIt includes three tools: pty-message,
  pty-ssh, a shell script named hostname.sh\n4. Tool Script Directory  \nIn config.yaml,
  the path to the script directory is defined.\nThis is where tool scripts like hostname.sh
  should be placed\n5. Hostname Script  \nThe hostname.sh script simply runs the hostname
  command.\nIt is executed as a tool within the container.\n6. Executing hostname
  from Chat  \nNow, let’s run the hostname tool in the chat.\nThis shows the name
  of the current host, which is the container.  \nAs expected, the output is: pms-docker-container\nThis
  confirms that the command is executed inside the Docker container.\n7. Using pty-ssh
  to Access the Host  \nNext, we use pty-ssh to establish a pty session with the host
  OS.\nSSH connection is attempted using host.docker.internal, which resolves to the
  Docker host.  \nAfter confirming the host identity and entering the password, the
  login succeeds.\n8. Confirming Host Environment  \nNow that we are connected to
  the host, we run: cat /etc/redhat-release  \nThis confirms that we are now in the
  host OS, which is CentOS 9.  \nIn contrast, the Docker container is running AlmaLinux
  9.\n\n\n\n### Demo: Interactive Bash via PTY\n![Demo bash](https://raw.githubusercontent.com/phoityne/pty-mcp-server/main/docs/demo_bash.gif)\n1.
  Configure bash-mcp-server in mcp.json  \nIn this file, register bash-mcp-server
  as an MCP server.  \nSpecify the command as pty-mcp-server and pass the configuration
  file config.yaml as an argument.\n2. Settings in config.yaml  \nThe config.yaml
  file defines the log directory, the directory for tools, and prompt detection patterns.
  \ \nThese settings establish the environment for the AI to interact with bash through
  the PTY.\n3. Place tools-list.json in the toolsDir  \nYou need to place tools-list.json
  in the directory specified by toolsDir.  \nThis file declares the tools available
  to the AI, including pty-bash and pty-message.  \n4. AI Connects to Bash and Selects
  Commands Autonomously  \nThe AI connects to bash through the pseudo terminal and
  \ndecides which commands to execute based on the context.  \n5. Confirming the Command
  Execution Results  \nThe output of the getenforce command shows whether SELinux
  is in Enforcing mode.  \nThis result appears on the terminal or in logs, allowing
  the user to verify the system status.\n\n\n### Demo: Shell Script Execution\n![Demo
  shellscript](https://raw.githubusercontent.com/phoityne/pty-mcp-server/main/docs/demo_script.gif)\n\n1.
  mcp.json Configuration  \nStarts the pty-mcp-server in stdio mode, passing config.yaml
  as an argument.\n2. Overview of config.yaml  \nSpecifies log directory, tools directory,
  and prompt strings.  \nThe tools-list.json in toolsDir defines which tools are exposed.\n3.
  Role of tools-list.json  \nLists available script tools, with only the script_add
  tool registered here.\n4. Role and Naming Convention of the tools Folder  \nStores
  executable shell scripts called via the mcp server.  \nThe tool names in tools-list.json
  match the shell script filenames in this folder.\n5. Execution from VSCode GitHub
  Copilot  \nRuns script_add.sh with the command `#script_add 2 3`, executing the
  addition.\n6. Confirming the Result  \nReturns \"5\", indicating the operation was
  successful.\n\n\n### Demo: Haskell Debugging with `cabal repl`\n![Demo haskell cabal
  repl](https://raw.githubusercontent.com/phoityne/pty-mcp-server/main/docs/demo_cabal.gif)
  \ \nRef : [haskell cabal debug prompt](https://github.com/phoityne/pty-mcp-server/blob/main/assets/prompts/haskell-cabal-debug-prompt.md)\n\n1.
  Target Code Overview  \nA function in MyLib.hs is selected to inspect its runtime
  state using cabal repl and an AI-driven debug interface.\n2. MCP Server Initialization
  \ \nThe MCP server is launched to allow structured interaction between the AI and
  the debugging commands.\n3. Debugger Prompt and Environment Setup  \nThe AI receives
  a prompt, starts cabal repl, and loads the module to prepare for runtime inspection.\n4.
  Debugging Execution Begins  \nThe target function is executed and paused at a predefined
  point for runtime observation.\n5. State Inspection and Output  \nRuntime values
  and control flow are displayed to help verify logic and observe internal behavior.\n6.
  Summary  \nIntegration with pty-msp-server enables automated runtime inspection
  for Haskell applications.\n\n---\n\n## Architecture Guide (Software Architecture
  and Technical Details)\n\n### Architectural Strategy\n\nThe architecture of the
  `pty-mcp-server` project is designed with medium-to-large scale systems in mind.
  Emphasis is placed on **modularity**, **maintainability**, and **scalability**,
  especially in environments involving multiple teams or organizations.\n\nTo achieve
  these goals, the system is structured as a collection of well-separated packages,
  each responsible for a specific concern or domain. This package-oriented design
  provides several strategic benefits.\n\nThe overall package structure adheres to
  the principles of **Onion Architecture**, reflecting a layered design that places
  the domain model at the core. Furthermore, the **internal module structure within
  each package** is also guided by a layered approach, maintaining clear separation
  between pure data definitions, domain services, and infrastructure concerns.\n\n###
  Role of `pty-mcp-server` as a Dependency Injector\n\nIn addition to managing REPL
  communication, `pty-mcp-server` is **not merely an executable module**, but also
  acts as a **dependency injector** for the entire system.\n\n- It is capable of **referencing
  all relevant PMS packages**, including those that it depends on.\n- This allows
  it to **construct and wire together application components** across multiple packages
  and modules in a unified manner.\n- By centralizing this dependency resolution,
  `pty-mcp-server` provides a single point of control over **cross-cutting dependencies**,
  improving visibility and control over the system architecture.\n\nAs a result, inter-package
  and inter-module dependencies can be **centrally coordinated and managed**, which
  promotes better encapsulation, reusability, and testability throughout the system.\n\n###
  Dependencies\n\nThis package depends on the following packages:  \n- [`pms-ui-request`](https://github.com/phoityne/pms-ui-request)\n-
  [`pms-ui-response`](https://github.com/phoityne/pms-ui-response)\n- [`pms-ui-notification`](https://github.com/phoityne/pms-ui-notification)\n-
  [`pms-infrastructure`](https://github.com/phoityne/pms-infrastructure)\n- [`pms-infra-cmdrun`](https://github.com/phoityne/pms-infra-cmdrun)\n-
  [`pms-infra-watch`](https://github.com/phoityne/pms-infra-watch)\n- [`pms-application-service`](https://github.com/phoityne/pms-application-service)\n-
  [`pms-domain-service`](https://github.com/phoityne/pms-domain-service)\n- [`pms-domain-model`](https://github.com/phoityne/pms-domain-model)\n\n###
  Rationale for Package Separation\n\n- **Clear Interface Definition**  \n  Each package
  exposes only its minimal, well-defined public API. This enforces clean module boundaries
  and reduces unintended dependencies between components.\n\n- **Team and Vendor Ownership**
  \ \n  In larger projects, different teams or external vendors can own specific packages.
  Clear separation ensures well-defined responsibilities and supports collaborative
  development across organizational boundaries.\n\n- **Repository and Release Independence**
  \ \n  Packages can be split into separate repositories and versioned independently.
  This allows for modular development and flexible release workflows, reducing build
  times and simplifying integration.\n\n- **Improved Maintainability and Extensibility**
  \ \n  By isolating concerns, the impact of code changes is limited to relevant modules.
  This minimizes regressions and facilitates safe, incremental improvements over time.\n\n###
  Intended Use Cases\n\n- Medium-to-large scale enterprise systems involving multiple
  developers or teams  \n- Modular systems with independent development and release
  cycles for components  \n- Projects that require long-term maintainability, extensibility,
  and isolation of concerns\n\nThis architecture follows a layered and modular approach.
  Domain models, domain services, application logic, and infrastructure concerns are
  each encapsulated in their own package, enabling clean composition while preserving
  separation of responsibilities.\n\n\n### Deployment Diagram\n![Deployment Diagram](https://raw.githubusercontent.com/phoityne/pty-mcp-server/main/docs/01-1.png)\n\n###
  Package Structure\n![Package Structure](https://raw.githubusercontent.com/phoityne/pty-mcp-server/main/docs/01-2.png)\n\n----\n"
description-type: markdown
hash: e2923be17a3b5a2dd4b274d0b05767d37cce06d10f7ae7ec0cbd43f2d2a0fb7a
homepage: https://github.com/phoityne/pty-mcp-server
latest: 0.0.4.0
license-name: Apache-2.0
maintainer: phoityne.hs@gmail.com
synopsis: pty-mcp-server
test-bench-deps: {}
