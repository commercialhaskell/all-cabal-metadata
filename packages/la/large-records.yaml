homepage: ''
changelog-type: markdown
hash: 36945c264fe6660179986c1e1e3e31e06a1a9d13963c8fcfe2fba4cbae5709c9
test-bench-deps:
  newtype: -any
  sop-core: -any
  base: -any
  generic-deriving: -any
  json-sop: -any
  tasty-quickcheck: -any
  mtl: -any
  tasty-hunit: -any
  record-hasfield: -any
  transformers: -any
  tasty: -any
  ghc-dump-core: -any
  generics-sop: -any
  record-dot-preprocessor: -any
  QuickCheck: -any
  large-records: -any
  microlens: -any
  aeson: -any
  template-haskell: -any
  vector: -any
maintainer: edsko@well-typed.com
synopsis: Efficient compilation for large records, linear in the size of the record
changelog: |
  # Revision history for large-records

  ## 0.1.0.0 -- 2021-08-19

  * First public release
basic-deps:
  sop-core: '>=0.5 && <0.6'
  base: '>=4.13 && <4.15'
  text: '>=1.2.4 && <1.5'
  syb: '>=0.7 && <0.8'
  containers: '>=0.6.2 && <0.7'
  haskell-src-exts: '>=1.21.1 && <1.24'
  mtl: '>=2.2.1 && <2.3'
  record-hasfield: '>=1.0 && <1.1'
  generics-sop: '>=0.5 && <0.6'
  haskell-src-meta: '>=0.8.3 && <0.9'
  microlens: '>=0.1.5 && <0.5'
  aeson: '>=1.4.4 && <1.6'
  template-haskell: -any
  vector: '>=0.12 && <0.13'
all-versions:
- 0.1.0.0
author: Edsko de Vries
latest: 0.1.0.0
description-type: haddock
description: |-
  For many reasons, the internal code generated for modules
  that contain records is quadratic in the number of record
  fields. For large records (more than 30 fields, say), this
  can become problematic, leading to large compilation times
  and high memory requirements for ghc. The large-records
  library provides a way to define records that is guaranteed
  to result in ghc core that is /linear/ in the number of
  record fields.
license-name: BSD-3-Clause
