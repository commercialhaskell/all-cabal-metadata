homepage: http://github.com/ekmett/ad
changelog-type: markdown
hash: d8122681c049a642a54eacf3a17c5f5c9a0a5f9159568ee4c7ea3ba52176fe7b
test-bench-deps:
  base: -any
  ad: -any
  filepath: -any
  criterion: -any
  doctest: ! '>=0.9.0.1 && <0.17'
  erf: -any
  directory: -any
maintainer: ekmett@gmail.com
synopsis: Automatic Differentiation
changelog: ! "4.3.5 [2018.01.18]\n------------------\n* Add `Semigroup` instance for
  `Id`.\n\n4.3.4\n-----\n* Support `doctest-0.12`\n\n4.3.3\n-----\n* Revamp `Setup.hs`
  to use `cabal-doctest`. This makes it build\n  with `Cabal-2.0`, and makes the `doctest`s
  work with `cabal new-build` and\n  sandboxes.\n\n4.3.2.1\n-----\n* GHC 8 support\n*
  Fix Kahn mode's `**` implementation\n* Fix multiple problems in Erf and InvErf methods\n\n4.3.2\n-----\n*
  Added `NoEq` versions of several combinators that can be used when `Eq` isn't available
  on the numeric type involved.\n\n4.3.1\n-----\n* Further improvements have been
  made in the performance of `Sparse` mode, at least asymptotically, when used on
  functions with many variables.\n  Since this is the target use-case for `Sparse`
  in the first place, this seems like a good trade-off. Note: this results in an API
  change, but\n  only in the API of an `Internal` module, so this is treated as a
  minor version bump.\n\n4.3\n---\n* Made drastic improvements in the performance
  of `Tower` and `Sparse` modes thanks to the help of BjÃ¶rn von Sydow.\n* Added constrained
  convex optimization.\n* Incorporated some suggestions from [herbie](http://herbie.uwplse.org/z)
  for improving floating point accuracy.\n\n4.2.4\n-----\n* Added `Newton.Double`
  modules for performance.\n\n4.2.3\n-----\n* `reflection` 2 support\n\n4.2.2\n-----\n*
  *Major* bug fix for `grads`, `jacobians`, and anything that uses `Sparse` mode in
  `Numeric.AD`. Derivatives after the first two\n  were previously incorrect.\n\n4.2.1.1\n-------\n*
  Support `nats` version 1\n\n4.2.1\n-----\n* Added `stochasticGradientDescent`.\n\n4.2\n---\n*
  Removed broken `Directed` mode.\n* Added `Numeric.AD.Rank1` combinators and moved
  most infinitesimal handling back out of the modes and into an `AD` wrapper.\n\n4.1\n---\n*
  Fixed a bug in the type of `conjugateGradientAscent` and `conjugateGradientDescent`
  that prevent users from being able to ever call it.\n\n4.0.0.1\n-------\n* Added
  the missing `instances.h` header file to `extra-source-files`.\n\n4.0\n---\n* An
  overhaul permitting monomorphic modes was completed by @alang9.\n* Add a `ForwardDouble`
  monomorphic mode\n\n3.4\n---\n* Added support for `erf` and `inverf`, etc. from
  `Data.Number.Erf`.\n* Split the infinitesimal and mode into two separate parameters
  to facilitate inlining and easier extension of the API.\n\n3.3.1\n-----\n* Build
  system improvements\n* Removed unused LANGUAGE pragmas\n* Added HLint configuration\n*
  We now use exactly the same versions of the packages used to build `ad` when running
  the doctests.\n\n3.3\n---\n* Renamed `Reverse` to `Kahn` and `Wengert` to `Reverse`.
  We use Arthur Kahn's topological sorting algorithm to\n  sort the tape after the
  fact in Kahn mode, while the stock Reverse mode builds a Wengert list as it goes,
  which\n  is more efficient in practice.\n\n3.2.2\n-----\n* Export of the `conjugateGradientDescent`
  and `gradientDescent` from `Numeric.AD`\n\n3.2.1\n---\n* `conjugateGradientDescent`
  now stops before it starts returning NaN results.\n\n3.2\n---\n* Renamed `Chain`
  to `Wengert` to reflect its use of Wengert lists for reverse mode.\n* Renamed `lift`
  to `auto` to avoid conflict with the more prevalent `transformers` library.\n* Fixed
  a bug in `Numeric.AD.Forward.gradWith'`, which caused it to return the wrong value
  for the primal.\n\n3.1.4\n-----\n* Added a better \"convergence\" test for `findZero`\n*
  Compute `tan` and `tanh` derivatives directly.\n\n3.1.3\n-----\n* Added `conjugateGradientDescent`
  and `conjugateGradientAscent` to `Numeric.AD.Newton`.\n\n3.1.2\n-----\n* Dependency
  bump\n\n3.1\n---\n* Added `Chain` mode, which is `Reverse` using a linear tape that
  doesn't need to be sorted.\n* Added a suite of doctests.\n* Bug fix in `Forward`
  mode. It was previously yielding incorrect results for anything that used `bind`
  or `bind'` internally.\n\n3.0\n---\n* Moved the contents of `Numeric.AD.Mode.Mixed`
  into `Numeric.AD`\n* Split off `Numeric.AD.Variadic` for the variadic combinators\n*
  Removed the `UU`, `FU`, `UF`, and `FF` type aliases.\n* Stopped exporting the types
  for `Mode` and `AD` from almost every module. Import `Numeric.AD.Types` if necessary.\n*
  Renamed `Tensors` to `Jet`\n* Dependency bump to be compatible with ghc 7.4.1 and
  mtl 2.1\n* More aggressive zero tracking.\n* `diff (**n) 0` for constant n and `diff
  (0**)` both now yield the correct answer for all modes.\n"
basic-deps:
  free: ! '>=4.6.1 && <6'
  data-reify: ! '>=0.6 && <0.7'
  reflection: ! '>=1.4 && <3'
  base: ! '>=4.3 && <5'
  comonad: ! '>=4 && <6'
  semigroups: ! '>=0.16 && <1'
  array: ! '>=0.2 && <0.6'
  erf: ! '>=2.0 && <2.1'
  containers: ! '>=0.2 && <0.7'
  nats: ! '>=0.1.2 && <2'
  transformers: ! '>=0.3 && <0.6'
all-versions:
- '0.12'
- '0.13'
- '0.15'
- '0.17'
- '0.18'
- '0.19'
- '0.20'
- '0.21'
- '0.22'
- '0.23'
- '0.24'
- '0.27'
- '0.28'
- '0.30.0'
- '0.31.0'
- '0.32.0'
- '0.33.0'
- '0.40'
- '0.40.1'
- '0.44.0'
- '0.44.1'
- '0.44.2'
- '0.44.3'
- '0.44.4'
- '0.45.0'
- '0.46.0'
- '0.46.1'
- '0.46.2'
- '0.47.0'
- '1.0.0'
- '1.0.1'
- '1.0.2'
- '1.0.3'
- '1.0.4'
- '1.0.5'
- '1.0.6'
- '1.1.0'
- '1.1.0.1'
- '1.1.1'
- '1.1.3'
- '1.2.0'
- '1.2.0.1'
- '1.2.0.2'
- '1.3'
- '1.3.0.1'
- '1.3.1'
- '1.4'
- '1.5'
- '1.5.0.1'
- '1.5.0.2'
- '3.0'
- '3.0.1'
- '3.1.1'
- '3.1.2'
- '3.1.3'
- '3.1.4'
- '3.2'
- '3.2.1'
- '3.2.2'
- '3.3.0.1'
- '3.3.1'
- '3.3.1.1'
- '3.4'
- '4.0'
- '4.0.0.1'
- '4.1'
- '4.2'
- '4.2.0.1'
- '4.2.1'
- '4.2.1.1'
- '4.2.2'
- '4.2.3'
- '4.2.4'
- '4.3'
- '4.3.1'
- '4.3.2'
- '4.3.2.1'
- '4.3.3'
- '4.3.4'
- '4.3.5'
author: Edward Kmett
latest: '4.3.5'
description-type: markdown
description: ! "ad\n==\n\n[![Hackage](https://img.shields.io/hackage/v/ad.svg)](https://hackage.haskell.org/package/ad)
  [![Build Status](https://secure.travis-ci.org/ekmett/ad.png?branch=master)](http://travis-ci.org/ekmett/ad)\n\nA
  package that provides an intuitive API for [Automatic Differentiation](http://en.wikipedia.org/wiki/Automatic_differentiation)
  (AD) in Haskell. Automatic differentiation provides a means to calculate the derivatives
  of a function while evaluating it. Unlike numerical methods based on running the
  program with multiple inputs or symbolic approaches, automatic differentiation typically
  only decreases performance by a small multiplier.\n\nAD employs the fact that any
  program `y = F(x)` that computes one or more value does so by composing multiple
  primitive operations. If the (partial) derivatives of each of those operations is
  known, then they can be composed to derive the answer for the derivative of the
  entire program at a point.\n\nThis library contains at its core a single implementation
  that describes how to compute the partial derivatives of a wide array of primitive
  operations. It then exposes an API that enables a user to safely combine them using
  standard higher-order functions, just as you would with any other Haskell numerical
  type.\n\nThere are several ways to compose these individual [Jacobian matrices](http://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant).
  We hide the choice used by the API behind an explicit \"Mode\" type-class and universal
  quantification. This prevents users from [confusing infinitesimals](http://conway.rutgers.edu/~ccshan/wiki/blog/posts/Differentiation/).
  If you want to risk infinitesimal confusion in order to get greater flexibility
  in how you curry, flip and generally combine the differential operators, then the
  `Rank1.*` modules are probably your cup of tea.\n\nFeatures\n--------\n\n * Provides
  forward- and reverse- mode AD combinators with a common API.\n * Optional type-level
  \"branding\" is available to prevent the end user from confusing infinitesimals\n
  * Each mode has a separate module full of combinators, with a consistent look and
  feel.\n\nExamples\n---------\n\nYou can compute derivatives of functions\n\n    Prelude
  Numeric.AD> diff sin 0 {- cos 0 -}\n    1.0\n\nOr both the answer and the derivative
  of a function:\n\n    Prelude Numeric.AD> diff' (exp . log) 2\n    (2.0,1.0)\n\nYou
  can compute the derivative of a function with a constant parameter using `auto`:\n\n
  \   Prelude Numeric.AD> let t = 2.0 :: Double\n    Prelude Numeric.AD> diff (\\
  x -> auto t * sin x) 0\n    2.0\n\nYou can use a symbolic numeric type, like the
  one from `simple-reflect` to obtain symbolic derivatives:\n\n    Prelude Debug.SimpleReflect
  Numeric.AD> diff atanh x\n    recip (1 - x * x) * 1\n\nYou can compute gradients
  for functions that take non-scalar values in the form of a Traversable functor full
  of AD variables.\n\n    Prelude Numeric.AD Debug.SimpleReflect> grad (\\[x,y,z]
  -> x * sin (x + log y)) [x,y,z]\n    [ 0 + (0 + sin (x + log y) * 1 + 1 * (0 + cos
  (x + log y) * (0 + x * 1)))\n    , 0 + (0 + recip y * (0 + 1 * (0 + cos (x + log
  y) * (0 + x * 1))))\n    , 0\n    ]\n\nwhich one can simplify to:\n\n    [ sin (x
  + log y) + cos (x + log y) * x, recip y * cos (x + log y) * x, 0 ]\n\nIf you need
  multiple derivatives you can calculate them with `diffs`:\n\n    Prelude Numeric.AD>
  take 10 $ diffs sin 1\n    [0.8414709848078965,0.5403023058681398,-0.8414709848078965,-0.5403023058681398,0.8414709848078965,0.5403023058681398,-0.8414709848078965,-0.5403023058681398,0.8414709848078965,0.5403023058681398]\n\nor
  if your function takes multiple inputs, you can use grads, which returns an 'f-branching
  stream' of derivatives, that you can\ninspect lazily. Somewhat more intuitive answers
  can be obtained by converting the stream into the polymorphically recursive\n`Jet`
  data type. With that we can look at a single \"layer\" of the answer at a time:\n\nThe
  answer:\n\n    Prelude Numeric.AD> headJet $ jet $  grads (\\[x,y] -> exp (x * y))
  [1,2]\n    7.38905609893065\n\nThe gradient:\n\n    Prelude Numeric.AD> headJet
  $ tailJet $ jet $  grads (\\[x,y] -> exp (x * y)) [1,2]\n    [14.7781121978613,7.38905609893065]\n\nThe
  hessian (n * n matrix of 2nd derivatives)\n\n    Prelude Numeric.AD> headJet $ tailJet
  $ tailJet $ jet $  grads (\\[x,y] -> exp (x * y)) [1,2]\n    [[29.5562243957226,22.16716829679195],[22.16716829679195,7.38905609893065]]\n\nOr
  even higher order tensors of derivatives as a jet.\n\n    Prelude Numeric.AD> headJet
  $ tailJet $ tailJet $ tailJet $ jet $  grads (\\[x,y] -> exp (x * y)) [1,2]\n    [[[59.1124487914452,44.3343365935839],[44.3343365935839,14.7781121978613]],[[44.3343365935839,14.7781121978613],[14.7781121978613,7.38905609893065]]]\n\nNote
  the redundant values caused by the various symmetries in the tensors. The `ad` library
  is careful to compute\neach distinct derivative only once, lazily and to share the
  resulting computation.\n\nOverview\n--------\n\n### Modules\n\n * `Numeric.AD` computes
  using whichever mode or combination thereof is suitable to each individual combinator.
  This mode is the default, re-exported by `Numeric.AD`\n * `Numeric.AD.Mode.Forward`
  provides basic forward-mode AD. It is good for computing simple derivatives.\n *
  `Numeric.AD.Mode.Sparse` computes a sparse forward-mode AD tower. It is good for
  higher derivatives or large numbers of outputs.\n * `Numeric.AD.Mode.Kahn` computes
  with reverse-mode AD. It is good for computing a few outputs given many inputs.\n
  * `Numeric.AD.Mode.Reverse` computes with reverse-mode AD. It is good for computing
  a few outputs given many inputs, when not using sparks heavily.\n * `Numeric.AD.Mode.Tower`
  computes a dense forward-mode AD tower useful for higher derivatives of single input
  functions.\n * `Numeric.AD.Newton` provides a number of combinators for root finding
  using Newton's method with quadratic convergence.\n * `Numeric.AD.Halley` provides
  a number of combinators for root finding using Halley's method with cubic convergence.\n
  * `Numeric.AD.Rank1.*` provides combinators for AD that are strictly rank-1. This
  makes it easier to flip and contort them with higher order functions at the expense
  of type safety when it comes to infinitsimal confusion.\n\n### Combinators\n\nWhile
  not every mode can provide all operations, the following basic operations are supported,
  modified as appropriate by the suffixes below:\n\n * `grad` computes the gradient
  (vector of partial derivatives at a given point) of a function.\n * `jacobian` computes
  the Jacobian matrix of a function at a point.\n * `diff` computes the derivative
  of a function at a point.\n * `du` computes a directional derivative of a function
  at a point.\n * `hessian` computes the Hessian matrix (matrix of second partial
  derivatives) of a function at a point.\n\n### Combinator Suffixes\n\nThe following
  suffixes alter the meanings of the functions above as follows:\n\n * `'` also return
  the answer\n * `With` lets the user supply a function to blend the input with the
  output\n * `F` is a version of the base function lifted to return a `Traversable`
  (or `Functor`) result\n * `s` means the function returns all higher derivatives
  in a list or f-branching `Stream`\n * `T` means the result is transposed with respect
  to the traditional formulation (usually to avoid paying for transposing back)\n
  * `0` means that the resulting derivative list is padded with 0s at the end.\n *
  `NoEq` means that an infinite list of converging values is returned rather than
  truncating the list when they become constant\n\nContact Information\n-------------------\n\nContributions
  and bug reports are welcome!\n\nPlease feel free to contact me through github or
  on the #haskell IRC channel on irc.freenode.net.\n\n-Edward Kmett\n"
license-name: BSD3
