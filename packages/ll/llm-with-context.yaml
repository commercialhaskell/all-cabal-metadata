all-versions:
- 0.1.0.0
author: lazyLambda
basic-deps:
  aeson: '>=2.2.3 && <2.3'
  base: '>=4.20.2 && <4.21'
  bytestring: '>=0.12.2 && <0.13'
  containers: '>=0.7 && <0.8'
  data-default: '>=0.8.0 && <0.9'
  directory: '>=1.3.8 && <1.4'
  http-client: '>=0.7.19 && <0.8'
  http-client-tls: '>=0.3.6 && <0.4'
  http-types: '>=0.12.4 && <0.13'
  parsec: '>=3.1.18 && <3.2'
  scrappy-core: '>=0.1.0 && <0.2'
  text: '>=2.1.3 && <2.2'
  transformers: '>=0.6.1 && <0.7'
changelog: |
  # Revision history for llm-with-context

  ## 0.1.0.0 -- YYYY-mm-dd

  * First version. Released on an unsuspecting world.
changelog-type: markdown
description: Using Proxy and StateT we manage a typed conversation with conversation
  history. We can also perform more customized qqueries of history to be included
  in the next response.
description-type: haddock
hash: 8cb3c0f75a9a0357a4a95c664a306a20226eed5d262415628dc3adc2d419d773
homepage: ''
latest: 0.1.0.0
license-name: MIT
maintainer: galen.sprout@gmail.com
synopsis: 'Typified interactions with LLMs             '
test-bench-deps: {}
