all-versions:
- 0.1.0.0
author: tushar
basic-deps:
  base: '>=4.7 && <5'
  bytestring: '>=0.9 && <0.13'
  derive-storable: '>=0.2 && <0.4'
changelog: |
  # Changelog for `llama-cpp-hs`

  All notable changes to this project will be documented in this file.

  The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
  and this project adheres to the
  [Haskell Package Versioning Policy](https://pvp.haskell.org/).

  ## Unreleased

  ## 0.1.0.0 - YYYY-MM-DD
changelog-type: markdown
description: "# llama-cpp-hs\n\nHaskell bindings over [llama.cpp](https://github.com/ggml-org/llama.cpp)\n\nThis
  package provides both low-level and high-level interfaces to interact with the LLaMA
  C++ inference engine via Haskell FFI. \nIt allows you to run LLMs locally in pure
  C/C++, with support for GPU acceleration and quantized models.\n\n## Features\n\n-
  Low-level access to the full LLaMA C API using Haskell FFI.\n- Higher-level convenience
  functions for easier model interaction.\n- Examples provided for quickly getting
  started.\n\n---\n\n## Example Usage\n\nCheck out the `/examples` directory to see
  how to load and query models directly from Haskell.\n\n---\n\n## Setup\n\n### 1.
  Using Nix (Recommended)\n\nEnsure that [Nix](https://nixos.org/download.html) is
  installed on your system.\n\nThen, enter the development shell:\n\n```bash\nnix-shell\n```\n\nBuild
  the project using Stack:\n\n```bash\nstack build\n```\n\n### 2. Using Stack (Manual
  Setup)\n\nIf you prefer not to use Nix, follow these steps:\n\n1. Clone and install
  [`llama.cpp`](https://github.com/ggml-org/llama.cpp) manually.\n2. Make sure `llama.h`
  is available at `/usr/local/include/` and compiled `libllama.a` or `libllama.so`
  at `/usr/local/lib/`.\n3. Install Stack if you havenâ€™t already: https://docs.haskellstack.org/en/stable/install_and_upgrade/\n4.
  Then proceed with:\n\n```bash\nstack build\n```\n\n---\n\n## Models\n\nTo use this
  library, you'll need to download one of the many open-source GGUF models available
  on Hugging Face\n\nSearch for compatible GGUF models:\n- [Hugging Face GGUF Models](https://huggingface.co/models?search=gguf)\n\n---\n\n##
  Current State\n\nThe codebase is still under active development and may undergo
  breaking changes. Use it with caution in production environments.\n\nPull requests,
  issues, and community contributions are highly encouraged!\n\n---\n\n## Contributing\n\nContributions
  are welcome!\n\n---\n\n## License\n\nThis project is licensed under [MIT](LICENSE).\n\n---\n\n##
  Thank You\n\nThanks to [ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)
  for making local LLM inference fast, lightweight, and accessible!\n"
description-type: markdown
hash: c8c9d35d955e7c7c2326144c7944a594fb163d62fbfeae28a11a26d6b4a55a1a
homepage: https://github.com/tusharad/llama-cpp-hs#readme
latest: 0.1.0.0
license-name: MIT
maintainer: tusharadhatrao@gmail.com
synopsis: Haskell FFI bindings to the llama.cpp LLM inference library
test-bench-deps:
  base: '>=4.7 && <5'
  bytestring: '>=0.9 && <0.13'
  derive-storable: '>=0.2 && <0.4'
  llama-cpp-hs: '>=0'
  tasty: '>=0'
  tasty-hunit: '>=0'
