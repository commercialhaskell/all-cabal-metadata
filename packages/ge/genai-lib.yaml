all-versions:
- '1.3'
- '2.0'
- 2.0.1
- 2.1.0
author: Dino Morelli
basic-deps:
  aeson: '>=2.1.2.1 && <2.3'
  base: '>=4.18.1.0 && <5'
  bytestring: '>=0.11.5.2 && <0.13'
  containers: '>=0.6.7 && <0.7'
  genai-lib: '>=0'
  http-client: '>=0.7.16 && <0.8'
  http-client-tls: '>=0.3.6.3 && <0.4'
  scientific: '>=0.3.7.0 && <0.4'
  servant: '>=0.20.1 && <0.21'
  servant-client: '>=0.20 && <0.21'
  servant-client-core: '>=0.20 && <0.21'
  string-conv: '>=0.2.0 && <0.3'
  text: '>=2.0.2 && <3'
  time: '>=1.12.2 && <1.13'
changelog: |
  2.0.1 (2025-04-23)

    * Fixed build issues with cabal and package dep ranges
    * Removed some whitespace in example code

  2.0 (2025-04-22)

    * Extensive redesign of this library
    * Work done to support multiple LLM servers
    * Redesigned how LLM options work using Semigroup
    * Now supporting loading the API key from a file
    * Added code to make the TLS request timeout longer
    * Added and adjusted docs including Haddock API docs
    * Redesigned failure with throwIO and a custom exception type
    * Added keep_alive to the Ollama request

  1.3 (2025-03-27)

    * Initial release
changelog-type: markdown
description: |
  # genai-lib


  ## Synopsis

  A library for interacting with various generative AI LLMs


  ## Description

  A library for performing completions and chats with various generative AI LLMs
  (Large Language Models). Works today with Ollama and OpenAI with more to come
  in the future.

  This project is very much in-progress and incomplete.


  ## Using this library

  An example, more available in `src/example`

      import Control.Exception (Handler (..), catches)
      import Data.Text.Lazy.IO qualified as TL
      import GenAILib (ClientError, Request (..), jsonToText, mkRequest, numopt,
        stringopt, systemmsg, usermsg)
      import GenAILib.HTTP (GenAIException, openaiV1Chat, openaiV1ChatJ,
        tokenFromFile)
      import GenAILib.OpenAI (OpenAIRequest, getMessage)


      main :: IO ()
      main = do
        openaiJSON
        openaiData

      -- A simple example with no error handling that expects an Aeson Value
      -- (OpenAIRequest is an instance of ToJSON) and displays the encoded JSON
      -- response
      openaiJSON :: IO ()
      openaiJSON = do
        let req :: OpenAIRequest = mkRequest "gpt-3.5-turbo" $ usermsg "Why is the sky blue?"
        token <- tokenFromFile "path/to/openai/key"
        TL.putStrLn . jsonToText =<< openaiV1ChatJ token Nothing req

      -- Another example with exception handling that expects an OpenAIResponse
      -- data structure, displaying that and also just the response text
      openaiData :: IO ()
      openaiData = do
        let req :: OpenAIRequest = mkRequest "gpt-3.5-turbo"
                (  systemmsg "Answer in the style of Bugs Bunny. Try to work the phrase \"What's up Doc?\" in somewhere."
                <> usermsg "Why is the sky blue?"
                <> numopt "temperature" 0.8
                <> stringopt "service_tier" "default"
                )
        token <- tokenFromFile "path/to/openai/key"
        res <- openaiV1Chat token Nothing req `catches`
          [ Handler (\(e :: GenAIException) -> error . show $ e)
          , Handler (\(e :: ClientError) -> error . show $ e)
          ]
        print res  -- The entire OpenAIResponse
        TL.putStrLn . getMessage $ res  -- Just the response Message Content


  ## Getting source

  Source code is available from codeberg at the [genai-lib](https://codeberg.org/dinofp/genai-lib.git) project page.


  ## Contact

  Dino Morelli <dino@ui3.info>
description-type: markdown
hash: eba688115fd8df0674d56e5e4dde2e956a65e0e05783f3942aa3523d9ea8bee1
homepage: ''
latest: 2.1.0
license-name: ISC
maintainer: dino@ui3.info
synopsis: A library for interacting with various generative AI LLMs
test-bench-deps: {}
