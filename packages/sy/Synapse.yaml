all-versions:
- 0.1.0.0
- 0.1.0.1
author: JktuJQ
basic-deps:
  base: '>=4.17.2.1 && <4.21.0.0'
  hashable: '>=1.4.7 && <1.5'
  random: '>=1.3.0 && <1.4'
  terminal-progress-bar: '>=0.4.2 && <0.5'
  unordered-containers: '>=0.2.20 && <0.3'
  vector: '>=0.13.2 && <0.14'
changelog: "# Revision history for Synapse\r\n\r\n## 0.1.0.0 -- 2025-03-20\r\n\r\n*
  First version.\r\n"
changelog-type: markdown
description: "# ![Synapse logo](./SynapseLogo.png) Synapse ![Synapse logo](./SynapseLogo.png)\r\n\r\n`Synapse`
  is a machine learning library written in pure Haskell,\r\nthat makes creating and
  training neural networks an easy job.\r\n\r\n## \U0001F528 Design \U0001F528\r\n\r\nGoals
  of `Synapse` library are to provide interface that is:\r\n\r\n* easily extensible\r\n*
  simple\r\n* powerful\r\n\r\nHaskell ecosystem only offers a few of machine learning
  libraries,\r\nbut all of them have very complicated interface -\r\n[neural](https://hackage.haskell.org/package/neural)
  does not have too much extensibility and its typing is pretty hard,\r\n[grenade](https://hackage.haskell.org/package/grenade),
  although very powerful, uses a lot of type trickery that is impossible to reason
  about for beginners\r\nand [hasktorch](https://hackage.haskell.org/package/hasktorch)
  is a wrapper that is not as convenient as one might want.\r\n\r\n`Synapse` tries
  to resemble Python's [Keras](https://keras.io/api/) API,\r\nwhich is a unified interface
  for backends such as [Pytorch](https://pytorch.org/) and [Tensorflow](https://www.tensorflow.org/).\r\n\r\nEven
  though Python code practices usually are not what we want to see in Haskell code,\r\nmantaining
  the same level of accessibility and flexibility is what `Synapse` is focused on.\r\n\r\n##
  \U0001F4BB Usage \U0001F4BB\r\n\r\n`Synapse` comes batteries-included with its own
  matrices,\r\nautodifferentiation system,\r\nand neural networks building blocks.\r\n\r\n###
  `Vec`s and `Mat`s\r\n\r\nClever usage of Haskell's typeclasses allows operations
  to be written 'as is' for different types.\r\n\r\nFollowing example emulates dense
  layer forward propagation on plain matrices.\r\n\r\n```Haskell\r\ninput   = M.rowVec
  $ V.fromList [1.0, 2.0, 3.0]\r\n\r\nweights = M.replicate (3, 3) 1.0\r\nbias    =
  M.rowVec $ V.replicate 3 0.0\r\n\r\noutput  = tanh (input `matMul` weights `addMatRows`
  bias)\r\n```\r\n\r\n### Symbolic operations and autograd system\r\n\r\n`Synapse`
  autograd system implements reverse-mode dynamic automatic differentiation, where
  graph of operations is created on the fly.\r\nIts usage is much simpler than [ad](https://hackage.haskell.org/package/ad),\r\nand
  it is easily extensible - all you need is to define a function that produces `Symbol`
  which will hold gradients of\r\nthat function and that is it!\r\nThere is a [backprop](https://hackage.haskell.org/package/backprop)
  library that has a very similar implementation to `Synapse`'s one,\r\nso if you
  are familiar with [backprop](https://hackage.haskell.org/package/backprop) you will
  have no problems using `Synapse.Autograd` module.\r\n\r\nSpeaking of the previous
  example - you might want to record gradients of those operations, and that is just
  as easy:\r\n\r\n```Haskell\r\ninput   = symbol (SymbolIdentifier \"input\") $ M.rowVec
  $ V.fromList [1.0, 2.0, 3.0]\r\n\r\nweights = symbol (SymbolIdentifier \"weights\")
  $ M.replicate (3, 3) 1.0\r\nbias    = symbol (SymbolIdentifier \"bias\") $ M.rowVec
  $ V.replicate 3 0.0\r\n\r\noutput  = tanh (input `matMul` weights `addMatRows` bias)\r\n```\r\n\r\nYou
  just need to set the names for your symbolic variables, and you are good to go -
  `Synapse` will take care of the rest.\r\n\r\nLook at the `Synapse` implementation
  of some common operations:\r\n\r\n```Haskell\r\n(+) a b = symbolicBinaryOp (+) a
  b [(a, id), (b, id)]\r\n(*) a b = symbolicBinaryOp (*) a b [(a, (* b)), (b, (a *))]\r\n\r\nexp
  x = symbolicUnaryOp exp x [(x, (* exp x))]\r\nsin x = symbolicUnaryOp sin x [(x,
  (* cos x))]\r\n```\r\n\r\nProvided functions (`symbolicUnaryOp`, `symbolicBinaryOp`)
  expect an operation that will be performed on *values* of those symbols,\r\nsymbols
  themselves and a list of tuples,\r\nwhere each tuple represents a gradient (the
  symbol which gradient is taken and function that implements chain rule - multiplies
  already calculated gradient (*symbol*) by the gradient of the function (another
  *symbol*)).\r\nUsing those, defining new symbolic operations is very easy,\r\nand
  you should note that any composition of symbolic operations is itself a symbolic
  operation.\r\n\r\nThis implementation is really easy to use:\r\n\r\n```Haskell\r\na
  = symbol (SymbolIdentifier \"a\") 4\r\nb = symbol (SymbolIdentifier \"b\") 3\r\nc
  = renameSymbol (SymbolIdentifier \"c\") ((a * a) + (b * b))\r\n\r\ngetGradientsOf
  (a * (b + b) * a) `wrt` a  -- == 4 * 4 * 3\r\ngetGradientsOf (c * c) `wrt` c            --
  == 2 * 25\r\n\r\nnthGradient 2 (a * (b + b) * a) a         -- == 4 * 3\r\nnthGradient
  4 (sin c) c                   -- == sin c\r\n```\r\n\r\n`Synapse` does not care
  what is the type of your symbols: it might be `Int`, `Double`, `Vec`, `Mat`, anything
  that instantiates `Symbolic` typeclass -\r\ntypes just need to match with each other
  and with types of operations.\r\n\r\n### Neural networks\r\n\r\n`Synapse` takes
  as much of [Keras](https://keras.io/api/) API as it is possible, but is also provides
  additional abstractions leveraging Haskell's type system.\r\n\r\n#### Functions\r\n\r\nEverything
  that is a function in a common sense, is a function too in `Synapse`.\r\n\r\n`Activation`,
  `Loss`, `Metric`, `LearningRate`, `Constraint`, `Initializer`, `Regularizer` newtypes\r\njust
  wrap any plain Haskell function with needed type!\r\n\r\nThat means that to create
  new activation function, loss function, etc. you just need to create Haskell function
  with appropriate constrains.\r\n\r\n```Haskell\r\nsigmoid :: (Symbolic a, Floating
  a) => a -> a\r\nsigmoid x = recip $ exp (negate x) + symbolicOne x\r\n\r\nsigmoidActivation
  :: (Symbolic a, Floating a) => Activation a\r\nsigmoidActivation = Activation sigmoid\r\n```\r\n\r\n`symbolicOne`
  function represents constant that corresponds to identity element for given `x`.\r\nUsage
  of const literal `1.0` does not work, because that identity element also needs to
  know `x`s dimensions.\r\nIf `x` is matrix, you need to get `M.replicate (M.size
  x) 1.0`,\r\nnot the singleton `1.0`.\r\nWriting additional constraints like `Symbolic`
  and having to create constants using `symbolicOne` might seem tedious,\r\nbut that
  ensures type safety.\r\n\r\nYou can also specialize the function:\r\n\r\n```Haskell\r\ntype
  ActivationFn a = SymbolMat a -> SymbolMat a\r\n\r\nsigmoid :: (Symbolic a, Floating
  a) => ActivationFn a\r\nsigmoid x = recip $ exp (negate x) +. 1.0\r\n\r\nsigmoidActivation
  :: (Symbolic a, Floating a) => Activation a\r\nsigmoidActivation = Activation sigmoid\r\n```\r\n\r\nEven
  with all those limitations, it is still easy to create your own functions for any
  task.\r\n\r\n#### Layer system\r\n\r\n`AbstractLayer` typeclass is the most low-leveled
  abstraction of entire `Synapse` library.\r\n3 functions (`getParameters`, `updateParameters`
  and `symbolicForward`) are the backbone of entire neural networks interface.\r\nDocs
  on that typeclass as well as docs on those functions extensively describe invariants
  that `Synapse` expects from their implementations.\r\n\r\nWith the help of `Layer`
  existential datatype `Synapse` is able to\r\nbuild networks from any types that
  are instances of `AbstractLayer` typeclass,\r\nwhich means that this system is easily
  extendable.\r\n\r\n`Dense` layer, for example, supports regularizers, constraints,
  recording gradients on its forward operations, and that is built upon those 3 functions.\r\n\r\n####
  Training\r\n\r\nHere is the `train` function signature:\r\n\r\n```Haskell\r\ntrain\r\n
  \   :: (Symbolic a, Floating a, Ord a, Show a, RandomGen g, AbstractLayer model,
  Optimizer optimizer)\r\n    => model a\r\n    -> optimizer a\r\n    -> Hyperparameters
  a\r\n    -> Callbacks model optimizer a\r\n    -> g\r\n    -> IO (model a, [OptimizerParameters
  optimizer a], Vec (RecordedMetric a), g)\r\n```\r\n\r\nLet's break it down into
  pieces and examine that function.\r\n\r\n#### Models\r\n\r\n`model` represents any
  `AbstractLayer` instance, so it is the model with parameters that are going to be
  trained.\r\n\r\nAny layer can be a model, but more commonly you would use `SequentialModel`.\r\n`SequentialModel`
  is a newtype that wraps list of `Layer`s.\r\n`buildSequentialModel` function builds
  the model, ensuring that dimensions of layers match.\r\n\r\nThat is achieved by
  `LayerConfiguration` type alias and corresponding functions like `layerDense` and
  `layerActivation`.\r\n`LayerConfiguration` represents functions that are able to
  build a new layer upon the other layer, using information about its output dimension.\r\n\r\n```Haskell\r\nlayers
  = [ Layer . layerDense 1\r\n         , Layer . layerActivation (Activation tanh)\r\n
  \        , Layer . layerDense 1\r\n         ] :: [LayerConfiguration (Layer Double)]\r\n```\r\n\r\nYou
  just write your layers like that and let `buildSequentialModel` to figure out how
  to compose them.\r\n\r\nIt would look like this:\r\n\r\n```Haskell\r\nmodel = buildSequentialModel\r\n
  \           (InputSize 1)\r\n            [ Layer . layerDense 1\r\n            ,
  Layer . layerActivation (Activation tanh)\r\n            , Layer . layerDense 1\r\n
  \           ] :: SequentialModel Double\r\n```\r\n\r\n`InputSize` indicates size
  of input that will be supported by this model.\r\nModel can take any matrix of size
  `(n, i)`, where `i` was supplied as `InputSize i` when the model was built.\r\n\r\nSince
  `AbstractLayer` instance is a trainable model and `SequentialModel` is a model,
  it means that it is also an instance `AbstractLayer`.\r\nSome layers are inherently
  a composition of other layers (LSTM layers are the example) and `Synapse` supports
  this automatically.\r\n\r\n#### Optimizers\r\n\r\n`optimizer` represents any `Optimizer`
  instance.\r\nAny optimizer has its parameters (`OptimizerParameters`) which it uses
  to update parameters of a model.\r\n\r\nUpdate is done with the functions `optimizerUpdateStep`
  and `optimizerUpdateParameters`.\r\nThe second function is a mass update, so it
  needs gradients on all parameters of the model which are represented by symbolic
  matrices, while the first updates only one parameters which does not need to be
  symbolic, due to supplied exact gradient value.\r\n\r\nIt is pretty easy to implement
  your own optimizer.\r\nSee how `Synapse` implements `SGD`:\r\n\r\n```Haskell\r\ndata
  SGD a = SGD\r\n    { sgdMomentum :: a\r\n    , sgdNesterov :: Bool\r\n    } deriving
  (Eq, Show)\r\n\r\ninstance Optimizer SGD where\r\n    type OptimizerParameters SGD
  a = Mat a\r\n\r\n    optimizerInitialParameters _ parameter = zeroes (M.size parameter)\r\n\r\n
  \   optimizerUpdateStep (SGD momentum nesterov) (lr, gradient) (parameter, velocity)
  = (parameter', velocity')\r\n      where\r\n        velocity' = velocity *. momentum
  - gradient *. lr\r\n\r\n        parameter' = if nesterov\r\n                     then
  parameter + velocity' *. momentum - gradient *. lr\r\n                     else
  parameter + velocity'\r\n```\r\n\r\n#### Hyperparameters\r\n\r\nAny training has
  some hyperparameters that configure that training.\r\n\r\n```Haskell\r\ndata Hyperparameters
  a = Hyperparameters\r\n    { hyperparametersEpochs       :: Int\r\n    , hyperparametersBatchSize
  \   :: Int\r\n\r\n    , hyperparametersDataset      :: VecDataset a\r\n\r\n    ,
  hyperparametersLearningRate :: LearningRate a\r\n    , hyperparametersLoss         ::
  Loss a\r\n\r\n    , hyperparametersMetrics      :: Vec (Metric a)\r\n    }\r\n```\r\n\r\nThose
  hyperparameters include the number of epochs, batch size,\r\ndataset of vector samples
  (vector input and vector output),\r\nlearning rate function, loss function\r\nand
  metrics that will be recorded during training.\r\n\r\n#### Callbacks\r\n\r\n`Synapse`
  allows 'peeking' in the training process using callbacks system.\r\n\r\nSeveral
  type aliases\r\n(`CallbackFnOnTrainBegin`,\r\n`CallbackFnOnEpochBegin`,\r\n`CallbackFnOnBatchBegin`,\r\n`CallbackFnOnBatchEnd`,\r\n`CallbackFnOnEpochEnd`,\r\n`CallbackFnOnTrainEnd`)\r\nrepresent
  functions that take mutable references to training parameters and do something with
  them (read, print/save, modify, etc.).\r\n\r\nCallback system interface should be
  used with caution, because some changes might break the training completely,\r\nbut
  nonetheless, it is a very powerful instrument.\r\n\r\n#### Training process\r\n\r\nTraining
  itself consists of following steps:\r\n\r\n1. Training beginning (setting up initial
  parameters of model and optimizer)\r\n2. Epoch training (shuffling, batching and
  processing of the dataset)\r\n3. Batch training (update of parameters based on the
  result of batch processing, recording of metrics)\r\n4. Training end (collecting
  results of training)\r\n\r\nAll of that is handled by the `train` function.\r\n\r\nHere
  is an example of sine wave approximator which you could find at [tests](./test/)
  directory:\r\n\r\n```Haskell\r\nlet sinFn x = (-3.0) * sin (x + 5.0)\r\nlet model
  = buildSequentialModel (InputSize 1) [ Layer . layerDense 1\r\n                                               ,
  Layer . layerActivation (Activation cos)\r\n                                               ,
  Layer . layerDense 1\r\n                                               ] :: SequentialModel
  Double\r\nlet dataset = Dataset $ V.fromList $ [Sample (singleton x) (sinFn $ singleton
  x) | x <- [-pi, -pi+0.2 .. pi]]\r\n(trainedModel, _, losses, _) <- train model\r\n
  \                                  (SGD 0.2 False)\r\n                                   (Hyperparameters
  500 16 dataset (LearningRate $ const 0.01) (Loss mse) V.empty)\r\n                                   emptyCallbacks\r\n
  \                                  (mkStdGen 1)\r\n_ <- plot (PNG \"test/plots/sin.png\")\r\n
  \         [ Data2D [Title \"predicted sin\", Style Lines, Color Red] [Range (-pi)
  pi] [(x, unSingleton $ forward (singleton x) trainedModel) | x <- [-pi, -pi+0.05
  .. pi]]\r\n          , Data2D [Title \"true sin\", Style Lines, Color Green] [Range
  (-pi) pi] [(x, sinFn x) | x <- [-pi, -pi+0.05 .. pi]]\r\n          ]\r\nlet unpackedLosses
  = unRecordedMetric (unsafeIndex losses 0)\r\nlet lastLoss = unsafeIndex unpackedLosses
  (V.size unpackedLosses - 1)\r\nassertBool \"trained well enough\" (lastLoss < 0.01)\r\n```\r\n\r\n#####
  Prefix system\r\n\r\n`Synapse` manages gradients and parameters for layers with
  erased type information using prefix system.\r\n\r\n`SymbolIdentifier` is a prefix
  for name of symbolic parameters that are used in calculation.\r\nEvery used parameter
  should have unique name to be recognised by the autograd -\r\nit must start with
  given prefix and end with the numerical index of said parameter.\r\nFor example
  3rd layer with 2 parameters (weights and bias) should\r\nname its weights symbol
  `\"ml3w1\"` and name its bias symbol `\"ml3w2\"` (`\"ml3w\"` prefix will be supplied).\r\n\r\nPrefix
  system along with layer system require to carefully ensure all the invariants that
  `Synapse` imposes if you are willing to extend them (write your own layers, training
  loops, etc.).\r\nBut the user of this library should not worry about those getting
  in the way, because they are hidden behind an abstraction.\r\n\r\n## \U0001F4D6
  Future plans \U0001F4D6\r\n\r\n`Synapse` library is still under development and
  there is work to be done on:\r\n\r\n* **Performance**\r\n\r\n    `Synapse` 'brings
  its own guns' and, although it makes the library independent,\r\nit could make `Synapse`
  to miss on some things that are battle-tested and tuned to performance.\r\n\r\n
  \   That is especially true for `Synapse.Tensors` implementations of `Vec` and `Mat`.\r\nThose
  are built upon [vector](https://hackage.haskell.org/package/vector) library,\r\nwhich
  is good, but it is not suitable for heavy numerical calculations.\r\n\r\n    [hmatrix](https://hackage.haskell.org/package/hmatrix)
  which offers numerical computations based on BLAS and LAPACK is way more efficient.\r\nIt
  would be great if `Synapse` library could work with any matrix backend.\r\n\r\n*
  **Tensors**\r\n\r\n    It is really a shame that `Synapse.Tensors` does not have
  actual tensors.\r\n`Tensor` datatype would allow to get rid of `Vec` and `Mat` datatypes
  in favour of more powerful abstraction.\r\nTensor broadcasting could also be created
  to address issues that `Symbolic` typeclass is trying to solve.\r\n`Tensor` datatype
  could even present a unified frontend for any matrix backend that `Synapse` could
  use.\r\n\r\n* **GPU support**\r\n\r\n    This clause addresses all of the issues
  above.\r\nIt would severely increase performance of `Synapse` library,\r\nand it
  would greatly work with backend-independent tensors.\r\nHaskell ecosystem offers
  great [accelerate](https://hackage.haskell.org/package/accelerate) library which
  could help with all those problems.\r\n\r\n* **More out-of-the-box solutions**\r\n\r\n
  \   At this point, `Synapse` does not offer a wide variety of layers, activations,
  models, optimizers out-of-the-box.\r\nSuppling more of them would definitely help.\r\n\r\n*
  **Developer abstractions**\r\n\r\n    Currently, `Synapse`'s backbones are `SymbolIdentifier`s
  and implementations of `AbstractLayer`,\r\nwhich might be cumbersome for developers
  and be a bit unreliable.\r\nIf those systems could work naturally, without some
  hardcoding of values, it would be great.\r\n\r\n* **More monads!**\r\n\r\n    `Synapse`
  library does not use a lot of Haskell instruments, like optics, monad transformers,
  etc.\r\nAlthough it makes the library easy for beginners,\r\nI am sure that some
  of those instruments can offer richer expressiveness for the code\r\nand also address
  the issue of 'Developer abstractions'.\r\n\r\n## ❤️ Contributions ❤️\r\n\r\n`Synapse`
  library would benefit from every contribution to it: docs, code, small advertisement
  - you name it.\r\n\r\nIf you want to help me in the development, you could always
  contact me -\r\nmy [GitHub profile](https://github.com/JktuJQ) has links to my socials.\r\n"
description-type: markdown
hash: efcf93aaff74daafc7559bce3a16fc9e3f077559bed4f2ea6d7c896ee3e37bae
homepage: https://github.com/JktuJQ/Synapse
latest: 0.1.0.1
license-name: MIT
maintainer: odmamontov@gmail.com
synopsis: Synapse is a machine learning library written in pure Haskell.
test-bench-deps:
  HUnit: '>=1.6.2 && <1.7'
  Synapse: '>=0'
  base: '>=4.17.2.1 && <=4.21.0.0'
  easyplot: '>=1.0 && <1.1'
  random: '>=1.3.0 && <1.4'
