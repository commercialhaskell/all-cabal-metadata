all-versions:
- 0.1.0.0
- 0.1.0.1
- 0.1.1.0
- 0.2.0.0
- 0.3.0.0
- 0.3.0.1
author: Lars Bruenjes
basic-deps:
  JuicyPixels: '>=0'
  MonadRandom: '>=0'
  STMonadTrans: '>=0'
  ad: '>=0'
  ansi-terminal: '>=0'
  array: '>=0'
  attoparsec: '>=0'
  base: '>=4.7 && <5'
  bytestring: '>=0'
  containers: '>=0'
  deepseq: '>=0'
  directory: '>=0'
  filepath: '>=0'
  ghc-typelits-natnormalise: '>=0'
  hspec: '>=0'
  kan-extensions: '>=0'
  lens: '>=0'
  monad-par: '>=0'
  monad-par-extras: '>=0'
  mtl: '>=0'
  neural: '>=0'
  parallel: '>=0'
  pipes: '>=0'
  pipes-bytestring: '>=0'
  pipes-safe: '>=0'
  pipes-zlib: '>=0'
  profunctors: '>=0'
  random: '>=0'
  reflection: '>=0'
  text: '>=0'
  transformers: '>=0'
  typelits-witnesses: '>=0'
  vector: '>=0'
  vector-sized: '>=0'
changelog: ''
changelog-type: ''
description: "# neural - Neural Nets in native Haskell\n\n[![Build Status](https://travis-ci.org/brunjlar/neural.svg?branch=master)](https://travis-ci.org/brunjlar/neural)\n\n##
  Motivation\n\nThe goal of this project is to provide a flexible framework for \n[neural
  networks](https://en.wikipedia.org/wiki/Artificial_neural_network) \n(and similar
  parameterized models) in Haskell.\n\nThere are already a couple of neural network
  libraries out there on Hackage, but as far as I can tell,\nthey either\n\n- are
  wrappers for an engine written in another language or\n- offer a limitted choice
  of network architectures, training algorithms or error functions\n  or are not easily
  extensible.\n\nThe goal of this library is to have an implementation in native Haskell
  (reasonably efficient)\nwhich offers maximal flexibility.\n\nFurthermore, [gradient
  descent/backpropagation](https://en.wikipedia.org/wiki/Backpropagation) should work
  automatically, using\n[automatic differentiation](https://hackage.haskell.org/package/ad-4.3.2/docs/Numeric-AD.html).\nThis
  means that new and complicated activation functions and/or network architectures
  can be used without the need\nto first calculate derivatives by hand.\n\nIn order
  to provide a powerful and flexible API, models are constructed using *components*
  which behave as if they implemented the\n[Arrow and ArrowChoice](https://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Arrow.html)
  typeclasses. \nThey can therefore easily be combined and transformed.\n\nOnce a
  model has been constructed, it can be hooked up into a customized training algorithm
  using [pipes](https://hackage.haskell.org/package/pipes),\nso that various aspects
  of the algorithm (loading data, choosing random samples, reporting intermediate
  results, stop criterium etc.) can be defined in a modular,\ndecoupled way.\n\nEven
  though neural networks are the primary motivation for this project, any other kind
  of model can be\ndefined in the same framework, whenever the model depends on a
  collection of numerical parameters in a differentiable\nway. - One simple example
  for this would be [linear regression](https://en.wikipedia.org/wiki/Linear_regression).\n\n##
  Examples\n\nAt the moment, four [examples](examples) are included:\n\n- [sin](examples/sin)
  models the regression problem of approximating the sine function on the interval
  [0,2 pi].\n\n- [sqrt](examples/sqrt) models the similar regression problem of approximating
  the square root function\non the interval [0,4].\n\n- [iris](examples/iris) solves
  the famous [Iris Flower](https://en.wikipedia.org/wiki/Iris_flower_data_set) classification
  problem.\n\n- [MNIST](examples/MNIST) tackles the equally famous [MNIST](https://en.wikipedia.org/wiki/MNIST_database)
  problem of recognizing handwritten digits.\n"
description-type: markdown
hash: ead4c02f6f5d0ad634c417ec55c2c9b2a21e05e077bde6a14cd20d114bce9247
homepage: https://github.com/brunjlar/neural
latest: 0.3.0.1
license-name: MIT
maintainer: brunjlar@gmail.com
synopsis: Neural Networks in native Haskell
test-bench-deps:
  Glob: '>=0'
  MonadRandom: '>=0'
  base: '>=4.7 && <5'
  criterion: '>=0'
  doctest: '>=0'
  hspec: '>=0'
  neural: '>=0'
