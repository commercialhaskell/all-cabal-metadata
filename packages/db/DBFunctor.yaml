homepage: https://github.com/nkarag/DBFunctor#readme
changelog-type: markdown
hash: b3364601789374a4395eb607ce7cfd843f77df13b6b76a96a04e41db94e22919
test-bench-deps:
  MissingH: -any
  cereal: -any
  either: -any
  bytestring: -any
  base: ! '>=4.7 && <5'
  time: -any
  unordered-containers: -any
  text: -any
  containers: -any
  cassava: -any
  transformers: -any
  deepseq: -any
  DBFunctor: -any
  vector: -any
maintainer: nkarag@gmail.com
synopsis: DBFunctor - Functional Data Management =>  ETL/ELT Data Processing in Haskell
changelog: "\uFEFF# Changelog for DBFunctor\n\n### 0.1.0.0\n - Initial Version. Includes
  a full-working version of \n\t - Julius: A type-level Embedded Domain Specific (EDSL)
  Language for ETL\n\t - all common Relational Algebra operations, \n\t - the ETL
  Mapping and other typical ETL constructs and operations\n\t - operations applicable
  to all kinds of tabular data\n\t - In-memory, database-less data processing.\n\t
  \n### 0.1.1.0\n - Includes various enhancements (most notable is DML operations
  support) and fixes \n\t - Issue #1: Implemented agg function string_agg (listagg
  in Oracle) and the corresponding Julius clause\n\t - Issue #2: Implemented Julius
  Aggregate clauses: CountDist and CountStar\n\t - Issue #6 DML Enhancements\n\t \t-
  Implement Update Julius Clause\n\t \t- Implement Insert Operation and corresponding
  Julius Clause (both single RTuple INSERT and INSERT INTO SELECT) \n\t \t- Implement
  Merge/Upsert operation and corresponding Julius clause\n\t \t- Implement semi-join
  operation and corresponding Julius clause\n\t \t- Implement anti-join operation
  and corresponding Julius clause\n\t \t- Implement Delete operation and corresponding
  Julius clause\n\t - Issue #5 : Add support for UTCTime\n\t - Solve the CSV orphan
  instances problem by defining CSV with newtype\n\t - Fix problem with order by.
  I have noticed the following bug:\n```Haskell\n\t\t\t\t\t\t\t\t>>> let t1 = RDate
  {rdate = \"01/12/1990\", dtformat = \"DD/MM/YYYY\"}\n\t\t\t\t\t\t\t\t>>> let t2
  = RDate {rdate = \"1/12/1991\", dtformat = \"DD/MM/YYYY\"}\n\t\t\t\t\t\t\t\t>>>\tcompare
  t1 t2\n\t\t\t\t\t\t\t\t>>>\tEQ\n```\nFix: \n- Redefine the RDataType Ord instance
  based on the compare function instead of the (<=) function.\n- When comparing RDate
  types, convert them first to RTimeStamps and then compare these ones\n- The previous
  point apply it also to the Eq instance for RDataType.\n\n"
basic-deps:
  MissingH: -any
  cereal: -any
  either: -any
  bytestring: -any
  base: ! '>=4.7 && <5'
  time: -any
  unordered-containers: -any
  text: -any
  containers: -any
  cassava: -any
  transformers: -any
  deepseq: -any
  DBFunctor: -any
  vector: -any
all-versions:
- 0.1.0.0
- 0.1.1.0
author: Nikos Karagiannidis
latest: 0.1.1.0
description-type: markdown
description: "\uFEFF![dbfunctor logo](./dbfunctor.png)\r\n# DBFunctor:  Functional
  Data Management\r\n## ETL/ELT* Data Processing in Haskell\r\n**[DBFunctor](https://hackage.haskell.org/package/DBFunctor)**
  is a [Haskell](https://haskell-lang.org/) library for *ETL/ELT[^1]* data processing
  of tabular data. What does this mean?\r\nIt simply means that whenever you have
  a ***data analysis*, *data preparation*, or *data transformation* task** and you
  want to do it with Haskell type-safe code, that you enjoy, love and trust so much,
  now you can! \r\n### Main Features\r\n 1. **[Julius](https://hackage.haskell.org/package/DBFunctor-0.1.0.0/docs/Etl-Julius.html)
  DSL: A Type-Level Embedded Domain Specific  Language (EDSL) for ETL**\r\nProvides
  an intuitive type-level Embedded Domain Specific (EDSL) Language called *Julius*
  for expressing complex data flows (i.e., ETL flows)  but also for performing SQL-like
  data analysis. For more info check this [Julius tutorial](https://github.com/nkarag/haskell-DBFunctor/blob/master/doc/JULIUS-TUTORIAL.md).\r\n
  2. **Supports all known relational algrebra operations**\r\nJulius supports all
  known relational algebra operations (selection, projection, inner/outer join, grouping,
  ordering, aggregation, set operations etc.)\r\n 3. **Provides the ETL Mapping and
  other typical ETL constructs and operations**\r\nJulius implements typical ETL constructs
  such the *Column Mapping* and the *ETL Mapping*.\r\n 4. **Applicable to all kinds
  of tabular data**\r\nIt is applicable to all kinds of \"tabular data\" (see explanation
  below)\r\n 5. **In-memory, database-less data processing**\r\nData transformations
  or queries can run *in-memory*, within your Haskell code, without the need for a
  database to process your data.  \r\n6. **Offloading to a database for heavy queries/data
  transformations**\r\nIn addition, a query or data transformation  can be *offloaded
  to a Database*, when data don't fit in memory, or heavy data processing over large
  volumes of data is required. The result can be fetched into the client's memory
  \ (i.e., where your haskell code runs) in the `RTable` data structure  (see below),
  or stored in a database staging table.\r\n 7. **Workflow Operations**\r\nJulius
  provides common workflow operations. Workflows provide the ability to combine the
  evaluation of several different Julius Expressions (i.e., data pipelines) in an
  arbitrary logic. Examples of such operations include:\r\n - Ability to handle a
  failure of some operation in a Julius expression:\r\n\t - retry the failed operation
  (after corrective actions have taken place) and continue the evaluation of the Julius
  expression from this point onward.\r\n\t - skip the failed operation and move on
  with the rest operations in the pipeline.\r\n\t - restart the Julius expression
  from the beginning\r\n\t - terminate the Julius expression and skip all pending
  operations\r\n - Ability to start a Julius expression based on the success or failure
  result of another one\r\n - Ability to fork several different Julius expressions
  that will run concurrently\r\n - Conditional execution of Julius expressions and
  iteration functionality\r\n - Workflow hierarchy (i.e., flows, subflows etc.)\r\n
  8. **\"Declarative ETL\"**\r\nEnables *declarative ETL* implementation  in the same
  sense that SQL is declarative for querying data (see more below).\r\n### Typical
  examples of DBFunctor use-cases\r\n - **Build database-less Haskell apps.** Build
  your data processing haskell apps without the need to import your data in a database
  for querying functionality or any for executing any data transformations. Analyze
  your CSV files in-place with plain haskell code (*for Haskellers!*).\r\n - **Data
  Preparation.** I.e., clean-up data, calculate derived fields and variables, group
  by and aggregate etc., in order to feed some machine learning algorithm (*for Data
  Scientists*).\r\n - **Data Transformation.** in order to transform data from Data
  Model A to Data Model B (typical use-case *for Data Engineers* who perform ETL/ELT[^1]
  tasks for feeding Data Warehouses or Data Marts)\r\n - **Data Exploration.** Ad
  hoc data analysis tasks, in order to explore a data set for several purposes such
  as to find business insights and solve a specific business problem, or maybe to
  do data profiling in order to evaluate the quality of the data coming from a data
  source, etc (*for Data Analysts*).\r\n - **Business Intelligence.** Build reports,
  or dashboards in order to share business insights with others and drive decision
  making process (*for BI power-users*)\r\n\r\n[^1]:  **ETL** stands for **Extract
  Transform and Load** and is the standard technology for accomplishing data management
  tasks in Data Warehouses / Data Marts and in general for preparing data for any
  analytic purposes (Ad hoc queries, data exploration/data analysis, Reporting and
  Business Intelligence, feeding Machine Learning algorithms, etc.). **ELT** is a
  newer variation of ETL and means that the data are first Loaded into their final
  destination and then the data transformation runs in-place (as opposed to running
  at a separate staging area on possibly a different server)).\r\n\r\n### When to
  Use it?\r\nDBFunctor should be used whenever a data analysis, or data manipulation,
  or data transformation task, over *tabular data*, must be performed and we wish
  to perform it with Haskell code -yielding all the well-known (to Haskellers) benefits
  from doing that- without the need to use a database query engine for this task.\r\nDBFunctor
  provides an in-memory data structure called `RTable`, which implements the concept
  of a *Relational Table* (which -simply put- is a set of tuples) and all relevant
  relational algebra operations (Selection, Projection, Inner Join, Outer Joins, aggregations,
  Group By, Set Operations etc.).\r\nMoreover, it implements the concept of *Column
  Mapping* (for deriving new columns based on existing ones - by splitting , merging
  , or with any other possible combination using a lambda expression or a function
  to define the new value) and that of the *ETL Mapping*, which is the equivalent
  of a \"mapping\" in an ETL tool (like Informatica, Talend, Oracle Data Integrator,
  SSIS, Pentaho, etc.). With this powerful construct, one can **build arbitrary complex
  data pipelines**, which can enable any type of data transformations and all these
  **by writing Haskell code.**\r\n### What Kinds of Data?\r\nWith the term \"tabular
  data\" we mean any type of data that can be mapped to an RTable (e.g., CSV (or any
  other delimiter), DB Table/Query, JSON etc). Essentially, for a Haskell data type
  `a`to be \"tabular\", one must implement the following functions:\r\n```haskell\r\n
  \  toRTable :: RTableMData -> a -> RTable\r\n   fromRTable :: RTableMData -> RTable
  -> a\r\n```    \r\nThese two functions implement the \"logic\" of transforming data
  type `a` to/from an RTable based on specific RTable Metadata, which specify the
  column names and data types of the RTable, as well as (optionally) the primary key
  constraint, and/or alternative unique constraints (i.e., similar information provided
  with a CREATE TABLE statement in SQL) .\r\nBy implementing these two functions,
  data type `a` essentially becomes an instance of the type `class RTabular` and thus
  can be transformed with the  DBFunctor package. Currently, we have implemented a
  CSV data type (any delimeter allowed), based one the [Cassava](https://github.com/haskell-hvr/cassava)
  library, in order to enable data transformations over CSV files.\r\n### Current
  Status and Roadmap\r\nCurrently (version DBFunctor-0.1.0.0), the DBFunctor package
  **is stable for in-memory data transformation and queries of CSV files**  (any delimiter
  allowed), with the **Julius EDSL** (module Etl.Julius) , or directly via RTable
  functions (module RTable.Core). The use of the Julius language is strongly recommended
  because it simplifies greatly and standardizes the creation of complex ETL flows.\r\nAll
  in all, currently main features from #1 to #5 (from the list above) have been implemented
  and main features > #5  are future work that will be released in later versions.
  \r\n### Future Vision -> Declarative ETL\r\nOur ultimate goal is, eventually to
  make DBFunctor the **first *Declarative library for ETL/ELT, or data processing
  in general***, by exploiting the virtues of functional programming and Haskell strong
  type system in particular.\r\nHere we use \"declarative\"  in the same sense that
  SQL is a declarative language for querying data. (You only have to state what data
  you want to be returned and you don't care about how this will be accomplished -
  the DBMS engine does this for you behind the scenes).\r\nIn the same manner,  ideally,
  one should only need to code the desired data transformation from a *source schema*
  to a *target schema*, as well as all the *data integrity constraints* and *business
  rules* that should hold after the transformation and not having to define all the
  individual steps for implementing the transformation, as it is the common practice
  today. This will yield tremendous benefits compared to common ETL challenges faced
  today and change the way we build data transformation flows. Just to name a few:\r\n
  - Automated ETL coding driven by Source-to-Target mapping and business rules\r\n
  - ETL code correctness out-of-the-box\r\n - Data Integrity / Business Rules controls
  automatically embedded in your ETL code\r\n - Self-documented ETL code (Your documentation
  i.e., the Source-to-Target mapping and the business rules, is also the only code
  you need to write!)\r\n - Drastically minimize time-to-market for delivering Data
  Marts and Data Warehouses, or simply implementing Data Analysis tasks.\r\n\r\nThe
  above is inspired by the theoretical work on Categorical Databases by David Spivak,\r\n###
  Available Modules\r\n*DBFunctor* consists of the following set of Haskell modules:\r\n*
  **RTable.Core**: Implements the relational Table concept. Defines all necessary
  data types like `RTable` and `RTuple` as well as basic relational algebra operations
  on RTables.\r\n* **Etl.Julius**: A simple Embedded DSL for ETL/ELT data processing
  in Haskell\r\n* **RTable.Data.CSV**:  Implements `RTable` over CSV (TSV, or any
  other delimiter) files logic. It is based on the [Cassava](https://github.com/haskell-hvr/cassava)
  library.\r\n### A Very Simple Example\r\n In this example, we will load a CSV file,
  turn it into an RTable and then issue a very simple query on it and print the result,
  just to show the whole concept.\r\nSo  lets say we have a CSV file called test-data.csv.
  The file stores table metadata from an Oracle database. Each row represents a table
  stored in the database. Here is a small extract from the csv file:\r\n\r\n    $
  head test-data.csv\r\n        OWNER,TABLE_NAME,TABLESPACE_NAME,STATUS,NUM_ROWS,BLOCKS,LAST_ANALYZED\r\n
  \       APEX_030200,SYS_IOT_OVER_71833,SYSAUX,VALID,0,0,06/08/2012 16:22:36\r\n
  \       APEX_030200,WWV_COLUMN_EXCEPTIONS,SYSAUX,VALID,3,3,06/08/2012 16:22:33\r\n
  \       APEX_030200,WWV_FLOWS,SYSAUX,VALID,10,3,06/08/2012 22:01:21\r\n        APEX_030200,WWV_FLOWS_RESERVED,SYSAUX,VALID,0,0,06/08/2012
  16:22:33\r\n        APEX_030200,WWV_FLOW_ACTIVITY_LOG1$,SYSAUX,VALID,1,29,07/20/2012
  19:07:57\r\n        APEX_030200,WWV_FLOW_ACTIVITY_LOG2$,SYSAUX,VALID,14,29,07/20/2012
  19:07:57\r\n        APEX_030200,WWV_FLOW_ACTIVITY_LOG_NUMBER$,SYSAUX,VALID,1,3,07/20/2012
  19:08:00\r\n        APEX_030200,WWV_FLOW_ALTERNATE_CONFIG,SYSAUX,VALID,0,0,06/08/2012
  16:22:33\r\n        APEX_030200,WWV_FLOW_ALT_CONFIG_DETAIL,SYSAUX,VALID,0,0,06/08/2012
  16:22:33\r\n**1. Turn the CSV file into an RTable**\r\nThe first thing we want to
  do is to read the file and turn it into an RTable. In order to do this we need to
  define the RTable Metadata, which is the same information one can provide in an
  SQL CREATE TABLE statement, i,e, column names, column data types and integrity constraints
  (Primary Key, Unique Key only - no Foreign Keys). So lets see how this is done:
  \    \r\n```haskell\r\n    -- Define table metadata\r\n    src_DBTab_MData :: RTableMData\r\n
  \   src_DBTab_MData =\r\n\t    createRTableMData   (   \"sourceTab\"  -- table name\r\n
  \                                ,[  (\"OWNER\", Varchar)                                      --
  Owner of the table\r\n                                    ,(\"TABLE_NAME\", Varchar)
  \                               -- Name of the table\r\n                                    ,(\"TABLESPACE_NAME\",
  Varchar)                           -- Tablespace name\r\n                                    ,(\"STATUS\",Varchar)
  \                                    -- Status of the table object (VALID/IVALID)\r\n
  \                                   ,(\"NUM_ROWS\", Integer)                                  --
  Number of rows in the table\r\n                                    ,(\"BLOCKS\",
  Integer)                                    -- Number of Blocks allocated for this
  table\r\n                                    ,(\"LAST_ANALYZED\", Timestamp \"MM/DD/YYYY
  HH24:MI:SS\")   -- Timestamp of the last time the table was analyzed (i.e., gathered
  statistics)\r\n                                  ]\r\n                             )\r\n
  \                               [\"OWNER\", \"TABLE_NAME\"] -- primary key\r\n                                []
  -- (alternative) unique keys\r\n    main :: IO ()\r\n    main = do\r\n       --
  read source csv file\r\n       srcCSV <- readCSV \"./app/test-data.csv\"    \r\n
  \      let\r\n         -- turn source csv to an RTable\r\n         src_DBTab = toRTable
  src_DBTab_MData srcCSV\r\n    ...\r\n```\r\nWe have used the following functions:\r\n```haskell\r\n--
  | createRTableMData : creates RTableMData from input given in the form of a list\r\n--
  \  We assume that the column order of the input list defines the fixed column order
  of the RTuple.\r\ncreateRTableMData ::\r\n        (RTableName, [(ColumnName, ColumnDType)])\r\n
  \       -> [ColumnName]     -- ^ Primary Key. [] if no PK exists\r\n        -> [[ColumnName]]
  \  -- ^ list of unique keys. [] if no unique keys exists\r\n        -> RTableMData\r\n```\r\nin
  order to define the RTable metadata.\r\nFor reading the CSV file we have used:\r\n```haskell\r\n--
  | readCSV: reads a CSV file and returns a CSV data type (Treating CSV data as opaque
  byte strings)\r\nreadCSV ::\r\n    FilePath  -- ^ the CSV file\r\n    -> IO CSV
  \ -- ^ the output CSV type\r\n```\r\nFinally, in order to turn the CSV data type
  into an RTable, we have used function:\r\n```haskell\r\ntoRTable :: RTableMData
  -> CSV -> RTable\r\n```\r\nwhich comes from the `RTabular` type class instance of
  the `CSV` data type.\r\n **2. Query the RTable**\r\nOnce we have created an RTable,
  we can issue queries on it, or apply any type of data transformations. Note that
  due to immutability, each query or data transformation creates a new RTable.\r\n
  We will now issue the following query:\r\nWe return all the rows, which correspond
  to some filter predicate - in particular all rows where the `TABLE_NAME` includes
  some search string and the `LAST_ANALYZED` field is greater than an input date.\r\n\r\nFor
  this we use the Julius EDSL, in order to express the query and then with the function
  \r\n`runJulius`, we evaluate the expression into an RTable.\r\n```haskell\r\nrunJulius
  :: ETLMappingExpr -> IO RTable\r\n```\r\nHere is the Julius expression that yield
  the desired results.\r\n```haskell\r\njulExpr srch dtstr rtab = \r\n            EtlMapStart
  \r\n                :-> (EtlR $\r\n                        ROpStart\r\n                        :.
  \ (Filter (From $ Tab rtab) $\r\n                                FilterBy (\\t ->
  case instrRText (RText srch) (t <!> \"TABLE_NAME\") of\r\n                                                    Just
  p  -> True\r\n                                                    Nothing -> False\r\n
  \                                               &&\r\n                                                (t
  <!> \"LAST_ANALYZED\") >= (RTime $ toRTimestamp \"DD/MM/YYYY\" dtstr)\r\n                                         )\r\n
  \                           )\r\n                    )\r\n```\r\nA Julius expression
  is a *data processing chain* consisting of various Relational Algebra operations
  `(EtlR $ ...)`  and/or column mappings `(EtlC $ ...)` connected together via the
  `:->` data constructor,  of the form (Julius expressions are read *from top-to-bottom
  \ or from left-to-right*):\r\n```haskell\r\nmyJulExpression =\r\n\tEtlMapStart\r\n\t:->
  (EtlC $ ...)  -- this is a Column Mapping\r\n\t:-> (EtlR $   -- this is a series
  of Relational Algebra Operations\r\n\t     ROpStart\r\n\t  :. (Rel Operation 1)
  -- a relational algebra operation\r\n\t  :. (Rel Operation 2))\r\n\t:-> (EtlC $
  ...)  -- This is another Column Mapping\r\n\t:-> (EtlR $ -- more relational algebra
  operations\r\n\t     ROpStart\r\n\t  :. (Rel Operation 3)\r\n\t  :. (Rel Operation
  4)\r\n\t  :. (Rel Operation 5))\r\n\t:-> (EtlC $ ...) -- This is Column Mapping
  3\r\n\t:-> (EtlC $ ...) -- This is Column Mapping 4\r\n\t...\r\n```\r\nIn our example,
  the Julius expression consists only of a single relational algebra operation, namely
  a `Filter` operation, which uses an RTuple  predicate of the form \t`RTuple -> Bool`
  to filter out RTuples (i.e., rows) that dont satisfy this predicate. The predicate
  is expressed as the lambda expression:\r\n```haskell\r\nFilterBy (\\t -> case instrRText
  (RText srch) (t <!> \"TABLE_NAME\") of\r\n                                                    Just
  p  -> True\r\n                                                    Nothing -> False\r\n
  \                                               &&\r\n                                                (t
  <!> \"LAST_ANALYZED\") >= (RTime $ toRTimestamp \"DD/MM/YYYY\" dtstr)\r\n```\r\nWe
  use the [instrRText](https://hackage.haskell.org/package/DBFunctor-0.1.0.0/docs/RTable-Core.html#g:16)
  function to find these table_name values that include the `srch` string. Also, we
  use the [toRTimestamp](https://hackage.haskell.org/package/DBFunctor-0.1.0.0/docs/RTable-Core.html#g:15)
  function, in order to turn the date string `dtstr` into an `RTimestamp` data type
  and compare it against the `LAST_ANALYZED` column,\r\n\r\nFinally, in order to print
  the result of the query on the screen, we use the \r\n```haskell\r\nprintfRTable
  :: RTupleFormat -> RTable -> IO()\r\n``` \r\nfunction, which brings printf-like
  functionality into the printing of RTables\r\nAnd here is the output:\r\n```\r\n$
  stack exec -- exampleDBFunctor\r\n\r\nPrint all tables that incude a \"search string\"
  in their name and have been analyzed after a specific date\r\n\r\nGive me the search
  string:\r\nFLOW\r\nGive me the date in \"DD/MM/YYYY\" format:\r\n01/04/2018\r\n---------------------------------------------------------------------------------------------------------------------------------\r\nOWNER
  \          TABLE_NAME                        TABLESPACE_NAME     STATUS     NUM_ROWS
  \    BLOCKS     LAST_ANALYZED\r\n~~~~~           ~~~~~~~~~~                        ~~~~~~~~~~~~~~~
  \    ~~~~~~     ~~~~~~~~     ~~~~~~     ~~~~~~~~~~~~~\r\nAPEX_040100     WWV_FLOW_ACTIVITY_LOG1$
  \          SYSAUX              VALID      4052         155        04/04/2018 18:19:56\r\nAPEX_040100
  \    WWV_FLOW_ACTIVITY_LOG2$           SYSAUX              VALID      1771         92
  \        16/04/2018 17:33:16\r\nAPEX_040100     WWV_FLOW_ACTIVITY_LOG_NUMBER$     SYSAUX
  \             VALID      1            3          10/04/2018 16:09:25\r\nAPEX_040100
  \    WWV_FLOW_COMPANIES                SYSAUX              VALID      10           3
  \         16/04/2018 17:33:13\r\nAPEX_040100     WWV_FLOW_DATA                     SYSAUX
  \             VALID      109          155        16/04/2018 16:06:38\r\nAPEX_040100
  \    WWV_FLOW_DEBUG_MESSAGES2          SYSAUX              VALID      0            0
  \         10/04/2018 16:09:25\r\nAPEX_040100     WWV_FLOW_FND_USER                 SYSAUX
  \             VALID      50           3          05/04/2018 18:09:23\r\nAPEX_040100
  \    WWV_FLOW_PAGE_CACHE               SYSAUX              VALID      22           3
  \         05/04/2018 18:35:18\r\nAPEX_040100     WWV_FLOW_SESSIONS$                SYSAUX
  \             VALID      182          26         16/04/2018 16:07:13\r\nAPEX_040100
  \    WWV_FLOW_USER_ACCESS_LOG1$        SYSAUX              VALID      127          5
  \         11/04/2018 18:27:17\r\nAPEX_040100     WWV_FLOW_USER_ACCESS_LOG2$        SYSAUX
  \             VALID      39           5          16/04/2018 17:30:59\r\nAPEX_040100
  \    WWV_FLOW_USER_ACCESS_LOG_NUM$     SYSAUX              VALID      1            3
  \         12/04/2018 16:30:05\r\nAPEX_040100     WWV_FLOW_WORKSHEET_CONDITIONS     SYSAUX
  \             VALID      501          18         16/04/2018 17:33:03\r\nAPEX_040100
  \    WWV_FLOW_WORKSHEET_GROUP_BY       SYSAUX              VALID      22           3
  \         03/04/2018 17:44:07\r\nAPEX_040100     WWV_FLOW_WORKSHEET_RPTS           SYSAUX
  \             VALID      505          16         16/04/2018 17:33:01\r\nFLOWS_FILES
  \    WWV_FLOW_FILE_OBJECTS$            SYSAUX              VALID      302          16
  \        13/04/2018 18:00:05\r\n\r\n\r\n16 rows returned\r\n---------------------------------------------------------------------------------------------------------------------------------\r\n```\r\n**Here
  is the complete example**.\r\n```haskell\r\n{-# LANGUAGE OverloadedStrings #-}\r\n\r\nmodule
  Main where\r\n\r\nimport  Etl.Julius  \r\nimport  RTable.Data.CSV     (CSV, readCSV,
  toRTable, writeCSV)\r\nimport  Data.Text.IO as T   (getLine)\r\n\r\n-- This is the
  input source table metadata\r\n-- It includes the tables stored in an imaginary
  database\r\nsrc_DBTab_MData :: RTableMData\r\nsrc_DBTab_MData = \r\n    createRTableMData
  \  (   \"sourceTab\"  -- table name\r\n                            ,[  (\"OWNER\",
  Varchar)                                      -- Owner of the table\r\n                                ,(\"TABLE_NAME\",
  Varchar)                                -- Name of the table\r\n                                ,(\"TABLESPACE_NAME\",
  Varchar)                           -- Tablespace name\r\n                                ,(\"STATUS\",Varchar)
  \                                    -- Status of the table object (VALID/IVALID)\r\n
  \                               ,(\"NUM_ROWS\", Integer)                                  --
  Number of rows in the table\r\n                                ,(\"BLOCKS\", Integer)
  \                                   -- Number of Blocks allocated for this table\r\n
  \                               ,(\"LAST_ANALYZED\", Timestamp \"MM/DD/YYYY HH24:MI:SS\")
  \  -- Timestamp of the last time the table was analyzed (i.e., gathered statistics)
  \r\n                            ]\r\n                        )\r\n                        [\"OWNER\",
  \"TABLE_NAME\"] -- primary key\r\n                        [] -- (alternative) unique
  keys\r\n\r\n-- Result RTable metadata\r\nresult_tab_MData :: RTableMData\r\nresult_tab_MData
  = \r\n    createRTableMData   (   \"resultTab\"  -- table name\r\n                            ,[
  \ (\"OWNER\", Varchar)                                      -- Owner of the table\r\n
  \                               ,(\"TABLE_NAME\", Varchar)                                --
  Name of the table\r\n                                ,(\"LAST_ANALYZED\", Timestamp
  \"MM/DD/YYYY HH24:MI:SS\")   -- Timestamp of the last time the table was analyzed
  (i.e., gathered statistics) \r\n                            ]\r\n                        )\r\n
  \                       [\"OWNER\", \"TABLE_NAME\"] -- primary key\r\n                        []
  -- (alternative) unique keys\r\n\r\nmain :: IO ()\r\nmain = do\r\n     -- read source
  csv file\r\n    srcCSV <- readCSV \"./app/test-data.csv\"\r\n\r\n    putStrLn \"\\nPrint
  all tables that incude a \\\"search string\\\" in their name and have been analyzed
  after a specific date\\n\"\r\n    putStrLn \"Give me the search string: \"\r\n    search
  <- T.getLine\r\n\r\n    putStrLn \"Give me the date in \\\"DD/MM/YYYY\\\" format:
  \"\r\n    datestr <- Prelude.getLine\r\n        \r\n\r\n    -- print source RTable
  first n rows\r\n    -- RTable A\r\n    resultRTab <- runJulius $ julExpr search
  datestr $ toRTable src_DBTab_MData srcCSV \r\n    printfRTable (  \r\n                    --
  this is the equivalent when printing on the screen a list of columns, defined in
  a SELECT clause in SQL\r\n                    genRTupleFormat [\"OWNER\", \"TABLE_NAME\",
  \"TABLESPACE_NAME\", \"STATUS\", \"NUM_ROWS\", \"BLOCKS\", \"LAST_ANALYZED\"] genDefaultColFormatMap\r\n
  \                ) $ resultRTab  \r\n\r\n    -- save result to a CSV file\r\n    writeCSV
  \"./app/result-data.csv\" $ \r\n                    fromRTable result_tab_MData
  resultRTab\r\n    where\r\n        julExpr srch dtstr rtab = \r\n            EtlMapStart
  \r\n                :-> (EtlR $\r\n                        ROpStart\r\n                        :.
  \ (Filter (From $ Tab rtab) $\r\n                                FilterBy (\\t ->
  case instrRText (RText srch) (t <!> \"TABLE_NAME\") of\r\n                                                    Just
  p  -> True\r\n                                                    Nothing -> False\r\n
  \                                               &&\r\n                                                (t
  <!> \"LAST_ANALYZED\") >= (RTime $ toRTimestamp \"DD/MM/YYYY\" dtstr)\r\n                                         )\r\n
  \                           )\r\n                    )\r\n```\r\n### Julius DSL
  Tutorial \r\nWe have written [a Julius tutorial](https://github.com/nkarag/haskell-DBFunctor/blob/master/doc/JULIUS-TUTORIAL.md)
  to help you get started with Julius DSL. \r\n<a  name=\"howtorun\"></a> \r\n###
  How to run\r\n#### 1. Install Stack\r\nSee [this](https://docs.haskellstack.org/en/stable/GUIDE/)
  guide for help. If you have stack already installed, then we suggest you run a `stack
  upgrade`, in order to update it to the latest version and avoid any error messages
  due to bugs.\r\nThen run a `stack update`, in order to update the package index.\r\n####
  2. Initiate a project\r\n`$ stack new myDBFunctorProject`\r\n#### 3. Start coding
  with DBFunctor\r\nDon't forget:\r\n\r\n - to `import Etl.Julius` module\r\n - to
  use GHC extension `{-# LANGUAGE OverloadedStrings #-}`, since DBFunctor uses `Text`
  in all of its basic data types, this extension is necessary if you want to assign
  string literal values to an `RDataType` (the type of a column in an `RTable`)\r\n####
  4. Declare dependency with DBFunctor package\r\nEdit your package.yaml (or project.cabal)
  file and add the dependency to the DBFunctor package\r\n```\r\ndependencies:\r\n-
  DBFunctor\r\n```\r\nAlso, don't forget to add to your stack.yaml file the line:\r\n```\r\n
  \  extra-deps:\r\n    - DBFunctor-0.1.0.0\r\n```\r\n#### 5. Build and run you project\r\n```\r\n$
  stack build\r\n```\r\nRun\r\n``` \r\nstack exec -- myDBFunctorProject-exe\r\n```\r\n"
license-name: BSD-3-Clause
