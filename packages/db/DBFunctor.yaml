homepage: https://github.com/nkarag/haskell-DBFunctor#readme
changelog-type: markdown
hash: 4166e656f37301aecfcb7caca17cf51bfc0c9bbfd3eca0fe9529a672c4ad33ce
test-bench-deps:
  cereal: -any
  either: -any
  bytestring: -any
  base: '>=4.7 && <5'
  time: -any
  unordered-containers: -any
  text: -any
  containers: -any
  cassava: -any
  transformers: -any
  deepseq: -any
  DBFunctor: -any
  vector: -any
maintainer: nkarag@gmail.com
synopsis: DBFunctor - Functional Data Management =>  ETL/ELT Data Processing in Haskell
changelog: "\uFEFF# Changelog for DBFunctor\n\n### 0.1.0.0\n - Initial Version. Includes
  a full-working version of \n\t - Julius: A type-level Embedded Domain Specific (EDSL)
  Language for ETL\n\t - all common Relational Algebra operations, \n\t - the ETL
  Mapping and other typical ETL constructs and operations\n\t - operations applicable
  to all kinds of tabular data\n\t - In-memory, database-less data processing.\n\t
  \n### 0.1.1.0\n - Includes various enhancements (most notable is DML operations
  support) and fixes \n\t - Issue #1: Implemented agg function string_agg (listagg
  in Oracle) and the corresponding Julius clause\n\t - Issue #2: Implemented Julius
  Aggregate clauses: CountDist and CountStar\n\t - Issue #6 DML Enhancements\n\t \t-
  Implement Update Julius Clause\n\t \t- Implement Insert Operation and corresponding
  Julius Clause (both single RTuple INSERT and INSERT INTO SELECT) \n\t \t- Implement
  Merge/Upsert operation and corresponding Julius clause\n\t \t- Implement semi-join
  operation and corresponding Julius clause\n\t \t- Implement anti-join operation
  and corresponding Julius clause\n\t \t- Implement Delete operation and corresponding
  Julius clause\n\t - Issue #5 : Add support for UTCTime\n\t - Solve the CSV orphan
  instances problem by defining CSV with newtype\n\t - Fix problem with order by.
  I have noticed the following bug:\n```Haskell\n\t\t\t\t\t\t\t\t>>> let t1 = RDate
  {rdate = \"01/12/1990\", dtformat = \"DD/MM/YYYY\"}\n\t\t\t\t\t\t\t\t>>> let t2
  = RDate {rdate = \"1/12/1991\", dtformat = \"DD/MM/YYYY\"}\n\t\t\t\t\t\t\t\t>>>\tcompare
  t1 t2\n\t\t\t\t\t\t\t\t>>>\tEQ\n```\nFix: \n- Redefine the RDataType Ord instance
  based on the compare function instead of the (<=) function.\n- When comparing RDate
  types, convert them first to RTimeStamps and then compare these ones\n- The previous
  point apply it also to the Eq instance for RDataType.\n\n### 0.1.1.1\n- bumped a
  version number (the least significant) in order to upload on hackage a new version
  with correct github links\n\n### 0.1.2.0\n- Removed (indirect) dependency to random
  < 1.2 via package MissingH\n- Added support for UTCTime in RDataType \n\n### 0.1.2.1\n-
  Added test/data directory in the hackage uploaded tarball\n"
basic-deps:
  cereal: -any
  either: -any
  bytestring: -any
  base: '>=4.7 && <5'
  time: -any
  unordered-containers: -any
  text: -any
  containers: -any
  cassava: -any
  transformers: -any
  deepseq: -any
  DBFunctor: -any
  vector: -any
all-versions:
- 0.1.0.0
- 0.1.1.0
- 0.1.1.1
- 0.1.2.0
- 0.1.2.1
author: Nikos Karagiannidis
latest: 0.1.2.1
description-type: markdown
description: "\uFEFF![dbfunctor logo](./dbfunctor.png)\n# DBFunctor:  Functional Data
  Management\n## ETL/ELT* Data Processing in Haskell\n**[DBFunctor](https://hackage.haskell.org/package/DBFunctor)**
  is a [Haskell](https://haskell-lang.org/) library for *ETL/ELT[^1]* data processing
  of tabular data. What does this mean?\nIt simply means that whenever you have a
  ***data analysis*, *data preparation*, or *data transformation* task** and you want
  to do it with Haskell type-safe code, that you enjoy, love and trust so much, now
  you can! \n### Main Features\n 1. **[Julius](https://hackage.haskell.org/package/DBFunctor-0.1.0.0/docs/Etl-Julius.html)
  DSL: A Type-Level Embedded Domain Specific  Language (EDSL) for ETL**\nProvides
  an intuitive type-level Embedded Domain Specific (EDSL) Language called *Julius*
  for expressing complex data flows (i.e., ETL flows)  but also for performing SQL-like
  data analysis. For more info check this [Julius tutorial](https://github.com/nkarag/haskell-DBFunctor/blob/master/doc/JULIUS-TUTORIAL.md).\n
  2. **Supports all known relational algrebra operations**\nJulius supports all known
  relational algebra operations (selection, projection, inner/outer join, grouping,
  ordering, aggregation, set operations etc.)\n 3. **Provides the ETL Mapping and
  other typical ETL constructs and operations**\nJulius implements typical ETL constructs
  such the *Column Mapping* and the *ETL Mapping*.\n 4. **Applicable to all kinds
  of tabular data**\nIt is applicable to all kinds of \"tabular data\" (see explanation
  below)\n 5. **In-memory, database-less data processing**\nData transformations or
  queries can run *in-memory*, within your Haskell code, without the need for a database
  to process your data.  \n6. **Offloading to a database for heavy queries/data transformations**\nIn
  addition, a query or data transformation  can be *offloaded to a Database*, when
  data don't fit in memory, or heavy data processing over large volumes of data is
  required. The result can be fetched into the client's memory  (i.e., where your
  haskell code runs) in the `RTable` data structure  (see below), or stored in a database
  staging table.\n 7. **Workflow Operations**\nJulius provides common workflow operations.
  Workflows provide the ability to combine the evaluation of several different Julius
  Expressions (i.e., data pipelines) in an arbitrary logic. Examples of such operations
  include:\n - Ability to handle a failure of some operation in a Julius expression:\n\t
  - retry the failed operation (after corrective actions have taken place) and continue
  the evaluation of the Julius expression from this point onward.\n\t - skip the failed
  operation and move on with the rest operations in the pipeline.\n\t - restart the
  Julius expression from the beginning\n\t - terminate the Julius expression and skip
  all pending operations\n - Ability to start a Julius expression based on the success
  or failure result of another one\n - Ability to fork several different Julius expressions
  that will run concurrently\n - Conditional execution of Julius expressions and iteration
  functionality\n - Workflow hierarchy (i.e., flows, subflows etc.)\n 8. **\"Declarative
  ETL\"**\nEnables *declarative ETL* implementation  in the same sense that SQL is
  declarative for querying data (see more below).\n### Typical examples of DBFunctor
  use-cases\n - **Build database-less Haskell apps.** Build your data processing haskell
  apps without the need to import your data in a database for querying functionality
  or any for executing any data transformations. Analyze your CSV files in-place with
  plain haskell code (*for Haskellers!*).\n - **Data Preparation.** I.e., clean-up
  data, calculate derived fields and variables, group by and aggregate etc., in order
  to feed some machine learning algorithm (*for Data Scientists*).\n - **Data Transformation.**
  in order to transform data from Data Model A to Data Model B (typical use-case *for
  Data Engineers* who perform ETL/ELT[^1] tasks for feeding Data Warehouses or Data
  Marts)\n - **Data Exploration.** Ad hoc data analysis tasks, in order to explore
  a data set for several purposes such as to find business insights and solve a specific
  business problem, or maybe to do data profiling in order to evaluate the quality
  of the data coming from a data source, etc (*for Data Analysts*).\n - **Business
  Intelligence.** Build reports, or dashboards in order to share business insights
  with others and drive decision making process (*for BI power-users*)\n\n[^1]:  **ETL**
  stands for **Extract Transform and Load** and is the standard technology for accomplishing
  data management tasks in Data Warehouses / Data Marts and in general for preparing
  data for any analytic purposes (Ad hoc queries, data exploration/data analysis,
  Reporting and Business Intelligence, feeding Machine Learning algorithms, etc.).
  **ELT** is a newer variation of ETL and means that the data are first Loaded into
  their final destination and then the data transformation runs in-place (as opposed
  to running at a separate staging area on possibly a different server)).\n\n### When
  to Use it?\nDBFunctor should be used whenever a data analysis, or data manipulation,
  or data transformation task, over *tabular data*, must be performed and we wish
  to perform it with Haskell code -yielding all the well-known (to Haskellers) benefits
  from doing that- without the need to use a database query engine for this task.\nDBFunctor
  provides an in-memory data structure called `RTable`, which implements the concept
  of a *Relational Table* (which -simply put- is a set of tuples) and all relevant
  relational algebra operations (Selection, Projection, Inner Join, Outer Joins, aggregations,
  Group By, Set Operations etc.).\nMoreover, it implements the concept of *Column
  Mapping* (for deriving new columns based on existing ones - by splitting , merging
  , or with any other possible combination using a lambda expression or a function
  to define the new value) and that of the *ETL Mapping*, which is the equivalent
  of a \"mapping\" in an ETL tool (like Informatica, Talend, Oracle Data Integrator,
  SSIS, Pentaho, etc.). With this powerful construct, one can **build arbitrary complex
  data pipelines**, which can enable any type of data transformations and all these
  **by writing Haskell code.**\n### What Kinds of Data?\nWith the term \"tabular data\"
  we mean any type of data that can be mapped to an RTable (e.g., CSV (or any other
  delimiter), DB Table/Query, JSON etc). Essentially, for a Haskell data type `a`to
  be \"tabular\", one must implement the following functions:\n```haskell\n   toRTable
  :: RTableMData -> a -> RTable\n   fromRTable :: RTableMData -> RTable -> a\n```
  \   \nThese two functions implement the \"logic\" of transforming data type `a`
  to/from an RTable based on specific RTable Metadata, which specify the column names
  and data types of the RTable, as well as (optionally) the primary key constraint,
  and/or alternative unique constraints (i.e., similar information provided with a
  CREATE TABLE statement in SQL) .\nBy implementing these two functions, data type
  `a` essentially becomes an instance of the type `class RTabular` and thus can be
  transformed with the  DBFunctor package. Currently, we have implemented a CSV data
  type (any delimeter allowed), based one the [Cassava](https://github.com/haskell-hvr/cassava)
  library, in order to enable data transformations over CSV files.\n### Current Status
  and Roadmap\nCurrently (version DBFunctor-0.1.0.0), the DBFunctor package **is stable
  for in-memory data transformation and queries of CSV files**  (any delimiter allowed),
  with the **Julius EDSL** (module Etl.Julius) , or directly via RTable functions
  (module RTable.Core). The use of the Julius language is strongly recommended because
  it simplifies greatly and standardizes the creation of complex ETL flows.\nAll in
  all, currently main features from #1 to #5 (from the list above) have been implemented
  and main features > #5  are future work that will be released in later versions.
  \n### Future Vision -> Declarative ETL\nOur ultimate goal is, eventually to make
  DBFunctor the **first *Declarative library for ETL/ELT, or data processing in general***,
  by exploiting the virtues of functional programming and Haskell strong type system
  in particular.\nHere we use \"declarative\"  in the same sense that SQL is a declarative
  language for querying data. (You only have to state what data you want to be returned
  and you don't care about how this will be accomplished - the DBMS engine does this
  for you behind the scenes).\nIn the same manner,  ideally, one should only need
  to code the desired data transformation from a *source schema* to a *target schema*,
  as well as all the *data integrity constraints* and *business rules* that should
  hold after the transformation and not having to define all the individual steps
  for implementing the transformation, as it is the common practice today. This will
  yield tremendous benefits compared to common ETL challenges faced today and change
  the way we build data transformation flows. Just to name a few:\n - Automated ETL
  coding driven by Source-to-Target mapping and business rules\n - ETL code correctness
  out-of-the-box\n - Data Integrity / Business Rules controls automatically embedded
  in your ETL code\n - Self-documented ETL code (Your documentation i.e., the Source-to-Target
  mapping and the business rules, is also the only code you need to write!)\n - Drastically
  minimize time-to-market for delivering Data Marts and Data Warehouses, or simply
  implementing Data Analysis tasks.\n\nThe above is inspired by the theoretical work
  on Categorical Databases by David Spivak,\n### Available Modules\n*DBFunctor* consists
  of the following set of Haskell modules:\n* **RTable.Core**: Implements the relational
  Table concept. Defines all necessary data types like `RTable` and `RTuple` as well
  as basic relational algebra operations on RTables.\n* **Etl.Julius**: A simple Embedded
  DSL for ETL/ELT data processing in Haskell\n* **RTable.Data.CSV**:  Implements `RTable`
  over CSV (TSV, or any other delimiter) files logic. It is based on the [Cassava](https://github.com/haskell-hvr/cassava)
  library.\n### A Very Simple Example\n In this example, we will load a CSV file,
  turn it into an RTable and then issue a very simple query on it and print the result,
  just to show the whole concept.\nSo  lets say we have a CSV file called test-data.csv.
  The file stores table metadata from an Oracle database. Each row represents a table
  stored in the database. Here is a small extract from the csv file:\n\n    $ head
  test-data.csv\n        OWNER,TABLE_NAME,TABLESPACE_NAME,STATUS,NUM_ROWS,BLOCKS,LAST_ANALYZED\n
  \       APEX_030200,SYS_IOT_OVER_71833,SYSAUX,VALID,0,0,06/08/2012 16:22:36\n        APEX_030200,WWV_COLUMN_EXCEPTIONS,SYSAUX,VALID,3,3,06/08/2012
  16:22:33\n        APEX_030200,WWV_FLOWS,SYSAUX,VALID,10,3,06/08/2012 22:01:21\n
  \       APEX_030200,WWV_FLOWS_RESERVED,SYSAUX,VALID,0,0,06/08/2012 16:22:33\n        APEX_030200,WWV_FLOW_ACTIVITY_LOG1$,SYSAUX,VALID,1,29,07/20/2012
  19:07:57\n        APEX_030200,WWV_FLOW_ACTIVITY_LOG2$,SYSAUX,VALID,14,29,07/20/2012
  19:07:57\n        APEX_030200,WWV_FLOW_ACTIVITY_LOG_NUMBER$,SYSAUX,VALID,1,3,07/20/2012
  19:08:00\n        APEX_030200,WWV_FLOW_ALTERNATE_CONFIG,SYSAUX,VALID,0,0,06/08/2012
  16:22:33\n        APEX_030200,WWV_FLOW_ALT_CONFIG_DETAIL,SYSAUX,VALID,0,0,06/08/2012
  16:22:33\n**1. Turn the CSV file into an RTable**\nThe first thing we want to do
  is to read the file and turn it into an RTable. In order to do this we need to define
  the RTable Metadata, which is the same information one can provide in an SQL CREATE
  TABLE statement, i,e, column names, column data types and integrity constraints
  (Primary Key, Unique Key only - no Foreign Keys). So lets see how this is done:
  \    \n```haskell\n    -- Define table metadata\n    src_DBTab_MData :: RTableMData\n
  \   src_DBTab_MData =\n\t    createRTableMData   (   \"sourceTab\"  -- table name\n
  \                                ,[  (\"OWNER\", Varchar)                                      --
  Owner of the table\n                                    ,(\"TABLE_NAME\", Varchar)
  \                               -- Name of the table\n                                    ,(\"TABLESPACE_NAME\",
  Varchar)                           -- Tablespace name\n                                    ,(\"STATUS\",Varchar)
  \                                    -- Status of the table object (VALID/IVALID)\n
  \                                   ,(\"NUM_ROWS\", Integer)                                  --
  Number of rows in the table\n                                    ,(\"BLOCKS\", Integer)
  \                                   -- Number of Blocks allocated for this table\n
  \                                   ,(\"LAST_ANALYZED\", Timestamp \"MM/DD/YYYY
  HH24:MI:SS\")   -- Timestamp of the last time the table was analyzed (i.e., gathered
  statistics)\n                                  ]\n                             )\n
  \                               [\"OWNER\", \"TABLE_NAME\"] -- primary key\n                                []
  -- (alternative) unique keys\n    main :: IO ()\n    main = do\n       -- read source
  csv file\n       srcCSV <- readCSV \"./app/test-data.csv\"    \n       let\n         --
  turn source csv to an RTable\n         src_DBTab = toRTable src_DBTab_MData srcCSV\n
  \   ...\n```\nWe have used the following functions:\n```haskell\n-- | createRTableMData
  : creates RTableMData from input given in the form of a list\n--   We assume that
  the column order of the input list defines the fixed column order of the RTuple.\ncreateRTableMData
  ::\n        (RTableName, [(ColumnName, ColumnDType)])\n        -> [ColumnName]     --
  ^ Primary Key. [] if no PK exists\n        -> [[ColumnName]]   -- ^ list of unique
  keys. [] if no unique keys exists\n        -> RTableMData\n```\nin order to define
  the RTable metadata.\nFor reading the CSV file we have used:\n```haskell\n-- | readCSV:
  reads a CSV file and returns a CSV data type (Treating CSV data as opaque byte strings)\nreadCSV
  ::\n    FilePath  -- ^ the CSV file\n    -> IO CSV  -- ^ the output CSV type\n```\nFinally,
  in order to turn the CSV data type into an RTable, we have used function:\n```haskell\ntoRTable
  :: RTableMData -> CSV -> RTable\n```\nwhich comes from the `RTabular` type class
  instance of the `CSV` data type.\n **2. Query the RTable**\nOnce we have created
  an RTable, we can issue queries on it, or apply any type of data transformations.
  Note that due to immutability, each query or data transformation creates a new RTable.\n
  We will now issue the following query:\nWe return all the rows, which correspond
  to some filter predicate - in particular all rows where the `TABLE_NAME` includes
  some search string and the `LAST_ANALYZED` field is greater than an input date.\n\nFor
  this we use the Julius EDSL, in order to express the query and then with the function
  \n`runJulius`, we evaluate the expression into an RTable.\n```haskell\nrunJulius
  :: ETLMappingExpr -> IO RTable\n```\nHere is the Julius expression that yield the
  desired results.\n```haskell\njulExpr srch dtstr rtab = \n            EtlMapStart
  \n                :-> (EtlR $\n                        ROpStart\n                        :.
  \ (Filter (From $ Tab rtab) $\n                                FilterBy (\\t ->
  case instrRText (RText srch) (t <!> \"TABLE_NAME\") of\n                                                    Just
  p  -> True\n                                                    Nothing -> False\n
  \                                               &&\n                                                (t
  <!> \"LAST_ANALYZED\") >= (RTime $ toRTimestamp \"DD/MM/YYYY\" dtstr)\n                                         )\n
  \                           )\n                    )\n```\nA Julius expression is
  a *data processing chain* consisting of various Relational Algebra operations `(EtlR
  $ ...)`  and/or column mappings `(EtlC $ ...)` connected together via the `:->`
  data constructor,  of the form (Julius expressions are read *from top-to-bottom
  \ or from left-to-right*):\n```haskell\nmyJulExpression =\n\tEtlMapStart\n\t:->
  (EtlC $ ...)  -- this is a Column Mapping\n\t:-> (EtlR $   -- this is a series of
  Relational Algebra Operations\n\t     ROpStart\n\t  :. (Rel Operation 1) -- a relational
  algebra operation\n\t  :. (Rel Operation 2))\n\t:-> (EtlC $ ...)  -- This is another
  Column Mapping\n\t:-> (EtlR $ -- more relational algebra operations\n\t     ROpStart\n\t
  \ :. (Rel Operation 3)\n\t  :. (Rel Operation 4)\n\t  :. (Rel Operation 5))\n\t:->
  (EtlC $ ...) -- This is Column Mapping 3\n\t:-> (EtlC $ ...) -- This is Column Mapping
  4\n\t...\n```\nIn our example, the Julius expression consists only of a single relational
  algebra operation, namely a `Filter` operation, which uses an RTuple  predicate
  of the form \t`RTuple -> Bool` to filter out RTuples (i.e., rows) that dont satisfy
  this predicate. The predicate is expressed as the lambda expression:\n```haskell\nFilterBy
  (\\t -> case instrRText (RText srch) (t <!> \"TABLE_NAME\") of\n                                                    Just
  p  -> True\n                                                    Nothing -> False\n
  \                                               &&\n                                                (t
  <!> \"LAST_ANALYZED\") >= (RTime $ toRTimestamp \"DD/MM/YYYY\" dtstr)\n```\nWe use
  the [instrRText](https://hackage.haskell.org/package/DBFunctor-0.1.0.0/docs/RTable-Core.html#g:16)
  function to find these table_name values that include the `srch` string. Also, we
  use the [toRTimestamp](https://hackage.haskell.org/package/DBFunctor-0.1.0.0/docs/RTable-Core.html#g:15)
  function, in order to turn the date string `dtstr` into an `RTimestamp` data type
  and compare it against the `LAST_ANALYZED` column,\n\nFinally, in order to print
  the result of the query on the screen, we use the \n```haskell\nprintfRTable ::
  RTupleFormat -> RTable -> IO()\n``` \nfunction, which brings printf-like functionality
  into the printing of RTables\nAnd here is the output:\n```\n$ stack exec -- exampleDBFunctor\n\nPrint
  all tables that incude a \"search string\" in their name and have been analyzed
  after a specific date\n\nGive me the search string:\nFLOW\nGive me the date in \"DD/MM/YYYY\"
  format:\n01/04/2018\n---------------------------------------------------------------------------------------------------------------------------------\nOWNER
  \          TABLE_NAME                        TABLESPACE_NAME     STATUS     NUM_ROWS
  \    BLOCKS     LAST_ANALYZED\n~~~~~           ~~~~~~~~~~                        ~~~~~~~~~~~~~~~
  \    ~~~~~~     ~~~~~~~~     ~~~~~~     ~~~~~~~~~~~~~\nAPEX_040100     WWV_FLOW_ACTIVITY_LOG1$
  \          SYSAUX              VALID      4052         155        04/04/2018 18:19:56\nAPEX_040100
  \    WWV_FLOW_ACTIVITY_LOG2$           SYSAUX              VALID      1771         92
  \        16/04/2018 17:33:16\nAPEX_040100     WWV_FLOW_ACTIVITY_LOG_NUMBER$     SYSAUX
  \             VALID      1            3          10/04/2018 16:09:25\nAPEX_040100
  \    WWV_FLOW_COMPANIES                SYSAUX              VALID      10           3
  \         16/04/2018 17:33:13\nAPEX_040100     WWV_FLOW_DATA                     SYSAUX
  \             VALID      109          155        16/04/2018 16:06:38\nAPEX_040100
  \    WWV_FLOW_DEBUG_MESSAGES2          SYSAUX              VALID      0            0
  \         10/04/2018 16:09:25\nAPEX_040100     WWV_FLOW_FND_USER                 SYSAUX
  \             VALID      50           3          05/04/2018 18:09:23\nAPEX_040100
  \    WWV_FLOW_PAGE_CACHE               SYSAUX              VALID      22           3
  \         05/04/2018 18:35:18\nAPEX_040100     WWV_FLOW_SESSIONS$                SYSAUX
  \             VALID      182          26         16/04/2018 16:07:13\nAPEX_040100
  \    WWV_FLOW_USER_ACCESS_LOG1$        SYSAUX              VALID      127          5
  \         11/04/2018 18:27:17\nAPEX_040100     WWV_FLOW_USER_ACCESS_LOG2$        SYSAUX
  \             VALID      39           5          16/04/2018 17:30:59\nAPEX_040100
  \    WWV_FLOW_USER_ACCESS_LOG_NUM$     SYSAUX              VALID      1            3
  \         12/04/2018 16:30:05\nAPEX_040100     WWV_FLOW_WORKSHEET_CONDITIONS     SYSAUX
  \             VALID      501          18         16/04/2018 17:33:03\nAPEX_040100
  \    WWV_FLOW_WORKSHEET_GROUP_BY       SYSAUX              VALID      22           3
  \         03/04/2018 17:44:07\nAPEX_040100     WWV_FLOW_WORKSHEET_RPTS           SYSAUX
  \             VALID      505          16         16/04/2018 17:33:01\nFLOWS_FILES
  \    WWV_FLOW_FILE_OBJECTS$            SYSAUX              VALID      302          16
  \        13/04/2018 18:00:05\n\n\n16 rows returned\n---------------------------------------------------------------------------------------------------------------------------------\n```\n**Here
  is the complete example**.\n```haskell\n{-# LANGUAGE OverloadedStrings #-}\n\nmodule
  Main where\n\nimport  Etl.Julius  \nimport  RTable.Data.CSV     (CSV, readCSV, toRTable,
  writeCSV)\nimport  Data.Text.IO as T   (getLine)\n\n-- This is the input source
  table metadata\n-- It includes the tables stored in an imaginary database\nsrc_DBTab_MData
  :: RTableMData\nsrc_DBTab_MData = \n    createRTableMData   (   \"sourceTab\"  --
  table name\n                            ,[  (\"OWNER\", Varchar)                                      --
  Owner of the table\n                                ,(\"TABLE_NAME\", Varchar)                                --
  Name of the table\n                                ,(\"TABLESPACE_NAME\", Varchar)
  \                          -- Tablespace name\n                                ,(\"STATUS\",Varchar)
  \                                    -- Status of the table object (VALID/IVALID)\n
  \                               ,(\"NUM_ROWS\", Integer)                                  --
  Number of rows in the table\n                                ,(\"BLOCKS\", Integer)
  \                                   -- Number of Blocks allocated for this table\n
  \                               ,(\"LAST_ANALYZED\", Timestamp \"MM/DD/YYYY HH24:MI:SS\")
  \  -- Timestamp of the last time the table was analyzed (i.e., gathered statistics)
  \n                            ]\n                        )\n                        [\"OWNER\",
  \"TABLE_NAME\"] -- primary key\n                        [] -- (alternative) unique
  keys\n\n-- Result RTable metadata\nresult_tab_MData :: RTableMData\nresult_tab_MData
  = \n    createRTableMData   (   \"resultTab\"  -- table name\n                            ,[
  \ (\"OWNER\", Varchar)                                      -- Owner of the table\n
  \                               ,(\"TABLE_NAME\", Varchar)                                --
  Name of the table\n                                ,(\"LAST_ANALYZED\", Timestamp
  \"MM/DD/YYYY HH24:MI:SS\")   -- Timestamp of the last time the table was analyzed
  (i.e., gathered statistics) \n                            ]\n                        )\n
  \                       [\"OWNER\", \"TABLE_NAME\"] -- primary key\n                        []
  -- (alternative) unique keys\n\nmain :: IO ()\nmain = do\n     -- read source csv
  file\n    srcCSV <- readCSV \"./app/test-data.csv\"\n\n    putStrLn \"\\nPrint all
  tables that incude a \\\"search string\\\" in their name and have been analyzed
  after a specific date\\n\"\n    putStrLn \"Give me the search string: \"\n    search
  <- T.getLine\n\n    putStrLn \"Give me the date in \\\"DD/MM/YYYY\\\" format: \"\n
  \   datestr <- Prelude.getLine\n        \n\n    -- print source RTable first n rows\n
  \   -- RTable A\n    resultRTab <- runJulius $ julExpr search datestr $ toRTable
  src_DBTab_MData srcCSV \n    printfRTable (  \n                    -- this is the
  equivalent when printing on the screen a list of columns, defined in a SELECT clause
  in SQL\n                    genRTupleFormat [\"OWNER\", \"TABLE_NAME\", \"TABLESPACE_NAME\",
  \"STATUS\", \"NUM_ROWS\", \"BLOCKS\", \"LAST_ANALYZED\"] genDefaultColFormatMap\n
  \                ) $ resultRTab  \n\n    -- save result to a CSV file\n    writeCSV
  \"./app/result-data.csv\" $ \n                    fromRTable result_tab_MData resultRTab\n
  \   where\n        julExpr srch dtstr rtab = \n            EtlMapStart \n                :->
  (EtlR $\n                        ROpStart\n                        :.  (Filter (From
  $ Tab rtab) $\n                                FilterBy (\\t -> case instrRText
  (RText srch) (t <!> \"TABLE_NAME\") of\n                                                    Just
  p  -> True\n                                                    Nothing -> False\n
  \                                               &&\n                                                (t
  <!> \"LAST_ANALYZED\") >= (RTime $ toRTimestamp \"DD/MM/YYYY\" dtstr)\n                                         )\n
  \                           )\n                    )\n```\n### Julius DSL Tutorial
  \nWe have written [a Julius tutorial](https://github.com/nkarag/haskell-DBFunctor/blob/master/doc/JULIUS-TUTORIAL.md)
  to help you get started with Julius DSL. \n<a  name=\"howtorun\"></a> \n### How
  to run\n#### 1. Install Stack\nSee [this](https://docs.haskellstack.org/en/stable/GUIDE/)
  guide for help. If you have stack already installed, then we suggest you run a `stack
  upgrade`, in order to update it to the latest version and avoid any error messages
  due to bugs.\nThen run a `stack update`, in order to update the package index.\n####
  2. Initiate a project\n`$ stack new myDBFunctorProject`\n#### 3. Start coding with
  DBFunctor\nDon't forget:\n\n - to `import Etl.Julius` module\n - to use GHC extension
  `{-# LANGUAGE OverloadedStrings #-}`, since DBFunctor uses `Text` in all of its
  basic data types, this extension is necessary if you want to assign string literal
  values to an `RDataType` (the type of a column in an `RTable`)\n#### 4. Declare
  dependency with DBFunctor package\nEdit your package.yaml (or project.cabal) file
  and add the dependency to the DBFunctor package\n```\ndependencies:\n- DBFunctor\n```\nAlso,
  don't forget to add to your stack.yaml file the line:\n```\n   extra-deps:\n    -
  DBFunctor-0.1.1.0\n```\n#### 5. Build and run you project\n```\n$ stack build\n```\nRun\n```
  \nstack exec -- myDBFunctorProject-exe\n```\n"
license-name: BSD-3-Clause
