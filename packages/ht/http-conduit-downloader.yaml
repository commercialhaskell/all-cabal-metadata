all-versions:
- 1.0.0
- 1.0.1
- 1.0.2
- 1.0.3
- 1.0.4
- 1.0.5
- 1.0.6
- 1.0.7
- 1.0.8
- 1.0.9
- 1.0.10
- 1.0.11
- 1.0.12
- 1.0.13
- 1.0.14
- 1.0.15
- 1.0.16
- 1.0.17
- 1.0.18
- 1.0.19
- 1.0.20
- 1.0.21
- 1.0.22
- 1.0.23
- 1.0.24
- 1.0.25
- 1.0.30
- 1.0.31
- 1.0.32
- 1.0.33
- 1.1.0
- 1.1.1
- 1.1.2
- 1.1.3
- 1.1.4
author: Vladimir Shabanov <dev@vshabanov.com>
basic-deps:
  HsOpenSSL: '>=0.11.5'
  base: '>=4 && <5'
  bytestring: '>=0'
  data-default: '>=0'
  http-client: '>=0.7.3 && <0.7.4'
  http-client-openssl: '>=0.3.3 && <0.3.4'
  http-types: '>=0'
  network: '>=2.6'
  network-uri: '>=2.6'
  text: '>=0'
  time: '>=1.5.0'
  zlib: '>=0'
changelog: ''
changelog-type: ''
description: |-
  HTTP/HTTPS downloader built on top of @http-client@
  and used in <https://bazqux.com> crawler.

  Previously it was based on @http-conduit@ (hence the name) but since
  all the necessary parts are in @http-client@ now @http-conduit@ is no
  longer used.

  * Handles all possible http-client exceptions and returns
  human readable error messages.

  * Handles some web server bugs (returning 'deflate' data instead of 'gzip',
  invalid 'gzip' encoding).

  * Uses OpenSSL instead of 'tls' package (since 'tls' doesn't handle all sites and works slower than OpenSSL).

  * Ignores invalid SSL sertificates.

  * Receives data in 32k chunks internally to reduce memory fragmentation
  on many parallel downloads.

  * Download timeout.

  * Total download size limit.

  * Returns HTTP headers for subsequent redownloads and handles
  'Not modified' results.

  * Can be used with external DNS resolver (e.g. @concurrent-dns-cache@).
description-type: haddock
hash: 742352d258f5c4ac8e1254cf02ad0757baa7bb8ca7f365be88d92c1611c9b9a4
homepage: https://github.com/bazqux/http-conduit-downloader
latest: 1.1.4
license-name: BSD-3-Clause
maintainer: Vladimir Shabanov <dev@vshabanov.com>
synopsis: HTTP downloader tailored for web-crawler needs.
test-bench-deps: {}
