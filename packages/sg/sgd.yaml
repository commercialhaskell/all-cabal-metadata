all-versions:
- 0.1.0
- 0.2.0
- 0.2.1
- 0.2.2
- 0.2.3
- '0.3'
- 0.3.1
- 0.3.2
- 0.3.3
- 0.3.4
- 0.3.5
- 0.3.6
- 0.3.7
- 0.4.0
- 0.4.0.1
- 0.5.0.0
- 0.6.0.0
- 0.7.0.0
- 0.7.0.1
- 0.8.0.0
- 0.8.0.1
- 0.8.0.2
- 0.8.0.3
author: Jakub Waszczuk
basic-deps:
  base: '>=4.7 && <5'
  binary: '>=0.5 && <0.9'
  bytestring: '>=0.9 && <0.11'
  containers: '>=0.4 && <0.7'
  data-default: '>=0.7 && <0.8'
  deepseq: '>=1.3 && <1.5'
  filepath: '>=1.3 && <1.5'
  hmatrix: '>=0.19 && <0.21'
  logfloat: '>=0.12 && <0.14'
  monad-par: '>=0.3.4 && <0.4'
  mtl: '>=2.0 && <2.3'
  parallel: '>=3.2 && <3.3'
  pipes: '>=4.3 && <4.4'
  primitive: '>=0.5 && <0.7'
  random: '>=1.0 && <1.2'
  random-shuffle: '>=0.0.4 && <0.1'
  temporary: '>=1.1 && <1.4'
  vector: '>=0.10 && <0.13'
changelog: "-*-change-log-*-\n\n0.8.0   Apr 2019\n\t* Allow step size decay in Adam\n\t*
  Parallel calculation of the objective function\n"
changelog-type: text
description: |
  # Haskell stochastic gradient descent library

  Stochastic gradient descent (SGD) is a method for optimizing a global objective
  function defined as a sum of smaller, differentiable functions.  In each
  iteration of SGD the gradient is calculated based on a subset of the training
  dataset.  In Haskell, this process can be simply represented as a [fold over a
  of subsequent dataset
  subsets](https://blog.jle.im/entry/purely-functional-typed-models-1.html)
  (singleton elements in the extreme).

  However, it can be beneficial to select the subsequent subsets randomly (e.g.,
  shuffle the entire dataset before each pass).  Moreover, the dataset can be
  large enough to make it impractical to store it all in memory.  Hence, the
  `sgd` library adopts a [pipe](http://hackage.haskell.org/package/pipes)-based
  interface in which SGD takes the form of a process consuming dataset subsets
  (the so-called mini-batches) and producing a stream of output parameter values.

  The `sgd` library implements several SGD variants (SGD with momentum, AdaDelta,
  Adam) and handles heterogeneous parameter representations (vectors, maps, custom
  records, etc.).  It can be used in combination with automatic differentiation
  libraries ([ad](http://hackage.haskell.org/package/ad),
  [backprop](http://hackage.haskell.org/package/backprop)), which can be used to
  automatically calculate the gradient of the objective function.

  Look at [the hackage repository](http://hackage.haskell.org/package/sgd) for a
  library documentation.
description-type: markdown
hash: 1afe2ee01c09029c023d79cbc729e839adf9ab3193dfa1021f3134140f90b121
homepage: https://github.com/kawu/sgd#readme
latest: 0.8.0.3
license-name: BSD-3-Clause
maintainer: waszczuk.kuba@gmail.com
synopsis: Stochastic gradient descent library
test-bench-deps:
  ad: '>=4.3 && <4.4'
  base: '>=4.7 && <5'
  binary: '>=0.5 && <0.9'
  bytestring: '>=0.9 && <0.11'
  containers: '>=0.4 && <0.7'
  data-default: '>=0.7 && <0.8'
  deepseq: '>=1.3 && <1.5'
  filepath: '>=1.3 && <1.5'
  hmatrix: '>=0.19 && <0.21'
  logfloat: '>=0.12 && <0.14'
  monad-par: '>=0.3.4 && <0.4'
  mtl: '>=2.0 && <2.3'
  parallel: '>=3.2 && <3.3'
  pipes: '>=4.3 && <4.4'
  primitive: '>=0.5 && <0.7'
  random: '>=1.0 && <1.2'
  random-shuffle: '>=0.0.4 && <0.1'
  sgd: '>=0'
  tasty: '>=1.2 && <1.3'
  tasty-hunit: '>=0.10 && <0.11'
  temporary: '>=1.1 && <1.4'
  vector: '>=0.10 && <0.13'
