all-versions:
- 0.1.0
- 0.1.1
- 0.1.2
- 0.1.2.1
- 0.1.2.2
- 0.1.2.3
- 0.1.2.4
author: Sergey Mironov
basic-deps:
  MonadRandom: '>=0'
  base: '>=4.8 && <5'
  containers: '>=0'
  directory: '>=0'
  filepath: '>=0'
  hashable: '>=0'
  lens: '>=0'
  mersenne-random-pure64: '>=0'
  monad-loops: '>=0'
  mtl: '>=0'
  parsec: '>=0'
  pretty-show: '>=0'
  process: '>=0'
  random: '>=0'
  rl-satton: '>=0'
  stm: '>=0'
  template-haskell: '>=0'
  text: '>=0'
  time: '>=0'
  transformers: '>=0'
  unordered-containers: '>=0'
changelog: ''
changelog-type: ''
description: |-
  rl-satton provides implementation of algorithms, described in the
  'Reinforcement Learing: An Introduction' book by Richard S. Satton and Andrew
  G. Barto. In particular, TD(0), TD(lambda), Q-learing are implemented.
  Code readability was placed above performance.
  Usage examples are provided in the ./examples folder.
description-type: haddock
hash: 73a658b99d04036f314697615bece5fd911d51b10f6bd3a33f5f264c7c56c82a
homepage: https://github.com/grwlf/rl
latest: 0.1.2.4
license-name: BSD-3-Clause
maintainer: grrwlf@gmail.com
synopsis: Collection of Reinforcement Learning algorithms
test-bench-deps: {}
