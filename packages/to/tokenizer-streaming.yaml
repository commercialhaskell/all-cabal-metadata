all-versions:
- 0.1.0.0
- 0.1.0.1
author: Enum Cohrs
basic-deps:
  base: '>=4.9 && <5.0'
  bytestring: '>=0'
  mtl: '>=0'
  streaming: '>=0'
  streaming-bytestring: '>=0.1.6'
  streaming-commons: ^>=0.2.1.0
  text: '>=0'
  tokenizer-monad: ^>=0.2.2.0
changelog: |
  # Revision history for tokenizer-streaming

  ## 0.1.0.0 -- YYYY-mm-dd

  * First version. Released on an unsuspecting world.
changelog-type: markdown
description: |-
  # tokenizer-streaming

  **Motivation**: You might have stumpled upon the package tokenizer-monad. It
  is another project by me, for writing tokenizers that act on pure
  text/strings. However, there are situations when you cannot keep all the text
  in memory. You might want to tokenize text from network streams or from large
  corpus files.

  **Main idea**: A monad transformer called `TokenizerT` implements exactly the
  same methods as `Tokenizer` from tokenizer-monad, such that all tokenizers can
  be ported without code changes (if you used `MonadTokenizer` in the type signatures)

  ## Supported text types

   - streams of Char lists can be tokenized into streams of Char lists
   - streams of strict Text can be tokenized into streams of strict Text
   - streams of lazy Text can be tokenized into streams of lazy Text
   - streams of strict ASCII ByteStrings can be tokenized into streams of strict ASCII ByteStrings
   - streams of lazy ASCII ByteStrings can be tokenized into streams of lazy ASCII ByteStrings
   - bytestring streams (from streaming-bytestring) with Unicode encodings (UTF-8, UTF-16 LE & BE, UTF-32 LE & BE) can be tokenized into streams of strict Text
description-type: markdown
hash: 6a4354d90d1fe708a0ed6dc4fea28396916ad7faeba58f4d1c1072b116caccaf
homepage: ''
latest: 0.1.0.1
license-name: GPL-3.0-only
maintainer: darcs@enumeration.eu
synopsis: A variant of tokenizer-monad that supports streaming.
test-bench-deps: {}
