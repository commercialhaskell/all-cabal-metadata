all-versions:
- 0.1.0.0
author: Lev135
basic-deps:
  base: '>=4.14.3.0 && <4.15'
  containers: '>=0'
  transformers: '>=0'
changelog: "# Revision history for tokenizer\r\n\r\n## 0.1.0.0 -- 2022-09-20\r\n\r\n*
  First raw version is released.\r\n"
changelog-type: markdown
description: "Tokenizer\r\n===\r\n***WARNING this package is not tested enough for
  the moment.\r\nBugs are very likely here.***\r\n\r\nThis package provides solution
  for two problems:\r\n- split input string on tokens of specificated sort;\r\n- check
  tokenizing of *all possible* strings is unique.\r\n\r\nSome examples\r\n---\r\n*If
  you have problems with understanding the syntax we use read two\r\nsections bellow.*\r\n\r\n-
  *parse* make `Token` from a string\r\n- *checkUniqueTokenizing* check if every string
  can no more than one way to be\r\n  split into parts in such a way, that each of
  them would be matched\r\n  by one of the given tokens\r\n- *tokenize* tries to split
  given string on parts with the same\r\n  condition.\r\n\r\nHere everything is ok\r\n```hs\r\n>
  checkUniqueTokenizing $ parse <$> [\"ab\", \"bc\", \"abc\"]\r\nRight ()\r\n```\r\nand
  we can split string on these token with deterministic result:\r\n```hs\r\n> tokenize
  (makeTokenizeMap $ parse <$> [\"ab\", \"bc\", \"abc\"]) \"abbcabc\"\r\nRight [(\"ab\",\"ab\"),(\"bc\",\"bc\"),(\"abc\",\"abc\")]\r\n>
  tokenize (makeTokenizeMap $ parse <$> [\"ab\", \"bc\", \"abc\"]) \"abbcabca\"\r\nLeft
  (NoWayTokenize 7 [(\"ab\",\"ab\"),(\"bc\",\"bc\"),(\"abc\",\"abc\")])\r\n```\r\n\r\nWe
  can parse `\"ab\"` as `\"a\"` and `\"b\"` or `\"ab\"`\r\n```hs\r\n> checkUniqueTokenizing
  $ parse <$> [\"ab\", \"a\", \"b\"]\r\nLeft Conflicts: [(\"a\",a),(\"b\",b)] [(\"ab\",ab)]\r\n```\r\nwe
  *can* tokenize using this set of tokens, but sometimes it gives us\r\n`TwoWaysTokenize`
  error:\r\n```hs\r\n> tokenize (makeTokenizeMap $ parse <$> [\"a\", \"b\", \"ab\"])
  \"bba\"\r\nRight [(\"b\",\"b\"),(\"b\",\"b\"),(\"a\",\"a\")]\r\n> tokenize (makeTokenizeMap
  $ parse <$> [\"a\", \"b\", \"ab\"]) \"aab\"\r\nLeft (TwoWaysTokenize 1 [(\"a\",\"a\"),(\"ab\",\"ab\")]
  [(\"a\",\"a\"),(\"a\",\"a\"),(\"b\",\"b\")])\r\n```\r\nto solve the problem we can
  specify that that there should be no `b`\r\ncharacter after separate `a` token\r\n```hs\r\n>
  checkUniqueTokenizing $ parse <$> [\"ab\", \"a?!b\", \"b\"]\r\nRight ()\r\n> tokenize
  (makeTokenizeMap $ parse <$> [\"a?!b\", \"b\", \"ab\"]) \"aab\"\r\nRight [(\"a?!b\",\"a\"),(\"ab\",\"ab\")]\r\n```\r\n\r\nMore
  complex example. Problem is for string `\"ababab\"`:\r\n```hs\r\n> checkUniqueTokenizing
  $ parse <$> [\"ab\", \"aba\", \"bab\"]\r\nLeft Conflicts: [(\"ab\",ab),(\"ab\",ab),(\"ab\",ab)]
  [(\"aba\",aba),(\"bab\",bab)]\r\n```\r\n\r\nHere even `\"aab\"` can be split as
  `aa` and then `b` or `a*b`. However, current\r\nalgorithm gives another conflict
  `\"aaa*b\"` can be spit as `aa` and `a*b` or\r\nsimply `a*b`:\r\n``` hs\r\n> checkUniqueTokenizing
  $ parse <$> [\"a*b\", \"aa\", \"b\"]\r\nLeft Conflicts: [(\"aa\",aa),(\"a*b\",ab)]
  [(\"a*b\",aaab)]\r\n```\r\n\r\nTry it yourself by executing `cabal repl examples
  -f examples`\r\n\r\nWhat is a token?\r\n---\r\nA token is a parts' of string template.
  It consists of three parts. Each part\r\nprovides some restrictions on characters
  of a string that can be matched.\r\nThe main part of token is it's `body`. It describes
  characters of a string part,\r\nmatchable by token. Two others parts `behind` and
  `ahead` restrict symbols, that\r\ncan be situated before/after matched part respectively.
  Note that they are\r\nassumed to be satisfied if begin/end of line is achieved.\r\n\r\nEach
  part of token is a list, describing subsequent symbols from left to right.\r\nIn
  `behind` and `ahead` part we can specify for each position what symbols can\r\nbe
  used or cannot be used via `BlackWhiteSet`. In token's body we can restrict\r\nnot
  only one position, but some of subsequent positions. More precisely, we can\r\nmark
  a `BlackWhiteSet` to be `Repeatable` one or some (it's one or more) times.\r\n\r\nSyntax,
  used in examples\r\n---\r\nTo make examples more readable we provide simple language
  for describing tokens.\r\nWe'll use alpha characters as symbols and some punctuation
  for describing\r\ntoken's structure:\r\n- `{` and `}` for grouping set of characters,
  containing more then one char;\r\n- `!` means \"all, except those\" (`BlackSet`
  in this package' terminology);\r\n- `*` behind the charset means \"some characters,
  containing in this set\";\r\n- `?` at the beginning of `behind`/`ahead` parts;\r\n-
  `<` and `>` for grouping complex `behind`/`ahead` parts.\r\n\r\nThe grammar in EBNF:\r\n```ebnf\r\nBlackWhiteSet
  \    := ['!'], (letter | '{', {letter}, '}');\r\nRepeatable        := charset, ['*'];\r\n\r\nahead_or_behind
  \  := '?', (symbol | '<', {symbol}, '>');\r\nbody              := symbol, {symbol}\r\ntoken
  \            := [ahead_or_behind], body, [ahead_or_behind];\r\n```\r\nParser for
  tokens, described in this manner is available in `examples/Main.hs`\r\n\r\nTechnical
  details\r\n---\r\nUniqueness checking is provided by a modification of Sardinas-Patterson's\r\nalgorithm.
  Tokenizing process is written in the most simple way with\r\nnon-exponential asymptotic
  of the input string's length.\r\n\r\nUsage\r\n---\r\nIt's very likely, that all
  you need is exported from `Text.Tokenizer`.\r\n\r\nBug reports and feature requests\r\n---\r\nFeel
  free open issues at\r\n[the GitHub repo](https://github.com/Lev135/tokenizer/issues)\r\n\r\nContribution\r\n---\r\nI
  would be vary glad for any contribution. The are many ways to improve this lib:\r\n-
  improve documentation and examples;\r\n- add more tests to check, that everything
  works nice;\r\n- improve performance (I think there are many opportunities here
  in both\r\n  algorithms);\r\n- add benchmarks (connected with the previous).\r\n-
  *(this issue is mostly for me :)*\r\n  improve code readability (I've tried not
  to make it absolutely terrible, but\r\n  it's definitely not perfect);\r\n\r\nI
  know, that some of those problems (especially code readability) should be\r\nclosed
  by myself, but unfortunately I have no time to deal with them now.\r\n\r\nMaybe,
  I'm the package is too raw to publish it, but there are some reasons for\r\nme to
  do so:\r\n- I don't no when it will be improved enough;\r\n- it is needed for my
  main project ([FineTeX](https://github.com/lev135/FineTeX)).\r\n"
description-type: markdown
hash: fe6d6a3448caf7aa9887b4d02669f4c99c3025e88331baa57c43837ee3d78581
homepage: https://github.com/Lev135/tokenizer
latest: 0.1.0.0
license-name: MIT
maintainer: lev_135@mail.ru
synopsis: Check uniqueness and tokenize safely
test-bench-deps:
  base: '>=4.14.3.0 && <4.15'
  containers: '>=0'
  hspec: '>=0'
  megaparsec: '>=0'
  tokenizer: '>=0'
  transformers: '>=0'
