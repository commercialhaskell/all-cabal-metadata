all-versions:
- 0.5.0.0
author: Marco Zocca
basic-deps:
  aeson: '>=2.0 && <3'
  base: '>=4.14 && <5'
  benchpress: '>=0.2 && <0.3'
  boxes: '>=0.1 && <0.2'
  bytestring: '>=0.10 && <0.13'
  deepseq: '>=1.4 && <2'
  directory: '>=1.3 && <2'
  filepath: '>=1.4 && <2'
  hspec-core: '>=2.10 && <3'
  microlens: '>=0.4 && <0.6'
  process: '>=1.6 && <2'
  statistics: '>=0.16 && <0.17'
  text: '>=1.2 && <3'
  time: '>=1.9 && <2'
  vector: '>=0.12 && <0.14'
changelog: "# Changelog\n\n## [0.5.0]\n\n### Added\n\n- Clarified copyright notice
  for vendored code from `tasty-bench`, (c) Andrew Lelechenko 2021 -\n\n### Removed\n\n-
  **BREAKING:** Removed `whnf` (weak head normal form) evaluation combinators, the
  library is geared towards non-infinite input data\n  - Removed `whnf` - use `nf`
  instead for pure functions\n  - Removed `whnfIO` - use `nfIO` instead for IO actions\n
  \ - Removed `whnfAppIO` - use `nfAppIO` instead for functions returning IO\n  -
  Migration: Replace all `whnf*` calls with their `nf*` equivalents\n\n## [0.4.0]\n\n###
  Added\n\n- **QuickCheck property tests** for statistical computations in Runner
  module\n  - 28 property tests covering mathematical invariants and edge cases\n
  \ - Tests for `calculateTrimmedMean`, `calculateMAD`, `calculateIQR`, `detectOutliers`\n
  \ - Tests for `compareStats` tolerance logic (hybrid percentage/absolute)\n  - Tests
  for `checkVariance` warning generation\n  - Custom generators for valid timing data
  and benchmark configurations\n  - New test suite: `golds-gym-properties`\n\n- **Evaluation
  strategy combinators** for proper laziness handling\n  - `nf` - Force result to
  normal form (deep evaluation)\n  - `nfIO` - Normal form for IO actions\n  - `nfAppIO`
  - Normal form for functions returning IO\n  - `io` - Plain IO action (for backward
  compatibility)\n  - Vendored evaluation loops from tasty-bench with attribution\n\n-
  **`BenchAction` type** wrapping `Word64 -> IO ()` for benchmarkable actions\n  -
  Enables direct benchmarking of pure functions without manual `evaluate` calls\n\n###
  Changed\n\n- **BREAKING:** All benchmark API functions now accept `BenchAction`
  instead of `IO ()`\n  - `benchGolden :: String -> BenchAction -> Spec`\n  - `benchGoldenWith
  :: BenchConfig -> String -> BenchAction -> Spec`\n  - `benchGoldenWithExpectation
  :: String -> BenchConfig -> [Expectation] -> BenchAction -> Spec`\n  - Migration:
  wrap existing `IO ()` actions with `io` combinator\n  - New: use `nf`/`whnf` for
  pure functions instead of manual `evaluate`\n\n- **Output format:** Baseline now
  appears before Actual in comparison tables\n\n- **Error messages:** Tolerance values
  in failure messages are now extracted from expectations rather than config defaults\n\n###
  Fixed\n\n- Evaluation strategy bug where GHC could share computation across benchmark
  iterations\n- Error message wording for performance improvements (now says \"decreased
  by\" instead of \"increased by -X%\")\n- Flaky micro-benchmarks stabilized by increasing
  iteration counts (500-2000 iterations)\n\n### Dependencies\n\n- Added `deepseq >=
  1.4 && < 2` for `NFData` constraint\n- Added `QuickCheck >= 2.14 && < 3` for property
  tests (test suite only)\n\n## [0.3.0]\n\n### Added\n\n- **Lens-based expectation
  combinators** for custom performance assertions\n  - New `Test.Hspec.BenchGolden.Lenses`
  module with van Laarhoven lenses for `GoldenStats` fields\n  - Lenses: `_statsMean`,
  `_statsMedian`, `_statsTrimmedMean`, `_statsStddev`, `_statsMAD`, `_statsIQR`, `_statsMin`,
  `_statsMax`\n  - Smart metric selectors: `metricFor` and `varianceFor` automatically
  choose appropriate lens based on `BenchConfig`\n  - `Expectation` type for composable
  performance assertions\n  - `Tolerance` variants: `Percent`, `Absolute`, `Hybrid`,
  `MustImprove`, `MustRegress`\n  - Boolean composition operators: `(&&~)` for AND,
  `(||~)` for OR\n  - Infix operators: `(@~)` for percentage, `(@<)` for absolute,
  `(@<<)` for must-improve, `(@>>)` for must-regress\n  - `benchGoldenWithExpectation`
  combinator for custom lens-based expectations\n  - Enables assertions like \"must
  be 10% faster\", \"median within 10%\", \"IQR < 0.1ms\"\n\n- **Tolerance helper
  functions** for manual comparison\n  - `withinPercent`, `withinAbsolute`, `withinHybrid`
  for tolerance checking\n  - `mustImprove`, `mustRegress` for directional performance
  expectations\n  - `percentDiff`, `absDiff` utilities\n\n### Changed\n\n- **Refactored
  comparison logic to use lenses internally** (non-breaking)\n  - `compareStats` now
  uses `metricFor` instead of if/else branching\n  - `checkVariance` now uses `varianceFor`
  for cleaner metric selection\n\n### Dependencies\n\n- Added `microlens >= 0.4 &&
  < 0.6`\n\n## [0.2.0] - 2026-01-30\n\n### Added\n\n- **Hybrid tolerance mechanism**
  to prevent false failures from measurement noise\n  - New `absoluteToleranceMs`
  field in `BenchConfig` (default: `Just 0.01` ms)\n  - Benchmarks now pass if EITHER
  percentage tolerance OR absolute tolerance is satisfied\n  - Eliminates random failures
  for sub-millisecond operations where measurement noise causes large percentage variations
  despite negligible absolute differences\n  \n- **Enhanced failure messages** showing
  both tolerance thresholds\n  - Regression/improvement messages now display: `\"tolerance:
  15.0% or 0.010 ms\"` when absolute tolerance is configured\n  \n- **Example benchmarks**
  demonstrating tolerance configurations\n  - Hybrid tolerance (default)\n  - Percentage-only
  tolerance\n  - Strict absolute tolerance\n  - Relaxed tolerance for CI environments\n\n###
  Changed\n\n- Default `BenchConfig` now includes `absoluteToleranceMs = Just 0.01`
  (10 microseconds)\n- `BenchResult` constructors (`Regression`, `Improvement`) now
  include `Maybe Double` for absolute tolerance\n- Updated \"sort already sorted\"
  example to use robust statistics to handle outliers\n- Increased tolerance for percentage-only
  example to 30% to reduce false failures\n\n### Fixed\n\n- Random benchmark failures
  for fast operations (< 1ms) due to measurement noise\n- False regressions when absolute
  time differences are negligible but percentage variations are large\n- Inconsistent
  test results across runs for sub-millisecond operations\n\n## [0.1.0] - 2026-01-30\n\n###
  Added\n\n- Initial release of golds-gym\n- Golden testing framework for performance
  benchmarks\n- Architecture-specific golden files\n- Integration with hspec and benchpress\n-
  Configurable tolerance for mean time comparison\n- Robust statistics mode (trimmed
  mean, MAD, outlier detection)\n- Variance warnings\n- Environment variables for
  accepting/skipping benchmarks\n- Support for both standard and robust statistical
  methods\n"
changelog-type: markdown
description: "# golds-gym \U0001F3CB️\n\n[![CI](https://github.com/ocramz/golds-gym/actions/workflows/ci.yml/badge.svg)](https://github.com/ocramz/golds-gym/actions/workflows/ci.yml)
  [![golds-gym](https://img.shields.io/hackage/v/golds-gym)](https://hackage.haskell.org/package/golds-gym)\n\nGolden
  testing for performance benchmarks. Save timing baselines on first run, compare
  against them on subsequent runs.\n\n**Key Features:**\n- Architecture-specific baselines
  (different hardware = different golden files)\n- Hybrid tolerance (handles both
  fast <1ms and slow operations)\n- Robust statistics mode (outlier detection, trimmed
  mean)\n- Lens-based custom expectations (assert \"must be faster\", compare by median,
  etc.)\n\n## Quick Start\n\n```haskell\nimport Test.Hspec\nimport Test.Hspec.BenchGolden\nimport
  Data.List (sort)\n\nmain :: IO ()\nmain = hspec $ do\n  describe \"Performance\"
  $ do\n    -- Pure function with normal form evaluation (full evaluation of the function
  arguments)\n    benchGolden \"list append\" $\n      nf (\\xs -> xs ++ xs) [1..1000]\n\n
  \   -- Custom configuration\n    benchGoldenWith defaultBenchConfig\n      { iterations
  = 500\n      , tolerancePercent = 10.0\n      }\n      \"sorting\" $\n      nf ...
  \n```\n\n**Evaluation strategies** (required - specify how values are forced):\n-
  `nf f x` - Force result of `f x` to **normal form** (deep, full evaluation)\n- `nfIO
  action` - Execute IO action and force result to normal form\n- `nfAppIO f x` - Apply
  function, execute resulting IO, force result to normal form\n- `io action` - Plain
  IO action without additional forcing\n\n**Why evaluation strategies matter**: Without
  forcing, GHC may optimize away computations or share results across iterations,
  making benchmarks meaningless.\n\n**First run** creates `.golden/<arch>/<benchmark_name>.golden`
  with baseline stats.  \n**Subsequent runs** compare against baseline. Test fails
  if mean time changes beyond tolerance (default: ±15% OR ±0.01ms).\n\n**Output format**
  :\n```\nMetric  Baseline    Actual      Diff\n------  --------    ------      ----\nMean
  \   0.150 ms  0.170 ms   +13.3%\n```\n\n**Update baselines** after intentional changes:\n```bash\nGOLDS_GYM_ACCEPT=1
  stack test\n```\n\n## How It Works\n\nGolden files store timing statistics per architecture
  (e.g., `.golden/aarch64-darwin-Apple_M1/`):\n\n```json\n{\n  \"mean\": 1.234,\n
  \ \"stddev\": 0.056,\n  \"median\": 1.201,\n  \"architecture\": \"aarch64-darwin-Apple_M1\",\n
  \ \"timestamp\": \"2026-01-30T12:00:00Z\"\n}\n```\n\n**Hybrid tolerance** (default)
  prevents false failures: benchmarks pass if within **±15% OR ±0.01ms**. This handles
  measurement noise for fast operations (<1ms) while catching real regressions for
  slower code.\n\n## Configuration\n\nKey `BenchConfig` options:\n\n| Field | Default
  | Description |\n|-------|---------|-------------|\n| `iterations` | 100 | Number
  of benchmark iterations |\n| `tolerancePercent` | 15.0 | Allowed mean time deviation
  (%) |\n| `absoluteToleranceMs` | Just 0.01 | Absolute tolerance (ms) - enables hybrid
  mode |\n| `useRobustStatistics` | False | Use trimmed mean/MAD instead of mean/stddev
  |\n| `warmupIterations` | 5 | Warm-up runs before measurement |\n\nSee `BenchConfig`
  type for all options.\n\n**Environment variables:**\n- `GOLDS_GYM_ACCEPT=1` - Regenerate
  all golden files\n- `GOLDS_GYM_SKIP=1` - Skip benchmarks entirely (useful in CI)\n\n##
  Advanced: Robust Statistics\n\nStandard mean/stddev are sensitive to outliers (GC
  pauses, OS scheduling). Robust statistics provide outlier-resistant comparisons:\n\n```haskell\nbenchGoldenWith
  defaultBenchConfig\n  { useRobustStatistics = True  -- Use trimmed mean + MAD\n
  \ , trimPercent = 10.0          -- Remove top/bottom 10%\n  , outlierThreshold =
  3.0      -- Flag outliers >3 MADs from median\n  }\n  \"noisy benchmark\" $\n  nf
  computation input\n```\n\n**When to use:**\n- Benchmarking in noisy environments
  (shared CI, development machines)\n- Operations with occasional GC pauses or system
  interruptions\n- Fast operations (<1ms) with high variance\n- You see outliers in
  test output warnings\n\n## Advanced: Lens-Based Expectations\n\nFor fine-grained
  control, use lens-based expectations to assert custom performance requirements:\n\n```haskell\nimport
  Test.Hspec.BenchGolden.Lenses\n\n-- Compare by median instead of mean (more robust)\nbenchGoldenWithExpectation
  \"median comparison\" defaultBenchConfig\n  [expect _statsMedian (Percent 10.0)]\n
  \ (nf myAlgorithm input)\n\n-- Compose multiple requirements (both must pass)\nbenchGoldenWithExpectation
  \"strict requirements\" defaultBenchConfig\n  [ expect _statsMean (Percent 15.0)
  &&~\n    expect _statsIQR (Absolute 0.1)     -- Low variance required\n  ]\n  (nf
  criticalFunction data)\n```\n\n**Available lenses:** `_statsMean`, `_statsMedian`,
  `_statsTrimmedMean`, `_statsStddev`, `_statsMAD`, `_statsIQR`, `_statsMin`, `_statsMax`\n\n**Tolerance
  types:**\n- `Percent 15.0` - Within ±15%\n- `Absolute 0.01` - Within ±0.01ms\n-
  `Hybrid 15.0 0.01` - Within ±15% OR ±0.01ms\n- `MustImprove 10.0` - Must be ≥10%
  faster (for testing optimizations)\n- `MustRegress 5.0` - Must be ≥5% slower (for
  accepting controlled regressions)\n\n**Composition:** `(&&~)` for AND, `(||~)` for
  OR\n\n## Documentation\n\n- [API documentation](https://hackage.haskell.org/package/golds-gym)
  - Full Haddock docs\n- [Example benchmarks](example/Spec.hs) - Comprehensive usage
  examples\n- [CHANGELOG](CHANGELOG.md) - Version history and migration guides\n\n##
  Related Work\n* [tasty-bench](https://hackage.haskell.org/package/tasty-bench)\n\n##
  License\n\nMIT\n"
description-type: markdown
hash: 30f1c93cd8c07ceb8c41d305347eaded6b84d24b593de964e10c4e429edfa749
homepage: ''
latest: 0.5.0.0
license-name: MIT
maintainer: '@ocramz'
synopsis: Golden testing framework for performance benchmarks
test-bench-deps:
  QuickCheck: '>=2.14 && <3'
  base: '>=4.14 && <5'
  golds-gym: '>=0'
  hspec: '>=2.10 && <3'
  statistics: '>=0.16 && <0.17'
  text: '>=1.2 && <3'
  time: '>=1.9 && <2'
  vector: '>=0.12 && <0.14'
