all-versions:
- 0.1.0.0
- 0.1.0.1
author: Tom Lippincott
basic-deps:
  base: '>=4.7 && <5'
  bytestring: '>=0.10.8.1'
  cereal: '>=0.5.4.0'
  cereal-text: '>=0.1.0.2'
  containers: '>=0.5.10.2'
  ngram: '>=0'
  optparse-generic: '>=1.2.2'
  text: '>=1.2.2'
  zlib: '>=0.6.1'
changelog: ''
changelog-type: ''
description: "# NGram\n\nThis is a code base for experimenting with various approaches
  to n-gram-based\ntext modeling.\n\n## Compiling\n\nFirst install [Stack](https://docs.haskellstack.org)
  somewhere on your `PATH`.  For example, for `~/.local/bin`:\n\n```\nwget https://get.haskellstack.org/stable/linux-x86_64.tar.gz
  -O -|tar xpfz - -C /tmp\ncp /tmp/stack-*/stack ~/.local/bin\nrm -rf /tmp/stack-*\n```\n\nThen,
  while in the directory of this README file, run:\n\n```\nstack build\n```\n\nThe
  first time this runs will take a while, 10 or 15 minutes, as it builds an entire
  Haskell environment from scratch.  Subsequent compilations are very fast.\n\n##
  Running\n\nGenerally, the commands expect data to be text files where each line
  has the format:\n\n```\n${id}<TAB>${label}<TAB>${text}\n```\n\nWhen a model is applied
  to data, the output will generally have a header\nwith the format:\n\n```\nID<TAB>GOLD<TAB>${label_1_name}<TAB>${label_2_name}<TAB>...\n```\n\nand
  lines with the corresponding format:\n\n```\n${doc_id}<TAB>${gold_label_name}<TAB>${label_1_prob}<TAB>${label_2_prob}<TAB>...\n```\n\nwhere
  probabilities are represented as natural logarithms.\n\nThe remainder of this document
  describes the implemented models, most of which\nhave a corresponding command that
  *stack* will have installed.  The library aims\nto be parametric over the sequence
  types, and most commands allow users to \nspecify whether to consider bytes, unicode
  characters, or whitespace-delimited \ntokens.\n\n## Prediction by Partial Matching\n\nPPM
  is essentially an n-gram model with a particular backoff logic that can't \nquite
  be reduced to more widespread approaches to smoothing, but empirically \ntends to
  outperform them on short documents.  To create a PPM model, run:\n\n```bash\nsh>
  stack exec -- ngramClassifier train --train train.txt --dev dev.txt --n 4 --modelFile
  model.gz\nDev accuracy: 0.8566666666666667\n```\n\nThe model can then be applied
  to new data:\n\n```bash\nsh> stack exec -- ngramClassifier apply --test test.txt
  --modelFile model.gz --n 4 --scoresFile scores.txt\n```\n\nThe value of `--n` can
  also be less than the model size, which will run a bit \nfaster, and (perhaps) less
  tuned to the original training data.\n"
description-type: markdown
hash: 9c6559ab309a0cc49090525b0094fd40c7fa804c2ba3450c84dd085d910a8395
homepage: https://github.com/TomLippincott/ngram#readme
latest: 0.1.0.1
license-name: BSD-3-Clause
maintainer: tom@cs.jhu.edu
synopsis: Ngram models for compressing and classifying text.
test-bench-deps: {}
