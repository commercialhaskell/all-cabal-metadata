all-versions:
- '0.1'
- '0.2'
- '0.3'
- 0.3.1
- '0.4'
- '0.5'
- 0.5.1
- 0.6.0
- 0.6.1
- 0.6.2
- 0.6.2.1
author: Gwern
basic-deps:
  HTTP: '>=0'
  base: '>=4 && <5'
  bytestring: '>=0'
  containers: '>=0'
  curl: '>=0'
  network: '>=0'
  process: '>=0'
  random: '>=0'
changelog: ''
changelog-type: ''
description: |-
  `archiver` is a daemon which will process a specified text file,
  each line of which is a URL, and will (randomly) one by one request that
  the URLs be archived or spidered by <http://www.webcitation.org>,
  <http://www.archive.org>, and <http://www.wikiwix.com> for future reference.
  (One may optionally specify an arbitrary `sh` command like `wget` to download URLs locally.)

  Because the interface is a simple text file, this can be combined
  with other scripts; for example, a script using Sqlite to extract
  visited URLs from Firefox, or a program extracting URLs from Pandoc
  documents. (See <http://www.gwern.net/Archiving%20URLs>.)

  For explanation of the derivation of the code in `Network.URL.Archiver`,
  see <http://www.gwern.net/haskell/Wikipedia%20Archive%20Bot>.
description-type: haddock
hash: 8d87f19ff518c8b6cd3ecded5aeffcf438b534d8b4a13cbc6992302773a2b37e
homepage: ''
latest: 0.6.2.1
license-name: BSD-3-Clause
maintainer: Gwern <gwern@gwern.net>
synopsis: Archive supplied URLs in WebCite & Internet Archive
test-bench-deps: {}
