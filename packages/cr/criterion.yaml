all-versions:
- '0.1'
- 0.1.2
- 0.1.3
- 0.1.4
- 0.2.0
- 0.3.0
- 0.4.0
- 0.4.1.0
- 0.5.0.0
- 0.5.0.1
- 0.5.0.2
- 0.5.0.3
- 0.5.0.4
- 0.5.0.5
- 0.5.0.7
- 0.5.0.8
- 0.5.0.9
- 0.5.0.10
- 0.5.1.0
- 0.5.1.1
- 0.6.0.0
- 0.6.0.1
- 0.6.1.1
- 0.6.2.0
- 0.6.2.1
- 0.8.0.0
- 0.8.0.1
- 0.8.0.2
- 0.8.1.0
- 1.0.0.0
- 1.0.0.1
- 1.0.0.2
- 1.0.1.0
- 1.0.2.0
- 1.1.0.0
- 1.1.1.0
- 1.1.4.0
- 1.2.2.0
- 1.2.3.0
- 1.2.4.0
- 1.2.5.0
- 1.2.6.0
- 1.3.0.0
- 1.4.0.0
- 1.4.1.0
- 1.5.0.0
- 1.5.1.0
- 1.5.2.0
- 1.5.3.0
- 1.5.4.0
- 1.5.5.0
- 1.5.6.0
- 1.5.6.1
- 1.5.6.2
- 1.5.7.0
- 1.5.8.0
- 1.5.9.0
- 1.5.10.0
- 1.5.11.0
- 1.5.12.0
- 1.5.13.0
- 1.6.0.0
- 1.6.1.0
- 1.6.2.0
- 1.6.3.0
- 1.6.4.0
- 1.6.4.1
author: Bryan O'Sullivan <bos@serpentine.com>
basic-deps:
  Glob: '>=0.7.2'
  aeson: '>=2 && <2.3'
  base: '>=4.9 && <5'
  base-compat-batteries: '>=0.10 && <0.15'
  binary: '>=0.8.3.0'
  binary-orphans: '>=1.0.1 && <1.1'
  bytestring: '>=0.10.8.1 && <1.0'
  cassava: '>=0.3.0.0'
  code-page: '>=0'
  containers: '>=0'
  criterion: '>=0'
  criterion-measurement: '>=0.2 && <0.3'
  deepseq: '>=1.1.0.0'
  directory: '>=0'
  exceptions: '>=0.8.2 && <0.11'
  filepath: '>=0'
  js-chart: '>=2.9.4 && <3'
  microstache: '>=1.0.1 && <1.1'
  mtl: '>=2'
  mwc-random: '>=0.8.0.3'
  optparse-applicative: '>=0.18 && <0.20'
  parsec: '>=3.1.0'
  prettyprinter: '>=1.7 && <1.8'
  prettyprinter-ansi-terminal: '>=1.1 && <1.2'
  statistics: '>=0.14 && <0.17'
  text: '>=0.11'
  time: '>=0'
  transformers: '>=0'
  transformers-compat: '>=0.6.4'
  vector: '>=0.7.1'
  vector-algorithms: '>=0.4'
changelog: "1.6.4.1\r\n\r\n* Merge tutorial into README.\r\n\r\n1.6.4.0\r\n\r\n* Drop
  support for pre-8.0 versions of GHC.\r\n\r\n1.6.3.0\r\n\r\n* Remove a use of the
  partial `head` function within `criterion`.\r\n\r\n1.6.2.0\r\n\r\n* Require `optparse-applicative-0.18.*`
  as the minimum and add an explicit\r\n  dependency on `prettyprinter` and `prettyprinter-ansi-terminal`.\r\n\r\n1.6.1.0\r\n\r\n*
  Support building with `optparse-applicative-0.18.*`.\r\n\r\n1.6.0.0\r\n\r\n* `criterion-measurement-0.2.0.0`
  adds the `measPeakMbAllocated` field to\r\n  `Measured` for reporting maximum megabytes
  allocated. Since `criterion`\r\n  re-exports `Measured` from `Criterion.Types`,
  this change affects `criterion`\r\n  as well. Naturally, this affects the behavior
  of `Measured`'s `{To,From}JSON`\r\n  and `Binary` instances.\r\n* Fix a bug in which
  the `--help` text for the `--match` option was printed\r\n  twice in `criterion`
  applications.\r\n\r\n1.5.13.0\r\n\r\n* Allow building with `optparse-applicative-0.17.*`.\r\n\r\n1.5.12.0\r\n\r\n*
  Fix a bug introduced in version 1.5.9.0 in which benchmark names that include\r\n
  \ double quotes would produce broken HTML reports.\r\n\r\n1.5.11.0\r\n\r\n* Allow
  building with `aeson-2.0.0.0`.\r\n\r\n1.5.10.0\r\n\r\n* Fix a bug in which the `defaultMainWith`
  function would not use the\r\n  `regressions` values specified in the `Config` argument.
  This bug only\r\n  affected `criterion` the library—uses of the `--regressions`
  flag from\r\n  `criterion` executables themselves were unaffected.\r\n\r\n1.5.9.0\r\n\r\n*
  Fix a bug where HTML reports failed to escape JSON properly.\r\n\r\n1.5.8.0\r\n\r\n*
  The HTML reports have been reworked.\r\n\r\n  * The `flot` plotting library (`js-flot`
  on Hackage) has been replaced by\r\n    `Chart.js` (`js-chart`).\r\n  * Most practical
  changes focus on improving the functionality of the overview\r\n    chart:\r\n    *
  It now supports logarithmic scale (#213). The scale can be toggled by\r\n      clicking
  the x-axis.\r\n    * Manual zooming has been replaced by clicking to focus a single
  bar.\r\n    * It now supports a variety of sort orders.\r\n    * The legend can
  now be toggled on/off and is hidden by default.\r\n    * Clicking the name of a
  group in the legend shows/hides all bars in that\r\n      group.\r\n  * The regression
  line on the scatter plot shows confidence interval.\r\n  * Better support for mobile
  and print.\r\n  * JSON escaping has been made more robust by no longer directly
  injecting\r\n    reports as JavaScript code.\r\n\r\n1.5.7.0\r\n\r\n* Warn if an
  HTML report name contains newlines, and replace newlines with\r\n  whitespace to
  avoid syntax errors in the report itself.\r\n\r\n1.5.6.2\r\n\r\n* Use unescaped
  HTML in the `json.tpl` template.\r\n\r\n1.5.6.1\r\n\r\n* Bundle `criterion-examples`'
  `LICENSE` file.\r\n\r\n1.5.6.0\r\n\r\n* Allow building with `base-compat-batteries-0.11`.\r\n\r\n1.5.5.0\r\n\r\n*
  Fix the build on old GHCs with the `embed-data-files` flag.\r\n* Require `transformers-compat-0.6.4`
  or later.\r\n\r\n1.5.4.0\r\n\r\n* Add `parserWith`, which allows creating a `criterion`
  command-line interface\r\n  using a custom `optparse-applicative` `Parser`. This
  is usefule for sitations\r\n  where one wants to add additional command-line arguments
  to the default ones\r\n  that `criterion` provides.\r\n\r\n  For an example of how
  to use `parserWith`, refer to\r\n  `examples/ExtensibleCLI.hs`.\r\n\r\n* Tweak the
  way the graph in the HTML overview zooms:\r\n\r\n  * Zooming all the way out resets
  to the default view (instead of continuing\r\n    to zoom out towards empty space).\r\n
  \ * Panning all the way to the right resets to the default view in which zero\r\n
  \   is left-aligned (instead of continuing to pan off the edge of the graph).\r\n
  \ * Panning and zooming only affecs the x-axis, so all results remain in-frame.\r\n\r\n1.5.3.0\r\n\r\n*
  Make more functions (e.g., `runMode`) able to print the `µ` character on\r\n  non-UTF-8
  encodings.\r\n\r\n1.5.2.0\r\n\r\n* Fix a bug in which HTML reports would render
  incorrectly when including\r\n  benchmark names containing apostrophes.\r\n\r\n*
  Only incur a dependency on `fail` on old GHCs.\r\n\r\n1.5.1.0\r\n\r\n* Add a `MonadFail
  Criterion` instance.\r\n\r\n* Add some documentation in `Criterion.Main` about `criterion-measurement`'s\r\n
  \ new `nfAppIO` and `whnfAppIO` functions, which `criterion` reexports.\r\n\r\n1.5.0.0\r\n\r\n*
  Move the measurement functionality of `criterion` into a standalone package,\r\n
  \ `criterion-measurement`. In particular, `cbits/` and `Criterion.Measurement`\r\n
  \ are now in `criterion-measurement`, along with the relevant definitions of\r\n
  \ `Criterion.Types` and `Criterion.Types.Internal` (both of which are now under\r\n
  \ the `Criterion.Measurement.*` namespace).\r\n  Consequently, `criterion` now depends
  on `criterion-measurement`.\r\n\r\n  This will let other libraries (e.g. alternative
  statistical analysis\r\n  front-ends) to import the measurement functionality alone
  as a lightweight\r\n  dependency.\r\n\r\n* Fix a bug on macOS and Windows where
  using `runAndAnalyse` and other\r\n  lower-level benchmarking functions would result
  in an infinite loop.\r\n\r\n1.4.1.0\r\n\r\n* Use `base-compat-batteries`.\r\n\r\n1.4.0.0\r\n\r\n*
  We now do three samples for statistics:\r\n\r\n  * `performMinorGC` before the first
  sample, to ensure it's up to date.\r\n  * Take another sample after the action,
  without a garbage collection, so we\r\n    can gather legitimate readings on GC-related
  statistics.\r\n  * Then `performMinorGC` and sample once more, so we can get up-to-date\r\n
  \   readings on other metrics.\r\n\r\n  The type of `applyGCStatistics` has changed
  accordingly. Before, it was:\r\n\r\n  ```haskell\r\n     Maybe GCStatistics -- ^
  Statistics gathered at the end of a run.\r\n  -> Maybe GCStatistics -- ^ Statistics
  gathered at the beginning of a run.\r\n  -> Measured -> Measured\r\n  ```\r\n\r\n
  \ Now, it is:\r\n\r\n  ```haskell\r\n     Maybe GCStatistics -- ^ Statistics gathered
  at the end of a run, post-GC.\r\n  -> Maybe GCStatistics -- ^ Statistics gathered
  at the end of a run, pre-GC.\r\n  -> Maybe GCStatistics -- ^ Statistics gathered
  at the beginning of a run.\r\n  -> Measured -> Measured\r\n  ```\r\n\r\n  When diffing
  `GCStatistics` in `applyGCStatistics`, we carefully choose\r\n  whether to diff
  against the end stats pre- or post-GC.\r\n\r\n* Use `performMinorGC` rather than
  `performGC` to update garbage collection\r\n  statistics. This improves the benchmark
  performance of fast functions on large\r\n  objects.\r\n\r\n* Fix a bug in the `ToJSON
  Measured` instance which duplicated the\r\n  mutator CPU seconds where GC CPU seconds
  should go.\r\n\r\n* Fix a bug in sample analysis which incorrectly accounted for
  overhead\r\n  causing runtime errors and invalid results. Accordingly, the buggy\r\n
  \ `getOverhead` function has been removed.\r\n\r\n* Fix a bug in `Measurement.measure`
  which inflated the reported time taken\r\n  for `perRun` benchmarks.\r\n\r\n* Reduce
  overhead of `nf`, `whnf`, `nfIO`, and `whnfIO` by removing allocation\r\n  from
  the central loops.\r\n\r\n1.3.0.0\r\n\r\n* `criterion` was previously reporting
  the following statistics incorrectly\r\n  on GHC 8.2 and later:\r\n\r\n  * `gcStatsBytesAllocated`\r\n
  \ * `gcStatsBytesCopied`\r\n  * `gcStatsGcCpuSeconds`\r\n  * `gcStatsGcWallSeconds`\r\n\r\n
  \ This has been fixed.\r\n\r\n* The type signature of `runBenchmarkable` has changed
  from:\r\n\r\n  ```haskell\r\n  Benchmarkable -> Int64 -> (a -> a -> a) -> (IO ()
  -> IO a) -> IO a\r\n  ```\r\n\r\n  to:\r\n\r\n  ```haskell\r\n  Benchmarkable ->
  Int64 -> (a -> a -> a) -> (Int64 -> IO () -> IO a) -> IO a\r\n  ```\r\n\r\n  The
  extra `Int64` argument represents how many iterations are being timed.\r\n\r\n*
  Remove the deprecated `getGCStats` and `applyGCStats` functions (which have\r\n
  \ been replaced by `getGCStatistics` and `applyGCStatistics`).\r\n* Remove the deprecated
  `forceGC` field of `Config`, as well as the\r\n  corresponding `--no-gc` command-line
  option.\r\n* The header in generated JSON output mistakenly used the string `\"criterio\"`.\r\n
  \ This has been corrected to `\"criterion\"`.\r\n\r\n1.2.6.0\r\n\r\n* Add error
  bars and zoomable navigation to generated HTML report graphs.\r\n\r\n  (Note that
  there have been reports that this feature can be somewhat unruly\r\n  when using
  macOS and Firefox simultaneously. See\r\n  https://github.com/flot/flot/issues/1554
  for more details.)\r\n\r\n* Use a predetermined set of cycling colors for benchmark
  groups in HTML\r\n  reports. This avoids a bug in earlier versions of `criterion`
  where benchmark\r\n  group colors could be chosen that were almost completely white,
  which made\r\n  them impossible to distinguish from the background.\r\n\r\n1.2.5.0\r\n\r\n*
  Add an `-fembed-data-files` flag. Enabling this option will embed the\r\n  `data-files`
  from `criterion.cabal` directly into the binary, producing\r\n  a relocatable executable.
  (This has the downside of increasing the binary\r\n  size significantly, so be warned.)\r\n\r\n1.2.4.0\r\n\r\n*
  Fix issue where `--help` would display duplicate options.\r\n\r\n1.2.3.0\r\n\r\n*
  Add a `Semigroup` instance for `Outliers`.\r\n\r\n* Improve the error messages that
  are thrown when forcing nonexistent\r\n  benchmark environments.\r\n\r\n* Explicitly
  mark `forceGC` as deprecated. `forceGC` has not had any effect\r\n  for several
  releases, and it will be removed in the next major `criterion`\r\n  release.\r\n\r\n1.2.2.0\r\n\r\n*
  Important bugfix: versions 1.2.0.0 and 1.2.1.0 were incorrectly displaying\r\n  the
  lower and upper bounds for measured values on HTML reports.\r\n\r\n* Have `criterion`
  emit warnings if suspicious things happen during mustache\r\n  template substitution
  when creating HTML reports. This can be useful when\r\n  using custom templates
  with the `--template` flag.\r\n\r\n1.2.1.0\r\n\r\n* Add `GCStatistics`, `getGCStatistics`,
  and `applyGCStatistics` to\r\n  `Criterion.Measurement`. These are inteded to replace
  `GCStats` (which has\r\n  been deprecated in `base` and will be removed in GHC 8.4),
  as well as\r\n  `getGCStats` and `applyGCStats`, which have also been deprecated
  and will be\r\n  removed in the next major `criterion` release.\r\n\r\n* Add new
  matchers for the `--match` flag:\r\n  * `--match pattern`, which matches by searching
  for a given substring in\r\n    benchmark paths.\r\n  * `--match ipattern`, which
  is like `--match pattern` but case-insensitive.\r\n\r\n* Export `Criterion.Main.Options.config`.\r\n\r\n*
  Export `Criterion.toBenchmarkable`, which behaves like the `Benchmarkable`\r\n  constructor
  did prior to `criterion-1.2.0.0`.\r\n\r\n1.2.0.0\r\n\r\n* Use `statistics-0.14`.\r\n\r\n*
  Replace the `hastache` dependency with `microstache`.\r\n\r\n* Add support for per-run
  allocation/cleanup of the environment with\r\n  `perRunEnv` and `perRunEnvWithCleanup`,\r\n\r\n*
  Add support for per-batch allocation/cleanup with\r\n  `perBatchEnv` and `perBatchEnvWithCleanup`.\r\n\r\n*
  Add `envWithCleanup`, a variant of `env` with cleanup support.\r\n\r\n* Add the
  `criterion-report` executable, which creates reports from previously\r\n  created
  JSON files.\r\n\r\n1.1.4.0\r\n\r\n* Unicode output is now correctly printed on Windows.\r\n\r\n*
  Add Safe Haskell annotations.\r\n\r\n* Add `--json` option for writing reports in
  JSON rather than binary\r\n  format.  Also: various bugfixes related to this.\r\n\r\n*
  Use the `js-jquery` and `js-flot` libraries to substitute in JavaScript code\r\n
  \ into the default HTML report template.\r\n\r\n* Use the `code-page` library to
  ensure that `criterion` prints out Unicode\r\n  characters (like ², which `criterion`
  uses in reports) in a UTF-8-compatible\r\n  code page on Windows.\r\n\r\n* Give
  an explicit implementation for `get` in the `Binary Regression`\r\n  instance. This
  should fix sporadic `criterion` failures with older versions\r\n  of `binary`.\r\n\r\n*
  Use `tasty` instead of `test-framework` in the test suites.\r\n\r\n* Restore support
  for 32-bit Intel CPUs.\r\n\r\n* Restore build compatibilty with GHC 7.4.\r\n\r\n1.1.1.0\r\n\r\n*
  If a benchmark uses `Criterion.env` in a non-lazy way, and you try\r\n  to use `--list`
  to list benchmark names, you'll now get an\r\n  understandable error message instead
  of something cryptic.\r\n\r\n* We now flush stdout and stderr after printing messages,
  so that\r\n  output is printed promptly even when piped (e.g. into a pager).\r\n\r\n*
  A new function `runMode` allows custom benchmarking applications to\r\n  run benchmarks
  with control over the `Mode` used.\r\n\r\n* Added support for Linux on non-Intel
  CPUs.\r\n\r\n* This version supports GHC 8.\r\n\r\n* The `--only-run` option for
  benchmarks is renamed to `--iters`.\r\n\r\n1.1.0.0\r\n\r\n* The dependency on the
  either package has been dropped in favour of a\r\n  dependency on transformers-compat.
  \ This greatly reduces the number\r\n  of packages criterion depends on.  This shouldn't
  affect the\r\n  user-visible API.\r\n\r\n* The documentation claimed that environments
  were created only when\r\n  needed, but this wasn't implemented. (gh-76)\r\n\r\n*
  The package now compiles with GHC 7.10.\r\n\r\n* On Windows with a non-Unicode code
  page, printing results used to\r\n  cause a crash.  (gh-55)\r\n\r\n1.0.2.0\r\n\r\n*
  Bump lower bound on optparse-applicative to 0.11 to handle yet more\r\n  annoying
  API churn.\r\n\r\n1.0.1.0\r\n\r\n* Added a lower bound of 0.10 on the optparse-applicative
  dependency,\r\n  as there were major API changes between 0.9 and 0.10.\r\n"
changelog-type: markdown
description: "# Criterion: robust, reliable performance measurement\r\n\r\n[![Hackage](https://img.shields.io/hackage/v/criterion.svg)](https://hackage.haskell.org/package/criterion)
  [![Build Status](https://github.com/haskell/criterion/workflows/Haskell-CI/badge.svg)](https://github.com/haskell/criterion/actions?query=workflow%3AHaskell-CI)\r\n\r\n`criterion`
  is a library that makes accurate microbenchmarking in\r\nHaskell easy.\r\n\r\n<a
  href=\"https://hackage.haskell.org/package/criterion/src/www/fibber.html\" target=\"_blank\"><img
  src=\"https://hackage.haskell.org/package/criterion/src/www/fibber-screenshot.png\"></a>\r\n\r\n\r\n##
  Features\r\n\r\n* The simple API hides a lot of automation and details that you\r\n
  \ shouldn't need to worry about.\r\n\r\n* Sophisticated, high-resolution analysis
  which can accurately measure\r\n  operations that run in as little as a few hundred
  picoseconds.\r\n\r\n* [Output to active HTML](https://hackage.haskell.org/package/criterion/src/www/report.html)
  (with JavaScript charts), CSV,\r\n  and JSON. Write your own report templates to
  customize exactly how\r\n  your results are presented.\r\n\r\n* Linear regression
  model that allows measuring the effects of garbage\r\n  collection and other factors.\r\n\r\n*
  Measurements are cross-validated to ensure that sources of\r\n  significant noise
  (usually other activity on the system) can be\r\n  identified.\r\n\r\n\r\nTo get
  started, read the [tutorial below](#tutorial), and take a look\r\nat the programs
  in the [examples\r\ndirectory](https://github.com/haskell/criterion/tree/master/examples).\r\n\r\n\r\n##
  Credits and contacts\r\n\r\nThis library is written by Bryan O'Sullivan\r\n(<bos@serpentine.com>)
  and maintained by Ryan Scott (<ryan.gl.scott@gmail.com>).\r\nPlease report bugs
  via the\r\n[GitHub issue tracker](https://github.com/haskell/criterion/issues).\r\n\r\n\r\n#
  Tutorial\r\n\r\n\r\n## Getting started\r\n\r\nHere's `Fibber.hs`: a simple and complete
  benchmark, measuring the performance of\r\nthe ever-ridiculous `fib` function.\r\n\r\n```haskell\r\n{-
  cabal:\r\nbuild-depends: base, criterion\r\n-}\r\n\r\nimport Criterion.Main\r\n\r\n--
  The function we're benchmarking.\r\nfib :: Int -> Int\r\nfib m | m < 0     = error
  \"negative!\"\r\n      | otherwise = go m\r\n  where\r\n    go 0 = 0\r\n    go 1
  = 1\r\n    go n = go (n - 1) + go (n - 2)\r\n\r\n-- Our benchmark harness.\r\nmain
  = defaultMain [\r\n  bgroup \"fib\" [ bench \"1\"  $ whnf fib 1\r\n               ,
  bench \"5\"  $ whnf fib 5\r\n               , bench \"9\"  $ whnf fib 9\r\n               ,
  bench \"11\" $ whnf fib 11\r\n               ]\r\n  ]\r\n```\r\n([examples/Fibber.hs](https://github.com/haskell/criterion/blob/master/examples/Fibber.hs))\r\n\r\nThe\r\n[`defaultMain`](http://hackage.haskell.org/package/criterion/docs/Criterion-Main.html#v:defaultMain)\r\nfunction
  takes a list of\r\n[`Benchmark`](http://hackage.haskell.org/package/criterion/docs/Criterion-Main.html#t:Benchmark)\r\nvalues,
  each of which describes a function to benchmark.  (We'll come\r\nback to `bench`
  and `whnf` shortly, don't worry.)\r\n\r\nTo maximise our convenience, `defaultMain`
  will parse command line\r\narguments and then run any benchmarks we ask. Let's run
  our benchmark\r\nprogram (it might take some time if you never used Criterion before,
  since\r\nthe library has to be downloaded and compiled).\r\n\r\n```\r\n$ cabal run
  Fibber.hs\r\nbenchmarking fib/1\r\ntime                 13.77 ns   (13.49 ns ..
  14.07 ns)\r\n                     0.998 R²   (0.997 R² .. 1.000 R²)\r\nmean                 13.56
  ns   (13.49 ns .. 13.70 ns)\r\nstd dev              305.1 ps   (64.14 ps .. 532.5
  ps)\r\nvariance introduced by outliers: 36% (moderately inflated)\r\n\r\nbenchmarking
  fib/5\r\ntime                 173.9 ns   (172.8 ns .. 175.6 ns)\r\n                     1.000
  R²   (0.999 R² .. 1.000 R²)\r\nmean                 173.8 ns   (173.1 ns .. 175.4
  ns)\r\nstd dev              3.149 ns   (1.842 ns .. 5.954 ns)\r\nvariance introduced
  by outliers: 23% (moderately inflated)\r\n\r\nbenchmarking fib/9\r\ntime                 1.219
  μs   (1.214 μs .. 1.228 μs)\r\n                     1.000 R²   (1.000 R² .. 1.000
  R²)\r\nmean                 1.219 μs   (1.216 μs .. 1.223 μs)\r\nstd dev              12.43
  ns   (9.907 ns .. 17.29 ns)\r\n\r\nbenchmarking fib/11\r\ntime                 3.253
  μs   (3.246 μs .. 3.260 μs)\r\n                     1.000 R²   (1.000 R² .. 1.000
  R²)\r\nmean                 3.248 μs   (3.243 μs .. 3.254 μs)\r\nstd dev              18.94
  ns   (16.57 ns .. 21.95 ns)\r\n\r\n```\r\n\r\nEven better, the `--output` option
  directs our program to write a\r\nreport to the file [`fibber.html`](fibber.html).\r\n\r\n```shellsession\r\n$
  cabal run Fibber.hs -- --output fibber.html\r\n...similar output as before...\r\n```\r\n\r\nClick
  on the image to see a complete report. If you mouse over the data\r\npoints in the
  charts, you'll see that they are *live*, giving additional\r\ninformation about
  what's being displayed.\r\n\r\n<a href=\"https://hackage.haskell.org/package/criterion/src/www/fibber.html\"
  target=\"_blank\"><img src=\"https://hackage.haskell.org/package/criterion/src/www/fibber-screenshot.png\"></a>\r\n\r\n\r\n###
  Understanding charts\r\n\r\nA report begins with a summary of all the numbers measured.\r\nUnderneath
  is a breakdown of every benchmark, each with two charts and\r\nsome explanation.\r\n\r\nThe
  chart on the left is a\r\n[kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation)\r\n(also
  known as a KDE) of time measurements.  This graphs the\r\n*probability* of any given
  time measurement occurring.  A spike\r\nindicates that a measurement of a particular
  time occurred; its height\r\nindicates how often that measurement was repeated.\r\n\r\n>
  [!NOTE]\r\n> **Why not use a histogram?**\r\n> \r\n> A more popular alternative
  to the KDE for this kind of display is the\r\n> [histogram](https://en.wikipedia.org/wiki/Histogram).
  \ Why do we use a\r\n> KDE instead?  In order to get good information out of a histogram,
  you\r\n> have to\r\n> [choose a suitable bin size](https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width).\r\n>
  This is a fiddly manual task.  In contrast, a KDE is likely to be\r\n> informative
  immediately, with no configuration required.\r\n\r\nThe chart on the right contains
  the raw measurements from which the\r\nkernel density estimate was built. The $x$
  axis indicates the number\r\nof loop iterations, while the $y$ axis shows measured
  execution time\r\nfor the given number of iterations. The line “behind” the values
  is a\r\nlinear regression generated from this data.  Ideally, all measurements\r\nwill
  be on (or very near) this line.\r\n\r\n\r\n### Understanding the data under a chart\r\n\r\nUnderneath
  the chart for each benchmark is a small table of\r\ninformation that looks like
  this.\r\n\r\n|                      | lower bound | estimate   | upper bound |\r\n|----------------------|-------------|------------|-------------|\r\n|
  OLS regression       | 31.0 ms     | 37.4 ms    | 42.9 ms     |\r\n| R² goodness-of-fit
  \  | 0.887       | 0.942      | 0.994       |\r\n| Mean execution time  | 34.8 ms
  \    | 37.0 ms    | 43.1 ms     |\r\n| Standard deviation   | 2.11 ms     | 6.49
  ms    | 11.0 ms     |\r\n\r\nThe second row is the result of a linear regression
  run on the measurements displayed in the right-hand chart.\r\n\r\n* “**OLS regression**”
  estimates the time needed for a single\r\n  execution of the activity being benchmarked,
  using an\r\n  [ordinary least-squares regression model](https://en.wikipedia.org/wiki/Ordinary_least_squares).\r\n
  \ This number should be similar to the “mean execution time” row a\r\n  couple of
  rows beneath.  The OLS estimate is usually more accurate\r\n  than the mean, as
  it more effectively eliminates measurement\r\n  overhead and other constant factors.\r\n\r\n*
  “**R² goodness-of-fit**” is a measure of how accurately the linear\r\n  regression
  model fits the observed measurements. If the measurements\r\n  are not too noisy,
  R² should lie between 0.99 and 1, indicating an\r\n  excellent fit. If the number
  is below 0.99, something is confounding\r\n  the accuracy of the linear model.  A
  value below 0.9 is outright\r\n  worrisome.\r\n\r\n* “**Mean execution time**” and
  “**Standard deviation**” are\r\n  statistics calculated (more or less) from execution
  time divided by\r\n  number of iterations.\r\n\r\nOn either side of the main column
  of values are greyed-out lower and\r\nupper bounds.  These measure the *accuracy*
  of the main estimate using\r\na statistical technique called\r\n[*bootstrapping*](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).
  This\r\ntells us that when randomly resampling the data, 95% of estimates fell\r\nwithin
  between the lower and upper bounds.  When the main estimate is\r\nof good quality,
  the lower and upper bounds will be close to its\r\nvalue.\r\n\r\n\r\n## Reading
  command line output\r\n\r\nBefore you look at HTML reports, you'll probably start
  by inspecting\r\nthe report that criterion prints in your terminal window.\r\n\r\n```\r\nbenchmarking
  ByteString/HashMap/random\r\ntime                 4.046 ms   (4.020 ms .. 4.072
  ms)\r\n                     1.000 R²   (1.000 R² .. 1.000 R²)\r\nmean                 4.017
  ms   (4.010 ms .. 4.027 ms)\r\nstd dev              27.12 μs   (20.45 μs .. 38.17
  μs)\r\n```\r\n\r\nThe first column is a name; the second is an estimate. The third
  and\r\nfourth, in parentheses, are the 95% lower and upper bounds on the\r\nestimate.\r\n\r\n*
  `time` corresponds to the “OLS regression” field in the HTML table\r\n  above.\r\n\r\n*
  `R²` is the goodness-of-fit metric for `time`.\r\n\r\n* `mean` and `std dev` have
  the same meanings as “Mean execution time”\r\n  and “Standard deviation” in the
  HTML table.\r\n\r\n\r\n## How to write a benchmark suite\r\n\r\nA criterion benchmark
  suite consists of a series of\r\n[`Benchmark`](http://hackage.haskell.org/package/criterion/docs/Criterion-Main.html#t:Benchmark)\r\nvalues.\r\n\r\n```haskell\r\nmain
  = defaultMain [\r\n  bgroup \"fib\" [ bench \"1\"  $ whnf fib 1\r\n               ,
  bench \"5\"  $ whnf fib 5\r\n               , bench \"9\"  $ whnf fib 9\r\n               ,
  bench \"11\" $ whnf fib 11\r\n               ]\r\n  ]\r\n```\r\n\r\n\r\nWe group
  related benchmarks together using the\r\n[`bgroup`](http://hackage.haskell.org/package/criterion/docs/Criterion-Main.html#v:bgroup)\r\nfunction.
  \ Its first argument is a name for the group of benchmarks.\r\n\r\n```haskell\r\nbgroup
  :: String -> [Benchmark] -> Benchmark\r\n```\r\n\r\nAll the magic happens with the\r\n[`bench`](http://hackage.haskell.org/package/criterion/docs/Criterion-Main.html#v:bench)\r\nfunction.
  \ The first argument to `bench` is a name that describes the\r\nactivity we're benchmarking.\r\n\r\n```haskell\r\nbench
  :: String -> Benchmarkable -> Benchmark\r\nbench = Benchmark\r\n```\r\n\r\nThe\r\n[`Benchmarkable`](http://hackage.haskell.org/package/criterion/docs/Criterion-Main.html#t:Benchmarkable)\r\ntype
  is a container for code that can be benchmarked.\r\n\r\nBy default, criterion allows
  two kinds of code to be benchmarked.\r\n\r\n* Any `IO` action can be benchmarked
  directly.\r\n\r\n* With a little trickery, we can benchmark pure functions.\r\n\r\n\r\n###
  Benchmarking an `IO` action\r\n\r\nThis function shows how we can benchmark an `IO`
  action.\r\n\r\n```haskell\r\nimport Criterion.Main\r\n\r\nmain = defaultMain [\r\n
  \   bench \"readFile\" $ nfIO (readFile \"GoodReadFile.hs\")\r\n  ]\r\n```\r\n([examples/GoodReadFile.hs](https://github.com/haskell/criterion/blob/master/examples/GoodReadFile.hs))\r\n\r\nWe
  use\r\n[`nfIO`](http://hackage.haskell.org/package/criterion/docs/Criterion-Main.html#v:nfIO)\r\nto
  specify that after we run the `IO` action, its result must be\r\nevaluated to <span
  id=\"normal-form\">normal form</span>, i.e. so that\r\nall of its internal constructors
  are fully evaluated, and it contains\r\nno thunks.\r\n\r\n```haskell\r\nnfIO ::
  NFData a => IO a -> Benchmarkable\r\n```\r\n\r\nRules of thumb for when to use `nfIO`:\r\n\r\n*
  Any time that lazy I/O is involved, use `nfIO` to avoid resource\r\n  leaks.\r\n\r\n*
  If you're not sure how much evaluation will have been performed on\r\n  the result
  of an action, use `nfIO` to be certain that it's fully\r\n  evaluated.\r\n\r\n\r\n###
  `IO` and `seq`\r\n\r\nIn addition to `nfIO`, criterion provides a\r\n[`whnfIO`](http://hackage.haskell.org/package/criterion/docs/Criterion-Main.html#v:whnfIO)\r\nfunction
  that evaluates the result of an action only deep enough for\r\nthe outermost constructor
  to be known (using `seq`).  This is known as\r\n<span id=\"weak-head-normal-form\">**weak
  head normal form** (WHNF)</span>.\r\n\r\n```haskell\r\nwhnfIO :: IO a -> Benchmarkable\r\n```\r\n\r\nThis
  function is useful if your `IO` action returns a simple value\r\nlike an `Int`,
  or something more complex like a\r\n[`Map`](http://hackage.haskell.org/package/containers/docs/Data-Map-Lazy.html#t:Map)\r\nwhere
  evaluating the outermost constructor will do “enough work”.\r\n\r\n\r\n## Be careful
  with lazy I/O!\r\n\r\nExperienced Haskell programmers don't use lazy I/O very often,
  and\r\nhere's an example of why: if you try to run the benchmark below, it\r\nwill
  probably *crash*.\r\n\r\n```haskell\r\nimport Criterion.Main\r\n\r\nmain = defaultMain
  [\r\n    bench \"whnfIO readFile\" $ whnfIO (readFile \"BadReadFile.hs\")\r\n  ]\r\n```\r\n([examples/BadReadFile.hs](https://github.com/haskell/criterion/blob/master/examples/BadReadFile.hs))\r\n\r\nThe
  reason for the crash is that `readFile` reads the contents of a\r\nfile lazily:
  it can't close the file handle until whoever opened the\r\nfile reads the whole
  thing.  Since `whnfIO` only evaluates the very\r\nfirst constructor after the file
  is opened, the benchmarking loop\r\ncauses a large number of open files to accumulate,
  until the\r\ninevitable occurs:\r\n\r\n```shellsession\r\n$ ./BadReadFile\r\nbenchmarking
  whnfIO readFile\r\nopenFile: resource exhausted (Too many open files)\r\n```\r\n\r\n\r\n##
  Beware “pretend” I/O!\r\n\r\nGHC is an aggressive compiler.  If you have an `IO`
  action that\r\ndoesn't really interact with the outside world, *and* it has just
  the\r\nright structure, GHC may notice that a substantial amount of its\r\ncomputation
  can be memoised via “let-floating”.\r\n\r\nThere exists a\r\n[somewhat contrived
  example](https://github.com/haskell/criterion/blob/master/examples/ConduitVsPipes.hs)\r\nof
  this problem, where the first two benchmarks run between 40 and\r\n40,000 times
  faster than they “should”.\r\n\r\nAs always, if you see numbers that look wildly
  out of whack, you\r\nshouldn't rejoice that you have magically achieved fast\r\nperformance—be
  skeptical and investigate!\r\n\r\n\r\n> [!TIP]\r\n> **Defeating let-floating**\r\n>
  \r\n> Fortunately for this particular misbehaving benchmark suite, GHC has\r\n>
  an option named\r\n> [`-fno-full-laziness`](https://downloads.haskell.org/ghc/latest/docs/users_guide/using-optimisation.html#ghc-flag-ffull-laziness)\r\n>
  that will turn off let-floating and restore the first two benchmarks\r\n> to performing
  in line with the second two.\r\n>\r\n> You should not react by simply throwing `-fno-full-laziness`
  into\r\n> every GHC-and-criterion command line, as let-floating helps with\r\n>
  performance more often than it hurts with benchmarking.\r\n\r\n\r\n## Benchmarking
  pure functions\r\n\r\nLazy evaluation makes it tricky to benchmark pure code. If
  we tried to\r\nsaturate a function with all of its arguments and evaluate it\r\nrepeatedly,
  laziness would ensure that we'd only do “real work” the\r\nfirst time through our
  benchmarking loop.  The expression would be\r\noverwritten with that result, and
  no further work would happen on\r\nsubsequent loops through our benchmarking harness.\r\n\r\nWe
  can defeat laziness by benchmarking an *unsaturated* function—one\r\nthat has been
  given *all but one* of its arguments.\r\n\r\nThis is why the\r\n[`nf`](http://hackage.haskell.org/package/criterion/docs/Criterion-Main.html#v:nf)\r\nfunction
  accepts two arguments: the first is the almost-saturated\r\nfunction we want to
  benchmark, and the second is the final argument to\r\ngive it.\r\n\r\n```haskell\r\nnf
  :: NFData b => (a -> b) -> a -> Benchmarkable\r\n```\r\n\r\nAs the\r\n[`NFData`](http://hackage.haskell.org/package/deepseq/docs/Control-DeepSeq.html#t:NFData)\r\nconstraint
  suggests, `nf` applies the argument to the function, then\r\nevaluates the result
  to <a href=\"#normal-form\">normal form</a>.\r\n\r\nThe\r\n[`whnf`](http://hackage.haskell.org/package/criterion/docs/Criterion-Main.html#v:whnf)\r\nfunction
  evaluates the result of a function only to <a\r\nhref=\"#weak-head-normal-form\">weak
  head normal form</a> (WHNF).\r\n\r\n```haskell\r\nwhnf :: (a -> b) -> a -> Benchmarkable\r\n```\r\n\r\nIf
  we go back to our first example, we can now fully understand what's\r\ngoing on.\r\n\r\n```haskell\r\nmain
  = defaultMain [\r\n  bgroup \"fib\" [ bench \"1\"  $ whnf fib 1\r\n               ,
  bench \"5\"  $ whnf fib 5\r\n               , bench \"9\"  $ whnf fib 9\r\n               ,
  bench \"11\" $ whnf fib 11\r\n               ]\r\n  ]\r\n```\r\n([examples/Fibber.hs](https://github.com/haskell/criterion/blob/master/examples/Fibber.hs))\r\n\r\nWe
  can get away with using `whnf` here because we know that an\r\n`Int` has only one
  constructor, so there's no deeper buried\r\nstructure that we'd have to reach using
  `nf`.\r\n\r\nAs with benchmarking `IO` actions, there's no clear-cut case for when\r\nto
  use `whfn` versus `nf`, especially when a result may be lazily\r\ngenerated.\r\n\r\nGuidelines
  for thinking about when to use `nf` or `whnf`:\r\n\r\n* If a result is a lazy structure
  (or a mix of strict and lazy, such\r\n  as a balanced tree with lazy leaves), how
  much of it would a\r\n  real-world caller use?  You should be trying to evaluate
  as much of\r\n  the result as a realistic consumer would.  Blindly using `nf` could\r\n
  \ cause way too much unnecessary computation.\r\n\r\n* If a result is something
  simple like an `Int`, you're probably safe\r\n  using `whnf`—but then again, there
  should be no additional cost to\r\n  using `nf` in these cases.\r\n\r\n\r\n## Using
  the criterion command line\r\n\r\nBy default, a criterion benchmark suite simply
  runs all of its\r\nbenchmarks.  However, criterion accepts a number of arguments
  to\r\ncontrol its behaviour.  Run your program with `--help` for a complete\r\nlist.\r\n\r\n\r\n###
  Specifying benchmarks to run\r\n\r\nThe most common thing you'll want to do is specify
  which benchmarks\r\nyou want to run.  You can do this by simply enumerating each\r\nbenchmark.\r\n\r\n```shellsession\r\n$
  ./Fibber 'fib/fib 1'\r\n```\r\n\r\nBy default, any names you specify are treated
  as prefixes to match, so\r\nyou can specify an entire group of benchmarks via a
  name like\r\n`\"fib/\"`.  Use the `--match` option to control this behaviour. There
  are\r\ncurrently four ways to configure `--match`:\r\n\r\n* `--match prefix`: Check
  if the given string is a prefix of a benchmark\r\n  path. For instance, `\"foo\"`
  will match `\"foobar\"`.\r\n\r\n* `--match glob`: Use the given string as a Unix-style
  glob pattern. Bear in\r\n  mind that performing a glob match on benchmarks names
  is done as if they were\r\n  file paths, so for instance both `\"*/ba*\"` and `\"*/*\"`
  will match `\"foo/bar\"`,\r\n  but neither `\"*\"` nor `\"*bar\"` will match `\"foo/bar\"`.\r\n\r\n*
  `--match pattern`: Check if the given string is a substring (not necessarily\r\n
  \ just a prefix) of a benchmark path. For instance `\"ooba\"` will match\r\n  `\"foobar\"`.\r\n\r\n*
  `--match ipattern`: Check if the given string is a substring (not necessarily\r\n
  \ just a prefix) of a benchmark path, but in a case-insensitive fashion. For\r\n
  \ instance, `\"oObA\"` will match `\"foobar\"`.\r\n\r\n### Listing benchmarks\r\n\r\nIf
  you've forgotten the names of your benchmarks, run your program\r\nwith `--list`
  and it will print them all.\r\n\r\n\r\n### How long to spend measuring data\r\n\r\nBy
  default, each benchmark runs for 5 seconds.\r\n\r\nYou can control this using the
  `--time-limit` option, which specifies\r\nthe minimum number of seconds (decimal
  fractions are acceptable) that\r\na benchmark will spend gathering data.  The actual
  amount of time\r\nspent may be longer, if more data is needed.\r\n\r\n\r\n### Writing
  out data\r\n\r\nCriterion provides several ways to save data.\r\n\r\nThe friendliest
  is as HTML, using `--output`.  Files written using\r\n`--output` are actually generated
  from Mustache-style templates.  The\r\nonly other template provided by default is
  `json`, so if you run with\r\n`--template json --output mydata.json`, you'll get
  a big JSON dump of\r\nyour data.\r\n\r\nYou can also write out a basic CSV file
  using `--csv`, a JSON file using\r\n`--json`, and a JUnit-compatible XML file using
  `--junit`.  (The contents\r\nof these files are likely to change in the not-too-distant
  future.)\r\n\r\n\r\n## Linear regression\r\n\r\nIf you want to perform linear regressions
  on metrics other than\r\nelapsed time, use the `--regress` option.  This can be
  tricky to use\r\nif you are not familiar with linear regression, but here's a thumbnail\r\nsketch.\r\n\r\nThe
  purpose of linear regression is to predict how much one variable\r\n(the *responder*)
  will change in response to a change in one or more\r\nothers (the *predictors*).\r\n\r\nOn
  each step through a benchmark loop, criterion changes the number of\r\niterations.
  \ This is the most obvious choice for a predictor\r\nvariable.  This variable is
  named `iters`.\r\n\r\nIf we want to regress CPU time (`cpuTime`) against iterations,
  we can\r\nuse `cpuTime:iters` as the argument to `--regress`.  This generates\r\nsome
  additional output on the command line:\r\n\r\n```\r\ntime                 31.31
  ms   (30.44 ms .. 32.22 ms)\r\n                     0.997 R²   (0.994 R² .. 0.999
  R²)\r\nmean                 30.56 ms   (30.01 ms .. 30.99 ms)\r\nstd dev              1.029
  ms   (754.3 μs .. 1.503 ms)\r\n\r\ncpuTime:             0.997 R²   (0.994 R² ..
  0.999 R²)\r\n  iters              3.129e-2   (3.039e-2 .. 3.221e-2)\r\n  y                  -4.698e-3
  \ (-1.194e-2 .. 1.329e-3)\r\n```\r\n\r\nAfter the block of normal data, we see a
  series of new rows.\r\n\r\nOn the first line of the new block is an R² goodness-of-fit
  measure,\r\nso we can see how well our choice of regression fits the data.\r\n\r\nOn
  the second line, we get the slope of the `cpuTime`/`iters` curve,\r\nor (stated
  another way) how much `cpuTime` each iteration costs.\r\n\r\nThe last entry is the
  $y$-axis intercept.\r\n\r\n\r\n### Measuring garbage collector statistics\r\n\r\nBy
  default, GHC does not collect statistics about the operation of its\r\ngarbage collector.
  \ If you want to measure and regress against GC\r\nstatistics, you must explicitly
  enable statistics collection at\r\nruntime using `+RTS -T`.\r\n\r\n\r\n### Useful
  regressions\r\n\r\n| regression                     | `--regress`        | notes\r\n|
  -------------------------------|------------------- |-----------\r\n| CPU cycles
  \                    | `cycles:iters`     |\r\n| Bytes allocated                |
  `allocated:iters`  | `+RTS -T`\r\n| Number of garbage collections  | `numGcs:iters`
  \    | `+RTS -T`\r\n| CPU frequency                  | `cycles:time`      |\r\n\r\n\r\n##
  Tips, tricks, and pitfalls\r\n\r\nWhile criterion tries hard to automate as much
  of the benchmarking\r\nprocess as possible, there are some things you will want
  to pay\r\nattention to.\r\n\r\n* Measurements are only as good as the environment
  in which they're\r\n  gathered.  Try to make sure your computer is quiet when measuring\r\n
  \ data.\r\n\r\n* Be judicious in when you choose `nf` and `whnf`.  Always think
  about\r\n  what the result of a function is, and how much of it you want to\r\n
  \ evaluate.\r\n\r\n* Simply rerunning a benchmark can lead to variations of a few
  percent\r\n  in numbers.  This variation can have many causes, including address\r\n
  \ space layout randomization, recompilation between runs, cache\r\n  effects, CPU
  thermal throttling, and the phase of the moon.  Don't\r\n  treat your first measurement
  as golden!\r\n\r\n* Keep an eye out for completely bogus numbers, as in the case
  of\r\n  `-fno-full-laziness` above.\r\n\r\n* When you need trustworthy results from
  a benchmark suite, run each\r\n  measurement as a separate invocation of your program.
  \ When you run\r\n  a number of benchmarks during a single program invocation, you
  will\r\n  sometimes see them interfere with each other.\r\n\r\n\r\n### How to sniff
  out bogus results\r\n\r\nIf some external factors are making your measurements noisy,
  criterion\r\ntries to make it easy to tell.  At the level of raw data, noisy\r\nmeasurements
  will show up as “outliers”, but you shouldn't need to\r\ninspect the raw data directly.\r\n\r\nThe
  easiest yellow flag to spot is the R² goodness-of-fit measure\r\ndropping below
  0.9.  If this happens, scrutinise your data carefully.\r\n\r\nAnother easy pattern
  to look for is severe outliers in the raw\r\nmeasurement chart when you're using
  `--output`.  These should be easy\r\nto spot: they'll be points sitting far from
  the linear regression line\r\n(usually above it).\r\n\r\nIf the lower and upper
  bounds on an estimate aren't “tight” (close to\r\nthe estimate), this suggests that
  noise might be having some kind of\r\nnegative effect.\r\n\r\nA warning about “variance
  introduced by outliers” may be printed.\r\nThis indicates the degree to which the
  standard deviation is inflated\r\nby outlying measurements, as in the following
  snippet (notice that the\r\nlower and upper bounds aren't all that tight, too).\r\n\r\n```\r\nstd
  dev              652.0 ps   (507.7 ps .. 942.1 ps)\r\nvariance introduced by outliers:
  91% (severely inflated)\r\n```\r\n\r\n## Generating (HTML) reports from previous
  benchmarks with criterion-report\r\n\r\nIf you want to post-process benchmark data
  before generating a HTML report you\r\ncan use the `criterion-report` executable
  to generate HTML reports from\r\ncriterion generated JSON. To store the benchmark
  results run criterion with the\r\n`--json` flag to specify where to store the results.
  You can then use:\r\n`criterion-report data.json report.html` to generate a HTML
  report of the data.\r\n`criterion-report` also accepts the `--template` flag accepted
  by criterion.\r\n"
description-type: markdown
hash: e17b280bf4e461a4a14bc1c0a8fa3df60c8e2ca313cb3f2f12d4475d19d10120
homepage: https://github.com/haskell/criterion
latest: 1.6.4.1
license-name: BSD-3-Clause
maintainer: Ryan Scott <ryan.gl.scott@gmail.com>
synopsis: Robust, reliable performance measurement and analysis
test-bench-deps:
  HUnit: '>=0'
  QuickCheck: '>=2.4'
  aeson: '>=0'
  base: '>=0'
  base-compat: '>=0'
  base-compat-batteries: '>=0'
  bytestring: '>=0'
  criterion: '>=0'
  deepseq: '>=0'
  directory: '>=0'
  statistics: '>=0'
  tasty: '>=0'
  tasty-hunit: '>=0'
  tasty-quickcheck: '>=0'
  vector: '>=0'
