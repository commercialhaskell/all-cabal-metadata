all-versions:
- 0.1.0.0
- 0.1.0.1
- 0.1.0.2
- 0.1.0.3
- 0.1.1.3
- 0.1.2.0
- 0.1.3.0
- 0.2.0.0
- 0.2.1.0
author: tushar
basic-deps:
  aeson: '>=2 && <3'
  base: '>=4.7 && <5'
  base64-bytestring: '>=1 && <2'
  bytestring: '>=0.10 && <0.13'
  containers: '>=0.6 && <0.9'
  directory: '>=1 && <1.4'
  filepath: '>=1 && <1.6'
  http-client: '>=0.6 && <0.8'
  http-client-tls: '>=0.2 && <0.4'
  http-types: '>=0.7 && <0.13'
  mtl: '>=2 && <3'
  stm: '>=2 && <3'
  text: '>=1 && <3'
  time: '>=1 && <2'
changelog: "# Revision history for ollama-haskell\n\n## Unreleased\n\n## 0.2.1.0 --
  2025-09-24\n\n* Added blob module for managing binary large objects (GGUF files/safetensors).\n*
  Breaking: Added `dimensions` option in embeddings for specifying embedding dimensions.\n*
  Breaking: Added `doneReason` field in chat and generate responses.\n* Improved test
  suite with more comprehensive test cases.\n* Added `onComplete` callback function
  in stream handlers for better streaming control.\n* Breaking: Standardized field
  names across the library for consistency.\n* Applied fourmolu code formatting and
  added hlint for better code quality.\n\n## 0.2.0.0 -- 2025-06-05\n\n* Added stack
  matrix to ensure lib is buildable from lts-19.33\n* Made parameters & template fields
  optional in `ShowModelResponse`.\n* Added extra parameters fields in `ModelInfo`.\n*
  Added strict annotations for all fields.\n* Fixed ToJSON instance for delete model
  request body.\n* Removed duplicate code by using unified `withOllamaRequest` function
  for all API calls.\n* Added unified config type `OllamaConfig` to hold common configuration
  options.\n* Added validation for generate and chat functions to ensure required
  fields are present.\n* Added convience functions for generating Message and ToolCall
  types.\n* Added thinking field for chat and generate function.\n* Added ModelOptions
  type to encapsulate model options.\n* Added get ollama version function.\n* Added
  Common Manager, Callback functions and retry option in OllamaConfig.\n* Fixed tool_calls.\n*
  Added MonadIO versions of api functions.\n* Added more comprehensive error handling
  for API calls.\n* Added more comprehensive test cases for all functions.\n* Added
  schema builder for passing json format for structured output.\n\n## 0.1.3.0 -- 2025-03-25\n\n*
  Added options, tools and tool_calls fields in chat and generate.\n* Exported EmbeddingResponse.\n*
  Added Format argument in chat and generate function for structured output.\n\n##
  0.1.2.0 -- 2024-11-20\n\n* Added hostUrl and responseTimeOut options in generate
  function.\n* Added hostUrl and responseTimeOut options in chat function.\n\n## 0.1.1.3
  -- 2024-11-08\n\n* Increase response timeout to 15 minutes\n* Added encodeImage
  utility function that converts image filePath to base64 image data.\n* Added generateJson
  and chatJson. High level function to return response in Haskell type.\n\n## 0.1.0.3
  -- 2024-11-05\n\n* Moving to stack instead of cabal.\n\n## 0.1.0.2 -- 2024-10-18\n\n*
  Increased response timeout time for chat function. \n\n## 0.1.0.1 -- 2024-10-18\n\n*
  Renaming Lib.hs to OllamaExamples.hs as it was conflicting `Lib.hs` name\n\n## 0.1.0.0
  -- YYYY-mm-dd\n\n* First version. Released on an unsuspecting world.\n"
changelog-type: markdown
description: "# \U0001F999 Ollama Haskell\n\n<div style=\"text-align: center;\">\n<img
  src=\"./examples/ollama_haskell.png\" alt=\"logo image\" height=\"200\"/>\n</div>\n\n**`ollama-haskell`**
  is an unofficial Haskell client for [Ollama](https://ollama.com), inspired by [`ollama-python`](https://github.com/ollama/ollama-python).
  It enables interaction with locally running LLMs through the Ollama HTTP API — directly
  from Haskell.\n\n---\n\n## ✨ Features\n\n* \U0001F4AC Chat with models\n* ✍️ Text
  generation (with streaming)\n* ✅ Chat with structured messages and tools\n* \U0001F9E0
  Embeddings\n* \U0001F9F0 Model management (list, pull, push, show, delete)\n* \U0001F5C3️
  In-memory conversation history\n* ⚙️ Configurable timeouts, retries, streaming handlers\n\n---\n\n##
  ⚡ Quick Example\n\n```haskell\n{-# LANGUAGE OverloadedStrings #-}\nmodule Main where\n\nimport
  Data.Ollama.Generate\nimport qualified Data.Text.IO as T\n\nmain :: IO ()\nmain
  = do\n  let ops =\n        defaultGenerateOps\n          { modelName = \"gemma3\"\n
  \         , prompt = \"What is the meaning of life?\"\n          }\n  eRes <- generate
  ops Nothing\n  case eRes of\n    Left err -> putStrLn $ \"Something went wrong:
  \" ++ show err\n    Right r -> do\n      putStr \"LLM response: \"\n      T.putStrLn
  (genResponse r)\n```\n\n---\n\n## \U0001F4E6 Installation\n\nAdd to your `.cabal`
  file:\n\n```cabal\nbuild-depends:\n  base >=4.7 && <5,\n  ollama-haskell\n```\n\nOr
  use with `stack`/`nix-shell`.\n\n---\n\n## \U0001F4DA More Examples\n\nSee [`examples/OllamaExamples.hs`](examples/OllamaExamples.hs)
  for:\n\n* Chat with conversation memory\n* Structured JSON output\n* Embeddings\n*
  Tool/function calling\n* Multimodal input\n* Streaming and non-streaming variants\n\n---\n\n##
  \U0001F6E0 Prerequisite\n\nMake sure you have [Ollama installed and running locally](https://ollama.com/download).
  Run `ollama pull llama3` to download a model.\n\n---\n\n## \U0001F9EA Dev & Nix
  Support\n\nUse Nix:\n\n```bash\nnix-shell\n```\n\nThis will install `stack` and
  Ollama.\n\n---\n\n## \U0001F468‍\U0001F4BB Author\n\nCreated and maintained by [@tusharad](https://github.com/tusharad).
  PRs and feedback are welcome!\n\n---\n\n## \U0001F91D Contributing\n\nHave ideas
  or improvements? Feel free to [open an issue](https://github.com/tusharad/ollama-haskell/issues)
  or submit a PR!\n"
description-type: markdown
hash: f2648417acd999d194d54481017d69a9807157fc01882b9ec75a76f03b21d1d5
homepage: https://github.com/tusharad/ollama-haskell#readme
latest: 0.2.1.0
license-name: MIT
maintainer: tusharadhatrao@gmail.com
synopsis: Haskell client for ollama.
test-bench-deps:
  aeson: '>=0'
  base: '>=4.7 && <5'
  base64-bytestring: '>=1 && <2'
  bytestring: '>=0.10 && <0.13'
  containers: '>=0.6 && <0.9'
  directory: '>=1 && <1.4'
  filepath: '>=1 && <1.6'
  http-client: '>=0.6 && <0.8'
  http-client-tls: '>=0.2 && <0.4'
  http-types: '>=0.7 && <0.13'
  mtl: '>=2 && <3'
  ollama-haskell: '>=0'
  scientific: '>=0'
  silently: '>=0'
  stm: '>=2 && <3'
  tasty: '>=1.5'
  tasty-hunit: '>=0'
  text: '>=0'
  time: '>=1 && <2'
