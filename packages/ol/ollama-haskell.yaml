all-versions:
- 0.1.0.0
- 0.1.0.1
- 0.1.0.2
- 0.1.0.3
author: tushar
basic-deps:
  aeson: '>=0'
  base: '>=4.7 && <5'
  bytestring: '>=0'
  http-client: '>=0'
  http-types: '>=0'
  text: '>=0'
  time: '>=0'
changelog: "# Revision history for ollama-haskell\n\n## 0.1.0.3 -- 2024-11-05\n\n*
  Moving to stack instead of cabal.\n\n## 0.1.0.2 -- 2024-10-18\n\n* Increased response
  timeout time for chat function. \n\n## 0.1.0.1 -- 2024-10-18\n\n* Renaming Lib.hs
  to OllamaExamples.hs as it was conflicting `Lib.hs` name\n\n## 0.1.0.0 -- YYYY-mm-dd\n\n*
  First version. Released on an unsuspecting world.\n"
changelog-type: markdown
description: "# Ollama-haskell\n\n**ollama-haskell** is an unofficial Haskell binding
  for [Ollama](https://ollama.com), similar to [`ollama-python`](https://github.com/ollama/ollama-python).
  \n\nThis library allows you to interact with Ollama, a tool that lets you run large
  language models (LLMs) locally, from within your Haskell projects. \n\n## Examples\n\n```haskell\n{-#
  LANGUAGE OverloadedStrings #-}\nmodule Lib where\n\nimport Ollama (GenerateOps(..),
  defaultGenerateOps, generate)\n\nmain :: IO ()\nmain = do\n    void $\n      generate\n
  \       defaultGenerateOps\n          { modelName = \"llama3.2\"\n          , prompt
  = \"what is functional programming?\"\n          , stream = Just (T.putStr . Ollama.response_,
  pure ())\n          }\n```\n\n### Output\n\n```bash\nghci> import Lib\nghci> main\n\nWhether
  Haskell is a \"good\" language depends on what you're looking for in a programming
  language and your personal preferences. Here are some points to consider:\n\n**Pros:**\n\n1.
  **Strongly typed**: Haskell's type system ensures that you catch errors early, which
  leads to fewer bugs and easier maintenance.\n2. **Functional programming paradigm**:
  Haskell encourages declarative coding, making it easier to reason about code and
  write correct programs.\n3. **Garbage collection**: Haskell handles memory management
  automatically, freeing you from worries about manual memory deallocation.\n```\n\nYou
  can find practical examples demonstrating how to use the library in the `src/OllamaExamples.hs`
  file. \n\n## Prerequisite\n\nMake sure you have [Ollama](https://ollama.com) installed
  and running on your local machine. You can download it from [here](https://ollama.com/download).\n\n##
  How to Use It\n\n1. Include the `ollama-haskell` package in your `.cabal` file:\n
  \  ```cabal\n   build-depends:\n       base >= 4.7 && < 5,\n       ollama-haskell\n
  \  ```\n\n3. Import the `Ollama` module and start integrating with your local LLM.\n\n##
  Future Updates\n\n- [x] Improve documentation\n- [x] Add tests.\n- [x] Add examples.\n-
  [x] Add CI/CD pipeline.\n- [ ] `options` parameter in `generate`.\n\nStay tuned
  for future updates and improvements!\n\n## Author\n\nThis library is developed and
  maintained by [Tushar](https://github.com/tusharad). Feel free to reach out for
  any questions or suggestions!\n\n## Contributions\n\nContributions are welcome!
  If you'd like to improve the library, please submit a pull request or open an issue.
  Whether it's fixing bugs, adding new features, or improving documentation, all contributions
  are greatly appreciated.\n"
description-type: markdown
hash: e29d09a7c5fc02ceeaef3dfbd2597bb92c9cd58f8be168ae271755935dbbba19
homepage: https://github.com/tusharad/ollama-haskell#readme
latest: 0.1.0.3
license-name: MIT
maintainer: tusharadhatrao@gmail.com
synopsis: Haskell bindings for ollama.
test-bench-deps:
  aeson: '>=0'
  base: '>=4.7 && <5'
  bytestring: '>=0'
  http-client: '>=0'
  http-types: '>=0'
  ollama-haskell: '>=0'
  silently: '>=0'
  tasty: '>=0'
  tasty-hunit: '>=0'
  text: '>=0'
  time: '>=0'
