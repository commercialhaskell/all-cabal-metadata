all-versions:
- 0.1.0.0
- 0.1.1.0
- 0.1.2.0
- 0.1.3.0
- 0.1.4.0
- 0.1.5.0
author: Matthias Pall Gissurarson <mpg@mpg.is>
basic-deps:
  aeson: ^>=2.2
  base: '>=4.18 && <4.22'
  containers: '>=0.6 && <0.8'
  exceptions: ^>=0.10
  ghc: '>=9.6 && <9.14'
  modern-uri: ^>=0.3
  ollama-haskell: ^>=0.1
  req: ^>=3.13
  text: ^>=2.1
changelog: |
  # Revision history for OllamaHoles

  ## 0.1.5.0 -- 2024-05-01

  * Improve the list of things in scope
  * Let user add guidance to the LLM
  * Include docs of things in scope with the include-docs flag

  ## 0.1.4.0 -- 2024-04-30

  * Support GHC 9.8, 9.10 and 9.12

  ## 0.1.3.0 -- 2024-04-30

  * More robust handling of fits that cause errors during validation

  ## 0.1.2.0 -- 2024-04-30

  * Add validation of valid-hole fits returned by the LLM

  ## 0.1.1.0 -- 2024-04-29

  * Support calling remote LLMs

  ## 0.1.0.0 -- 2024-04-25

  * First version. Released on an unsuspecting world.
changelog-type: markdown
description: "# Ollama Holes\n\n![image](https://github.com/user-attachments/assets/649ffcd2-0560-47d6-bbbe-74bae08cbb70)\n\n##
  Introduction\nThis is an example of a typed-hole plugin for GHC that uses the [Ollama](https://ollama.com/)
  to host a local LLM to fill in holes in Haskell code.\n\nBefore using this plugin,
  make sure you have the Ollama CLI installed and the model you want to use is available.\nYou
  can install the Ollama CLI by following the instructions at [https://ollama.com/download](https://ollama.com/download),\nand
  you can install the default model (gemma3:27b) by running `ollama pull gemma3:27b`.\n\nNote
  that the speed and quality of the hole-fits generated by the plugin depends on\nthe
  model you use, and the default model requires a GPU to run efficiently.\nFor a smaller
  model, we suggest `gemma3:4b-it-qat`, or `deepcoder:1.5b`.\n\nThis plugin is also
  availble on Hackage [https://hackage.haskell.org/package/ollama-holes-plugin](https://hackage.haskell.org/package/ollama-holes-plugin)\n\n##
  Example\nGiven\n\n```haskell\n{-# OPTIONS_GHC -fplugin=GHC.Plugin.OllamaHoles #-}\n{-#
  OPTIONS_GHC -fplugin-opt=GHC.Plugin.OllamaHoles:model=gemma3:27b #-}\n{-# OPTIONS_GHC
  -fplugin-opt=GHC.Plugin.OllamaHoles:n=5 #-}\n\nmodule Main where\n\nimport Data.List\n\n\nmain
  :: IO ()\nmain = do let k = (_b :: [Int] -> [String])\n          print (k [1,2,3])\n\n```\n\nWe
  get the following output:\n\n\n```text\nMain.hs:12:20: error: [GHC-88464]\n    •
  Found hole: _b :: [Int] -> [String]\n      Or perhaps ‘_b’ is mis-spelled, or not
  in scope\n    • In the expression: _b :: [Int] -> [String]\n      In an equation
  for ‘k’: k = (_b :: [Int] -> [String])\n      In the expression:\n        do let
  k = (_b :: [Int] -> [String])\n           print (k [1, 2, ....])\n    • Relevant
  bindings include\n        k :: [Int] -> [String] (bound at Main.hs:12:15)\n        main
  :: IO () (bound at Main.hs:12:1)\n      Valid hole fits include\n        map show\n
  \       Prelude.map show\n        (\\xs -> map show xs)\n        (\\xs -> [show
  x | x <- xs])\n        (\\xs -> concatMap (return . show) xs)\n   |\n12 | main =
  do let k = (_b :: [Int] -> [String])\n   |                    ^^\n```\n\n\n## Guidance
  \n\nWe can also provide some guidance to the LLM, by having an identifier in scope
  called `_guide`,\ndefined as `_guide = Proxy :: Proxy (Text \"<guidance\")`.\n\nNote
  that this requires `GHC.TypeError` and `Data.Proxy`, with `DataKinds` enabled\n\nGiven\n\n```haskell\n{-#
  LANGUAGE DataKinds #-}\n{-# OPTIONS_GHC -fplugin=GHC.Plugin.OllamaHoles #-}\n{-#
  OPTIONS_GHC -fplugin-opt=GHC.Plugin.OllamaHoles:model=gemma3:27b #-}\n{-# OPTIONS_GHC
  -fplugin-opt=GHC.Plugin.OllamaHoles:n=5 #-}\n\nmodule Main where\n\nimport qualified
  Data.List as L\n\nimport GHC.TypeError\nimport Data.Proxy\n\nmain :: IO ()\nmain
  = do let _guide = Proxy :: Proxy (Text \"The function should take the list, sort
  it, and then print each integer.\")\n          let k = (_b :: [Int] -> [String])\n
  \         print (k [1,2,3])\n    \n```\n\nWe get:\n\n```text\nMain.hs:16:20: error:
  [GHC-88464]\n    • Found hole: _b :: [Int] -> [String]\n      Or perhaps ‘_b’ is
  mis-spelled, or not in scope\n    • In the expression: _b :: [Int] -> [String]\n
  \     In an equation for ‘k’: k = (_b :: [Int] -> [String])\n      In the expression:\n
  \       do let _guide = ...\n           let k = (_b :: [Int] -> [String])\n           print
  (k [1, 2, ....])\n    • Relevant bindings include\n        k :: [Int] -> [String]
  (bound at Main.hs:16:15)\n        _guide :: Proxy\n                    (Text\n                       \"The
  function should take the list, sort it, and then print each integer.\")\n          (bound
  at Main.hs:15:15)\n        main :: IO () (bound at Main.hs:15:1)\n   |\n16 |           let
  k = (_b :: [Int] -> [String])\n   |                    ^^\n\n```\n\n## Including
  Documentation\nYou can also pass the `-fplugin-opt=GHC.Plugin.OllamaHoles:include-docs`,
  flag,\nwhich will lookup the Haddock documentation (if available) for the functions
  in scope\nand provide it to the LLM. E.g. if `Data.List` is imported as `L`, the
  request to the\n`LLM` will include\n\n```text\n...\nDocumentation for `L.subsequences`:\n
  The 'subsequences' function returns the list of all subsequences of the argument.\nDocumentation
  for `L.tails`:\n \\(\\mathcal{O}(n)\\). The 'tails' function returns all final segments
  of the\n argument, longest first.\nDocumentation for `L.transpose`:\n The 'transpose'
  function transposes the rows and columns of its argument.\n...\n```\n\n## Installation\n\n1.
  Install [Ollama](https://ollama.com/download)\n2. Install the `gemma3:27b` model
  (or any other model you prefer) using the following command:\n\n```bash\n\nollama
  pull gemma3:27b\n```\n3. Clone this repository and navigate to the directory, and
  build the project using:\n\n```bash\ncabal build\n```\n4. Run the example using:\n\n```bash\ncabal
  build Test\n```\n\n5. Enjoy! If you want to change the underlying model, make sure
  to pass the model name via the plugin arguments (see example)\n\n## OpenAI and Gemini
  backends\n\nThe plugin now supports using the OpenAI API and Gemini APIs to generate
  valid hole fits.\nSimply set the backend flag `-fplugin-opt=GHC.Plugin.OllamaHoles:backend=openai`,\nor
  `-fplugin-opt=GHC.Plugin.OllamaHoles:backend=gemini`, and make sure that\nyou have
  the `OPENAI_API_KEY` or `GEMINI_API_KEY` set in your environment.\n\nTo use with
  any other OpenAI compatible api (like groq or OpenRouter), simply set\n`-fplugin-opt=GHC.Plugin.OllamaHoles:backend=openai`,\nand\n`-fplugin-opt=GHC.Plugin.OllamaHoles:openai_base_url=https://api.groq.com/openai`,\n`-fplugin-opt=GHC.Plugin.OllamaHoles:openai_key_name=GROQ_API_KEY`,\n"
description-type: markdown
hash: ea298fe5b731ba42f128e1b1f21a0d04b5c6ed7663dca9df269849e8d0c765f1
homepage: https://github.com/Tritlo/OllamaHoles
latest: 0.1.5.0
license-name: MIT
maintainer: Matthias Pall Gissurarson <mpg@mpg.is>
synopsis: A typed-hole plugin that uses LLMs to generate valid hole-fits
test-bench-deps: {}
