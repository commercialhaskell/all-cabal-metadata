all-versions:
- 1.0.0
- 1.0.1
- 1.0.2
- 1.0.3
author: Gabriella Gonzalez
basic-deps:
  base: '>=4.15.0.0 && <5'
  base64: '>=1.0 && <1.1'
  bytestring: '>=0.11.3.0'
  containers: '>=0.5.0.0'
  deepseq: '>=1.4.0.0'
  filepath: '>=0'
  megaparsec: <9.8
  pcre-light: '>=0.2'
  raw-strings-qq: '>=0'
  text: '>=0'
  unordered-containers: '>=0'
changelog: |
  1.0.3

  - [Fix source distribution](https://github.com/Gabriella439/tiktoken/commit/cc1024f1e00b852b11cf7a498b9b7b63c8c3884a)

    The `.tiktoken` files for each encoding were not being included correctly in
    the source distribution uploaded to Hackage.

  1.0.2

  - [Correctly handle gaps in ranks](https://github.com/Gabriella439/tiktoken/commit/cc82cb192b1f22a2185257b3a1045e2aaf25a4e8)

    The old implementation assumed that encoding don't have gaps in their ranks,
    but some do (especially near the end, typically reserved for special tokens).
    This change fixes the internal implementation to correctly handle those gaps.

  - [Fix `o200k_base` regex to match upstream](https://github.com/Gabriella439/tiktoken/commit/fa70dddb431af5a71b243325cf9ef72061721e99)

    The upstream `tiktoken` package uses a flavor of regex that subtly differs
    from the Haskell `pcre-light` package.  Specifically, they differ in whether
    they treat ideographic space (U+3000) as whitespace (which this change fixes).

    There may be other differences yet to be uncovered, but this is the only one
    that has arisen so far when comparing to upstream on a large corpus of text.

  1.0.1

  - [Small fixes to documentation](https://github.com/Gabriella439/tiktoken/commit/bb0eaf724a8120e7cd2ada2ad09835369bac02bc)

  1.0.0

  - Initial release
changelog-type: markdown
description: |
  # `tiktoken`

  This is a Haskell implementation of
  [`tiktoken`](https://github.com/openai/tiktoken), but just the tokenization
  logic.  In other words, given an existing encoding (like `cl100k_base`) you
  can tokenize a string (into smaller strings or token ranks).

  This means that you can't (yet) use this package to create your own new
  encodings, but you can use it to consume encodings.  In particular, this comes
  in handy for prompt engineering where you want to use as much of the available
  prompt tokens as possible (which requires accurately counting tokens).

  Encoding speed is ≈2.6-3.1 MB/s on an M1 MacBook Pro (using only one core since
  this package does not yet support parallel tokenization):

  ```
  All
    Encode 10 MB of Wikipedia
      r50k_base:   OK (23.88s)
        3.356 s ± 151 ms
      p50k_base:   OK (10.39s)
        3.445 s ±  31 ms
      p50k_edit:   OK (11.13s)
        3.693 s ± 240 ms
      cl100k_base: OK (11.16s)
        3.685 s ± 143 ms
      o200k_base:  OK (11.01s)
        3.648 s ± 134 ms
  ```
description-type: markdown
hash: 76c911527029f9e45380cbde2952e6475bab50f72c50f252f2ba2d2cedc79d5f
homepage: ''
latest: 1.0.3
license-name: BSD-3-Clause
maintainer: GenuineGabriella@gmail.com
synopsis: Haskell implementation of tiktoken
test-bench-deps:
  base: '>=0'
  bytestring: '>=0'
  deepseq: '>=0'
  filepath: '>=0'
  quickcheck-instances: '>=0'
  tasty: '>=0'
  tasty-bench: <0.5
  tasty-quickcheck: '>=0'
  tasty-silver: <3.4
  text: '>=0'
  tiktoken: '>=0'
