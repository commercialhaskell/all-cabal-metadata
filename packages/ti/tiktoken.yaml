all-versions:
- 1.0.0
author: Gabriella Gonzalez
basic-deps:
  base: '>=4.18.0.0 && <5'
  base64: '>=1.0 && <1.1'
  bytestring: '>=0.11.3.0'
  containers: '>=0.5.0.0'
  deepseq: '>=1.4.0.0'
  filepath: '>=0'
  megaparsec: <9.7
  pcre-light: '>=0.2'
  raw-strings-qq: '>=0'
  text: '>=0'
  transformers: '>=0.2.0.0'
  unordered-containers: '>=0'
  vector: <0.14
changelog: |
  1.0.0

  - Initial release
changelog-type: markdown
description: |
  # `tiktoken`

  This is a Haskell implementation of
  [`tiktoken`](https://github.com/openai/tiktoken), but just the tokenization
  logic.  In other words, given an existing encoding (like `cl100k_base`) you
  can tokenize a string (into smaller strings or token ranks).

  This means that you can't (yet) use this package to create your own new
  encodings, but you can use it to consume encodings.  In particular, this comes
  in handy for prompt engineering where you want to use as much of the available
  prompt tokens as possible (which requires accurately counting tokens).

  Encoding speed is ≈2.6-3.1 MB/s on an M1 MacBook Pro (using only one core since
  this package does not yet support parallel tokenization):

  ```
  All
    Encode 10 MB of Wikipedia
      r50k_base:   OK (23.88s)
        3.356 s ± 151 ms
      p50k_base:   OK (10.39s)
        3.445 s ±  31 ms
      p50k_edit:   OK (11.13s)
        3.693 s ± 240 ms
      cl100k_base: OK (11.16s)
        3.685 s ± 143 ms
      o200k_base:  OK (11.01s)
        3.648 s ± 134 ms
  ```
description-type: markdown
hash: bd3aebc5d8df2633f07a01726a08bf39cc252d48c75f8469c446ad444696b819
homepage: ''
latest: 1.0.0
license-name: BSD-3-Clause
maintainer: GenuineGabriella@gmail.com
synopsis: Haskell implementation of tiktoken
test-bench-deps:
  base: '>=0'
  bytestring: '>=0'
  deepseq: '>=0'
  filepath: '>=0'
  tasty: '>=0'
  tasty-bench: <0.4
  tasty-silver: <3.4
  text: '>=0'
  tiktoken: '>=0'
