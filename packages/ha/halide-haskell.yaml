all-versions:
- 0.0.1.0
- 0.0.2.0
author: Tom Westerhout
basic-deps:
  base: '>=4.16.0.0 && <5'
  bytestring: '>=0.11.1.0 && <0.12'
  constraints: '>=0.13.4 && <0.14'
  filepath: '>=1.4.2.1 && <2.0'
  halide-haskell: '>=0'
  inline-c: '>=0.9.1.6 && <0.10'
  inline-c-cpp: '>=0.5.0.0 && <0.6'
  primitive: '>=0.7.3.0 && <0.8'
  template-haskell: '>=2.18.0.0 && <3.0'
  temporary: '>=1.3 && <2.0'
  text: '>=1.2.5.0 && <3.0'
  unix: '>=2.7.2.2 && <3.0'
  vector: '>=0.12.3.0 && <0.13'
changelog: ''
changelog-type: ''
description: "<h1 align=\"center\">\nhalide-haskell\n</h1>\n\n<div align=\"center\">\n\n<img
  src=\"assets/construction_1f6a7.png\" width=\"32\">This project is still under heavy
  development and might not be production-ready<img src=\"assets/construction_1f6a7.png\"
  width=\"32\"><br>\nWe encourage experimenting with it and reporting any issues you
  run into via\n[Github issues](https://github.com/twesterhout/halide-haskell/issues).\n\n<br
  />\n\n[**Tutorials**](https://github.com/twesterhout/halide-haskell/tree/master/tutorials)
  | [**Documentation**](https://hackage.haskell.org/package/halide-haskell-0.0.1.0)
  | [**Showcases**](https://github.com/twesterhout/halide-haskell-examples)\n\n[![license](https://img.shields.io/github/license/twesterhout/halide-haskell.svg?style=flat-square)](LICENSE)\n[![build](https://img.shields.io/github/actions/workflow/status/twesterhout/halide-haskell/ci.yml?style=flat-square)](https://github.com/twesterhout/halide-haskell/actions/workflows/ci.yml)\n[![Hackage](https://img.shields.io/hackage/v/halide-haskell?style=flat-square)](https://hackage.haskell.org/package/halide-haskell)\n\n</div>\n\n<table>\n<tr>\n<td>\n\n[Halide](https://halide-lang.org/)
  is a programming language designed to make\nit easier to write high-performance
  image and array processing code on modern\nmachines. Rather than being a standalone
  programming language, Halide is\nembedded in C++. This means you write C++ code
  that builds an in-memory\nrepresentation of a Halide pipeline using Halide's C++
  API. You can then\ncompile this representation to an object file, or JIT-compile
  it and run it in\nthe same process.\n\n</td>\n</tr>\n</table>\n\n<h4 align=\"center\"
  >\nThis package provides Haskell bindings that allow to write Halide embedded in\nHaskell
  without C++ \U0001F60B.\n</h4>\n\n  - [Example usage](#-example-usage)\n  - [Installing](#-installing)\n
  \ - [Motivation](#-motivation)\n  - [Contributing](#-contributing)\n\n\n## \U0001F680
  Example usage\n\nAs a simple example, here's how you could implement array addition
  with halide-haskell:\n\n```haskell\n{-# LANGUAGE AllowAmbiguousTypes, DataKinds,
  OverloadedStrings, ViewPatterns #-}\nimport Language.Halide\n\n-- The algorithm\nmkArrayPlus
  = compile $ \\(buffer \"a\" -> a) (buffer \"b\" -> b) -> do\n  -- Create an index
  variable\n  i <- mkVar \"i\"\n  -- Define the resulting function. We call it \"out\".\n
  \ -- In pseudocode it's equivalent to the following: out[i] = a[i] + b[i]\n  out
  <- define \"out\" i $ a ! i + b ! i\n  -- Perform a fancy optimization and use SIMD:
  we split the loop over i into\n  -- an inner and an outer loop and then vectorize
  the inner loop\n  inner <- mkVar \"inner\"\n  split TailAuto i (i, inner) 4 out
  >>= vectorize inner\n\n-- Example usage of our Halide pipeline\nmain :: IO ()\nmain
  = do\n  let a, b :: [Float]\n      a = [1, 2, 3, 4, 5]\n      b = [6, 7, 8, 9, 10]\n
  \ -- Compile the code\n  arrayPlus <- mkArrayPlus\n  -- We tell Halide to treat
  our list as a one-dimensional buffer\n  withHalideBuffer @1 @Float a $ \\a' ->\n
  \   withHalideBuffer b $ \\b' ->\n      -- allocate a temporary buffer for the output\n
  \     allocaCpuBuffer [length a] $ \\out' -> do\n        -- execute the kernel --
  it is a normal function call!\n        arrayPlus a' b' out'\n        -- print the
  result\n        print =<< peekToList out'\n```\n\nFor more examples, have a look
  at the [tutorials](https://github.com/twesterhout/halide-haskell/tree/master/tutorials).\n\n##
  \U0001F4BB Installing\n\nCurrently, the easiest way to install the library is using\n[Nix](https://nixos.org/).
  It is not a fundamental limitation, because the\nlibrary itself is just a normal
  Cabal-based Haskell project, but installing &\npatching (not all our bug fixes have
  been upstreamed yet) the system\ndependencies is just too much work without Nix.\n\nSo,
  once you have Nix installed, you can add halide-haskell to your flake\ninputs like
  [this project\ndemonstrates](https://github.com/twesterhout/halide-haskell-examples/blob/main/flake.nix#L27)\nand
  then include it in your `build-depends` section in the Cabal file.\n\nIf you just
  want to try building the library, type\n\n```sh\nnix build\n```\n\nand to run an
  example, try\n\n```sh\nnix run\nnix run .#ghc927-intel-ocl.halide-haskell # for
  Intel OpenCL support\nnix run .#ghc927-cuda.halide-haskell      # for CUDA support\nnix
  run .#ghc944.halide-haskell           # to build with GHC 9.4.4 instead\n```\n\n(for
  OpenCL and CUDA, you may need to set `NIXPKGS_ALLOW_UNFREE=1`)\n\n## \U0001F929
  Motivation\n\nThe availability of Deep Learning frameworks such as\n[PyTorch](https://pytorch.org/)
  or [JAX](https://github.com/google/jax) has\nrevolutionized array processing, independently
  of whether one works on Machine\nLearning tasks or other numerical algorithms. The
  ecosystem in Haskell has been\ncatching up as well, and there are now multiple good
  array\nlibraries ([hmatrix](https://github.com/haskell-numerics/hmatrix),\n[massiv](https://github.com/lehins/massiv),\n[Accelerate](https://www.acceleratehs.org/),\n[arrayfire-haskell](https://github.com/arrayfire/arrayfire-haskell),\n[Hasktorch](https://github.com/hasktorch/hasktorch),
  are all high-quality\nlibraries). To accommodate multiple domains, such libraries\nhave
  to support hundreds if not thousands of operations (e.g. there are more\nthan 3.5
  thousand of so called [“native” functions in PyTorch](https://github.com/pytorch/pytorch/blob/6a09847c42bf7d33ba0aea5b083eebd846661ce1/aten/src/ATen/native/native_functions.yaml)),\nand
  this count does not include specializations for different device and data\ntypes).\n\nTo
  overcome this difficulty, we propose to build a common extension mechanism\nfor
  Haskell array libraries. The mechanism is based on embedding the\n[Halide](https://halide-lang.org/)
  language into Haskell that allows to\njust-in-time (JIT) compile computational kernels
  for various hardware.\n\n### \U0001F928 Why not Accelerate?\n\nOne might wonder,
  why write another package instead of relying on\n[Accelerate](https://www.acceleratehs.org/)
  for the JIT compilation of the\nkernels. Accelerate is a Haskell eDSL (embedded
  Domain Specific Language) for\ncollective operations on dense multi-dimensional
  arrays. It relies on\n[LLVM](https://llvm.org/) to JIT compile the computational
  kernels for the\ntarget architecture including multicore CPUs and GPUs. Users have
  to rely on\nAccelerate to generate high-performance kernels and have no way to force
  some\nlow-level optimizations. For example, [Trevor L. McDonell et\nal.](https://doi.org/10.1145/2887747.2804313)
  explain that the reason why\nhand-written [CUDA](https://www.nvidia.com/en-gb/geforce/technologies/cuda/)\nimplementation
  of the [N-body\nproblem](https://en.wikipedia.org/wiki/N-body_problem) outperforms
  Accelerate\nis the use of on-chip shared memory. Another example would be the matrix-matrix\nproduct
  where achieving maximal performance requires writing no fewer than six\nnested loops
  instead of the naive three ([ACM Trans. Math. Softw. 34, 3,\nArticle 12 (May 2008)](https://doi.org/10.1145/1356052.1356053)).\nAccelerate
  has no way of knowing that such optimizations have to be applied and\ncannot perform
  them automatically, and this is precisely the gap that we are\ntrying to fill by
  embedding Halide into Haskell.\n\nHalide is a C++ eDSL for high-performance image
  and array processing. Its core\nidea is to decouple the *algorithm* (i.e. what is
  computed) from the *schedule*\n(i.e. where and when it is computed). The eDSL allows
  to quickly prototype and\ntest the algorithm and then move on to the optimization.
  Optimizations such as\nfusion, tiling, parallelism and vectorization can be freely
  explored without\nthe risk of breaking the original algorithm definition. Schedulers
  can also be\ngenerated automatically by [advanced optimization\nalgorithms](https://halide-lang.org/papers/autoscheduler2019.html)\n\nHalide
  provides a lower level interface than Accelerate and thus does not aim\nto replace
  it. Instead, Halide can be used to extend Accelerate, and later on\none might even
  think about using Halide as a backend for Accelerate.\n\n## \U0001F528 Contributing\n\nCurrently,
  the best way to get started is to use Nix:\n\n```sh\nnix develop\n```\n\nThis will
  drop you into a shell with all the necessary tools to build the code\nsuch that
  you can do\n\n```sh\ncabal build\n```\n\nand\n\n```sh\ncabal test\n```\n"
description-type: markdown
hash: ee60378ac35e77713bf3018837d612a4666d8b8ebcef09cff172dc30ca1ad668
homepage: https://github.com/twesterhout/halide-haskell
latest: 0.0.2.0
license-name: BSD-3-Clause
maintainer: Tom Westerhout <14264576+twesterhout@users.noreply.github.com>
synopsis: Haskell bindings to Halide
test-bench-deps:
  HUnit: '>=1.6.2.0 && <1.7'
  QuickCheck: '>=2.14.2 && <3'
  base: '>=4.16.0.0 && <5'
  halide-haskell: '>=0'
  hspec: '>=2.9.7 && <3'
  inline-c: '>=0'
  inline-c-cpp: '>=0'
  text: '>=0'
  vector: '>=0'
