all-versions:
- 0.1.0
author: Huw Campbell <huw.campbell@gmail.com>
basic-deps:
  MonadRandom: '>=0.4 && <0.6'
  base: '>=4.8 && <5'
  bytestring: '>=0.10 && <0.11'
  cereal: '>=0.5 && <0.6'
  containers: '>=0.5 && <0.6'
  deepseq: '>=1.4 && <1.5'
  exceptions: '>=0.8 && <0.9'
  hmatrix: '>=0.18 && <0.19'
  mtl: '>=2.2.1 && <2.3'
  primitive: '>=0.6 && <0.7'
  singletons: '>=2.1 && <2.3'
  text: '>=1.2 && <1.3'
  vector: '>=0.11 && <0.13'
changelog: ''
changelog-type: ''
description: "Grenade\n=======\n\n[![Build Status](https://api.travis-ci.org/HuwCampbell/grenade.svg?branch=master)](https://travis-ci.org/HuwCampbell/grenade)\n\n```\nFirst
  shalt thou take out the Holy Pin, then shalt thou count to three, no more, no less.\nThree
  shall be the number thou shalt count, and the number of the counting shall be three.\nFour
  shalt thou not count, neither count thou two, excepting that thou then proceed to
  three.\nFive is right out.\n```\n\n\U0001F4A3 Machine learning which might blow
  up in your face \U0001F4A3\n\nGrenade is a composable, dependently typed, practical,
  and fast recurrent neural network library\nfor concise and precise specifications
  of complex networks in Haskell.\n\nAs an example, a network which can achieve ~1.5%
  error on MNIST can be\nspecified and initialised with random weights in a few lines
  of code with\n```haskell\ntype MNIST\n  = Network\n    '[ Convolution 1 10 5 5 1
  1, Pooling 2 2 2 2, Relu\n     , Convolution 10 16 5 5 1 1, Pooling 2 2 2 2, FlattenLayer,
  Relu\n     , FullyConnected 256 80, Logit, FullyConnected 80 10, Logit]\n    '[
  'D2 28 28, 'D3 24 24 10, 'D3 12 12 10, 'D3 12 12 10\n     , 'D3 8 8 16, 'D3 4 4
  16, 'D1 256, 'D1 256\n     , 'D1 80, 'D1 80, 'D1 10, 'D1 10]\n\nrandomMnist :: MonadRandom
  m => m MNIST\nrandomMnist = randomNetwork\n```\n\nAnd that's it. Because the types
  are so rich, there's no specific term level code\nrequired to construct this network;
  although it is of course possible and\neasy to construct and deconstruct the networks
  and layers explicitly oneself.\n\nIf recurrent neural networks are more your style,
  you can try defining something\n[\"unreasonably effective\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\nwith\n```haskell\ntype
  Shakespeare\n  = RecurrentNetwork\n    '[ R (LSTM 40 80), R (LSTM 80 40), F (FullyConnected
  40 40), F Logit]\n    '[ 'D1 40, 'D1 80, 'D1 40, 'D1 40, 'D1 40 ]\n```\n\nDesign\n------\n\nNetworks
  in Grenade can be thought of as a heterogeneous lists of layers, where\ntheir type
  includes not only the layers of the network, but also the shapes of\ndata that are
  passed between the layers.\n\nThe definition of a network is surprisingly simple:\n```haskell\ndata
  Network :: [*] -> [Shape] -> * where\n    NNil  :: SingI i\n          => Network
  '[] '[i]\n\n    (:~>) :: (SingI i, SingI h, Layer x i h)\n          => !x\n          ->
  !(Network xs (h ': hs))\n          -> Network (x ': xs) (i ': h ': hs)\n```\n\nThe
  `Layer x i o` constraint ensures that the layer `x` can sensibly perform a\ntransformation
  between the input and output shapes `i` and `o`.\n\nThe lifted data kind `Shape`
  defines our 1, 2, and 3 dimension types, used to\ndeclare what shape of data is
  passed between the layers.\n\nIn the MNIST example above, the input layer can be
  seen to be a two dimensional\n(`D2`), image with 28 by 28 pixels. When the first
  *Convolution* layer runs, it\noutputs a three dimensional (`D3`) 24x24x10 image.
  The last item in the list is\none dimensional (`D1`) with 10 values, representing
  the categories of the MNIST\ndata.\n\nUsage\n-----\n\nTo perform back propagation,
  one can call the eponymous function\n```haskell\nbackPropagate :: forall shapes
  layers.\n                 Network layers shapes -> S (Head shapes) -> S (Last shapes)
  -> Gradients layers\n```\nwhich takes a network, appropriate input and target data,
  and returns the\nback propagated gradients for the network. The shapes of the gradients
  are\nappropriate for each layer, and may be trivial for layers like `Rulu` which\nhave
  no learnable parameters.\n\nThe gradients however can always be applied, yielding
  a new (hopefully better)\nlayer with\n```haskell\napplyUpdate :: LearningParameters
  -> Network ls ss -> Gradients ls -> Network ls ss\n```\n\nLayers in Grenade are
  represented as Haskell classes, so creating one's own is\neasy in downstream code.
  If the shapes of a network are not specified correctly\nand a layer can not sensibly
  perform the operation between two shapes, then\nit will result in a compile time
  error.\n\nComposition\n-----------\n\nNetworks and Layers in Grenade are easily
  composed at the type level. As a `Network`\nis an instance of `Layer`, one can use
  a trained Network as a small component in a\nlarger network easily. Furthermore,
  we provide 2 layers which are designed to run\nlayers in parallel and merge their
  output (either by concatenating them across one\ndimension or summing by pointwise
  adding their activations). This allows one to\nwrite any Network which can be expressed
  as a\n[series parallel graph](https://en.wikipedia.org/wiki/Series-parallel_graph).\n\nA
  residual network layer specification for instance could be written as\n```haskell\ntype
  Residual net = Merge Trivial net\n```\nIf the type `net` is an instance of `Layer`,
  then `Residual net` will be too. It will\nrun the network, while retaining its input
  by passing it through the `Trivial` layer,\nand merge the original image with the
  output.\n\nSee the [MNIST](https://github.com/HuwCampbell/grenade/blob/master/examples/main/mnist.hs)\nexample,
  which has been overengineered to contain both residual style learning as well\nas
  inception style convolutions.\n\nGenerative Adversarial Networks\n-------------------------------\n\nAs
  Grenade is purely functional, one can compose its training functions in flexible\nways.
  [GAN-MNIST](https://github.com/HuwCampbell/grenade/blob/master/examples/main/gan-mnist.hs)\nexample
  displays an interesting, type safe way of writing a generative adversarial\ntraining
  function in 10 lines of code.\n\nLayer Zoo\n---------\n\nGrenade layers are normal
  haskell data types which are an instance of `Layer`, so\nit's easy to build one's
  own downstream code. We do however provide a decent set\nof layers, including convolution,
  deconvolution, pooling, pad, crop, logit, relu,\nelu, tanh, and fully connected.\n\nBuild
  Instructions\n------------------\nGrenade is most easily built with the [mafia](https://github.com/ambiata/mafia)\nscript
  that is located in the repository. You will also need the `lapack` and\n`blas` libraries
  and development tools. Once you have all that, Grenade can be\nbuild using:\n\n```\n./mafia
  build\n```\n\nand the tests run using:\n\n```\n./mafia test\n```\n\nGrenade builds
  with ghc 7.10 and 8.0.\n\nThanks\n------\nWriting a library like this has been on
  my mind for a while now, but a big shout\nout must go to [Justin Le](https://github.com/mstksg),
  whose\n[dependently typed fully connected network](https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html)\ninspired
  me to get cracking, gave many ideas for the type level tools I\nneeded, and was
  a great starting point for writing this library.\n\nPerformance\n-----------\nGrenade
  is backed by hmatrix, BLAS, and LAPACK, with critical functions optimised\nin C.
  Using the im2col trick popularised by Caffe, it should be sufficient for\nmany problems.\n\nBeing
  purely functional, it should also be easy to run batches in parallel, which\nwould
  be appropriate for larger networks, my current examples however are single\nthreaded.\n\nTraining
  15 generations over Kaggle's 41000 sample MNIST training set on a single\ncore took
  around 12 minutes, achieving 1.5% error rate on a 1000 sample holdout set.\n\nContributing\n------------\nContributions
  are welcome.\n"
description-type: markdown
hash: 14ffe6b8c4fe62f0a1ff5b3758bd8eb55daa1f906f67a5409dd035319b082cb6
homepage: ''
latest: 0.1.0
license-name: BSD-2-Clause
maintainer: Huw Campbell <huw.campbell@gmail.com>
synopsis: Practical Deep Learning in Haskell
test-bench-deps:
  MonadRandom: '>=0'
  ad: '>=0'
  base: '>=4.8 && <5'
  bytestring: '>=0.10 && <0.11'
  constraints: '>=0'
  criterion: '>=1.1 && <1.2'
  grenade: '>=0'
  hedgehog: '>=0.1 && <0.2'
  hmatrix: '>=0'
  mtl: '>=0'
  random: '>=0'
  reflection: '>=0'
  singletons: '>=0'
  text: '>=1.2 && <1.3'
  transformers: '>=0'
  typelits-witnesses: '>=0'
  vector: '>=0'
