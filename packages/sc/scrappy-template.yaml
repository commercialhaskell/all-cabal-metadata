all-versions:
- 0.1.0.0
author: Galen Sprout
basic-deps:
  aeson: '>=2.2.3 && <2.3'
  base: '>=4.19.0 && <4.21'
  bytestring: '>=0.12.2 && <0.13'
  containers: '>=0.7 && <0.8'
  lens: '>=5.3.6 && <5.4'
  modern-uri: '>=0.3.6 && <0.4'
  network-uri: '>=2.6.4 && <2.7'
  parsec: '>=3.1.18 && <3.2'
  parser-combinators: '>=1.3.1 && <1.4'
  scrappy-core: '>=0.1.0 && <0.2'
  text: '>=2.1.3 && <2.2'
  transformers: '>=0.6.1 && <0.7'
changelog: ''
changelog-type: ''
description: "# Scrappy #\n\n# <img src=\"./scrappy-logo.png.png\" width=\"200\">\n\n\nprovides
  a number of functions that allow for easily scraping certain patterns from websites
  as well control functions that allow you to rotate between multiple sites and rotate
  proxies in order to deal with common bot detection problems faced by scrapers. \n\n##
  Scraping ## \n\nScraping is parsing, where we don't care about the placement of
  our match in a chunk of data. ScraperT is really just ParsecT (ParsecT String ()
  Identity a)\n\n## How to depend on this project ## \n\nScrappy is currently not
  on Hackage because I honestly don't have time to perfect version bounds however
  through [Nix](https://nixos.org/download) you can get the package the following
  way:\n\nOR!! Just clone scrappy-tutorial and rename it \n\nOR if you really want
  to learn (hey good for you!) \n\n```bash \n# assuming you've done nix-setup... I'd
  show you how but it depends what setup you prefer and its super simple\nnix-env
  -f https://github.com/obsidiansystems/nix-thunk/archive/master.tar.gz -iA command\nnix-shell
  -p cabal2nix\ncd yourProjectDirectory\ncabal init \n# follow prompts\nmkdir deps
  \ngit clone https://github.com/Ace-Interview-Prep/scrappy.git deps/scrappy # or
  SSH url \nnix-thunk pack deps/scrappy # this is technically optional but recommended\n#
  add scrappy to your cabal file under build-depends\ncabal2nix . --shell > shell.nix
  -- this will create an easy to use shell.nix file with all of your build depends\nnix-shell
  #you could also do nix-shell shell.nix\ncabal run my-project-name-in-cabal-file
  \n```\n\n\n## Tutorials ## \n\n- https://medium.com/p/135283dc2af\n- https://github.com/Ace-Interview-Prep/scrappy-tutorial\n\n##
  Main functions, Types, and Classes ## \n\n```haskell\n\nclass ElementRep (a :: *
  -> *) where\n--type InnerHTMLRep a = something \n  elTag :: a b -> Elem\n  attrs
  :: a b -> Attrs\n  innerText' :: a b -> String \n  matches' :: a b -> [b]\n\ninstance
  ElementRep (Elem') where\n  elTag = _el\n  attrs = _attrs\n  innerText' = innerHtmlFull\n
  \ matches' = innerMatches\n\ndata Elem' a = Elem' { _el :: Elem \n                     ,
  _attrs :: Map String String \n                     , innerMatches :: [a] \n                     ,
  innerHtmlFull :: String\n                     } deriving Show\n\n-- | Manager returned
  may be a new manager, if the given manager failed \ngetHtml :: Manager -> Url ->
  IO (Manager, Html) \ngetHtml' :: Url -> IO Html\n\nscrape :: ScraperT a -> Html
  -> Maybe [a]\n\n-- | For more advanced control over inner structure see Elem.TreeElemParser\nelemParser
  :: Elem -> [(String, Maybe String)] \n            -> Maybe (ParsecT s u m a) --
  Optionally scraped pattern inside this el, if specified, return element must have
  at least 1 \n            -> ScraperT (Elem' a)\n\nel :: Elem -> [(String, String)]
  -> ScraperT (Elem' String)\n\n-- | Find any expressive pattern as long as it is
  inside of some HTML context \ncontains :: ScraperT (Elem' a) -> ScraperT a -> ScraperT
  a \n\n\n:t fmap snd getHtml' \"https://google.com\" >>= return . (runScraperOnHtml
  (el \"a\" [])   \n>>> IO (Maybe [Elem' a])\n\n-- Will get all <a id=\"link\"> 's
  on the url that are inside\n-- divs with a class of xyz\nexample :: IO (Maybe [Elem'
  a])\nexample = runScraperOnUrl url $ el \"div\" [(\"class\", \"xyz\")] `contains`\n
  \       el \"a\" [(\"id\", \"link\")]\n\n\n```\n\nCurrently with this library you
  can request HTML and run a multitude of scraper-patterns on it, for example:\n-
  Scrape an element\n- Scrape an element with a specific inner pattern\n- Scrape an
  element with an attribute that fits a specific parser-pattern \n- Scrape an element
  with a specific inner DOM-tree \n- Scrape a group of elements that *kinda* repeats
  \n   - For example, if you want a complex group that varies from page to page but
  to the user's eye looks the exact same (think of search results on a nice website)
  or even largely the same, then use the `htmlGroup` function from Scrappy.Elem.TreeElemParser
  \ \n- Scrape any **arbitrary** parser from the [parsec library](https://hackage.haskell.org/package/parsec)\n\nYou
  can also deeply chain! \n- Inside any instance of the ElementRep class (Elem', TreeHTML)
  through contains and contains'! \n   - For example use: think of getting all prices
  \n- In a sequence!\n   - skipToBody: to avoid matches in the <head>\n   - (</>>=)
  and (</>>) which are sequencers that take two parsers like Monadic actions (hence
  the characters chosen) but unlike running two parsers one after the other, there
  may be whatever random stuff in between! \n\n\n## TODO ## \n\nAs this is an Open
  Source ambitious project, there is much that is left to do which follows from the
  potential of this library as a result of it's inner workings\n\n- Upload to Hackage\n-
  Be included in the haskellPackages attribute of nixpkgs\n- grep/regex replacement
  (and streamEdit across multiple files, see below) \n  - I hate ReGeX with a burning
  passion. No way will I ever come back to an expression and say \"Oh that must match
  on emails!\". \n  - It would be neat to have a way to write haskell at the command
  line like \"grepScrappy -sR \"char 'a' >> some digit >> char 'b'\" but this string
  would quickly get massive so I wonder if there's a happy medium that exists such
  as shortforms for the most common parsers (char, many, some, digit, string, etc)
  that while we would still allow for any haskell string (which typifies to a ParsecT)
  there would be ways to shorten expressions\n- Improve `contains` and similar functions
  to be able to somehow apply the 'contained' parser to inside the start and end tags\n-
  More Generic typing and Monads for a SiteT, HistoryT, and WebT (an accumulator of
  new undiscovered sites)\n   - I have a number of complicated Monads all with their
  own varied implementations that could be greatly reduced to a SiteT which keeps
  state of what branches of the Site tree have been visited or seen.\n      - With
  a SiteT it would easily allow for logic to gather all new links (see Scrappy.Links)
  upon getting a new HTML via getHTML or sibling functions, and perhaps a new sibling
  function like getNextHTML which gets the next untouched HTML page \n      - SiteT
  extension would mean that we could create functions like `scrapeSite` which might
  look for all pattern matches across the site-tree. By extension this could apply
  to HistoryT and WebT via an incredibly simple interface. \n   - The concept of a
  SiteT implies there may be use in a HistoryT (ie all Sites visited with their respective
  site-trees) and a WebT which is meant to represent the entire web in the sense that
  it continues to extend and is not constrained to past history.\n   - Hopefully this
  gives intriguing ideas to why scrappy-nlp might be so powerful\n- Concurrency with
  streamEdits \n   - This functionality will help both for making it easier+quicker
  to perform concurrency when scraping (which is actually quite easy for different
  pages due to forkIO and standard library functions) but also makes a scrappy-DOM
  (see below) much easier to imagine as a robust web framework (imagine how long 10+
  prerender-like scrapers would take or rather 10 passthroughs). I see this as the
  biggest blocker to a 1st version of scrappy-DOM (which wouldn't yet have FRP or
  GHCJS functionality.. you could still write raw JavaScript in V1 though) \n   -
  Extending this idea, lazy-js (see below) is effectively a streamEdit performer and
  so this logic would also provide incredible speed to this 'category' of the DOM
  as well. \n- Break up into scrappy-core and scrappy-requests\n   - This is so that
  scrappy-core can be used in frontend apps that are built using GHCJS. GHCJS purposely
  doesn't build networking packages. \n- Fork a streamEdit package\n   - A lot of
  streamEdit packages are exceptionally complex while library functions like Data.Text
  leave much to be desired. scrappy-streamEdit would be a perfect medium\n- Fork a
  scrappy-staleLink (or better name) package \n   - For use with [Obelisk](https://github.com/obsidiansystems/obelisk)
  the static directory which will provide a list of static assets which are no longer
  in use and might be good to delete\n- Fork a scrappy-templates package\n   - This
  is in the works currently for templating files/strings with variables such as large
  prompts for GPT-*, for example you could template a prompt for resumes that is job
  dependent in an ergonomic/easy-to-use way\n   - See Scrappy.Templates\n   - To reduce
  runtime errors from performing IO, it would be nice to have a staticFile finder
  like with staticWhich: a package that ensures an executable's existence *at compile
  time* \n- scrappy + nlp\n   - It would be advantageous to use certain functions
  like a recursive scraper from pageN -> page(N+M), where M is based on a stop condition
  and/or the arrow a successful pattern match (see Scrappy.Run). While this can be
  easily done with HTML patterns, this extends and lends well to NLP analysis for
  the text that is contained in the structural HTML\n- scrappy-DOM\n   - I currently
  have a bone to pick with the `prerender` function from [reflex-dom](https://github.com/reflex-frp/reflex-dom)
  as it can easily fail. For example, if the browser insists on inserting a <tbody>
  inside a <table> tag then reflex-dom panics because the DOM is not as expected,
  even if the DOM area is on a completely different branch and way earlier than it.
  Intuitively, using streamEdit on an element with an attribute like prerenderID=\"some
  unique string\" would be an incredibly dependable solution. \n   - The blockers
  to this are of course that it's only worth doing if it will be an improvement to
  reflex-dom as a whole (or maybe we integrate with reflex-dom) and reflex-dom is
  truly exceptional with FRP, GHCJS bindings, and mobile-readiness \n- scrappy-headless\n
  \  - This is effectively a headless browser which has the ability to read and run
  JS in response to events. A lot of work has been done on this in the Scrappy.BuildActions
  module and in [lazy-js](https://github.com/augyg/lazy-js) which is a fork of this
  project\n   - Also need to implement httpHistory in a more robust way to make a
  headless browser more legit \n- lazy-js \n   - This is a foundational project for
  both scrappy-headless and scrappy-DOM which is a lazy reader and writer of JavaScript.
  An example use is virtualizing clicking a link to bypass any JavaScript bot detectors
  and/or just to follow JavaScript based events like a button which calls some JS
  to change window.location (like a fancy href)\n\n"
description-type: text
hash: 0d02264fc4176c6259a032fa6595f16d41b92afc99e36061d95a19476c8a69bf
homepage: https://github.com/Ace-Interview-Prep/scrappy
latest: 0.1.0.0
license-name: BSD-3-Clause
maintainer: galen.sprout@gmail.com
synopsis: html pattern matching library and high-level interface concurrent requests
  lib for webscraping
test-bench-deps: {}
