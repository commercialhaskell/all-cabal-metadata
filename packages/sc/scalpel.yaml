homepage: https://github.com/fimad/scalpel
changelog-type: markdown
hash: 942d692314b0cc1d0c2374d78a90bda5171d8879b38c7437653b208155922b8b
test-bench-deps:
  base: ! '>=4.7 && <5'
  text: -any
  criterion: ! '>=1.1'
  HUnit: -any
  scalpel: -any
  regex-tdfa: -any
  regex-base: -any
  tagsoup: -any
maintainer: willcoster@gmail.com
synopsis: A high level web scraping library for Haskell.
changelog: ! "# Change Log\n\n## HEAD\n\n## 0.3.1\n\n- Added the `innerHTML` and `innerHTMLs`
  scraper.\n- Added the `match` function which allows for the creation of arbitrary\n
  \ attribute predicates.\n- Fixed build breakage with GHC 8.0.1.\n\n## 0.3.0.1\n\n-
  Make tag and attribute matching case-insensitive.\n\n## 0.3.0\n\n- Added benchmarks
  and many optimizations.\n- The `select` method is removed from the public API.\n-
  Many methods now have a constraint that the string type parametrizing\n  TagSoup's
  tag type now must be order-able.\n- Added `scrapeUrlWithConfig` that will hopefully
  put an end to multiplying\n  `scrapeUrlWith*` methods.\n- The default behaviour
  of the `scrapeUrl*` methods is to attempt to infer the\n  character encoding from
  the `Content-Type` header.\n\n## 0.2.1.1\n\n- Cleanup stale instance references
  in documentation of TagName and\n  AttributeName.\n\n## 0.2.1\n\n- Made Scraper
  an instance of MonadPlus.\n\n## 0.2.0.1\n\n- Fixed examples in documentation and
  added an examples folder for ready to\n  compile examples. Added travis tests to
  ensures that examples remain\n  compilable.\n\n## 0.2.0\n\n- Removed the StringLike
  parameter from the Selector, Selectable,\n  AttributePredicate, AttributeName, and
  TagName types. Instead they are now\n  agnostic to the underlying string type, and
  are only constructable with\n  Strings and the Any type.\n\n## 0.1.3.1\n\n- Tighten
  dependencies and drop download-curl all together.\n\n## 0.1.3\n\n- Add the html
  and html scraper primitives for extracting raw HTML.\n\n## 0.1.2\n\n- Make scrapeURL
  follow redirects by default.\n- Expose a new function scrapeURLWithOpts that takes
  a list of curl options.\n- Fix bug (#2) where image tags that do not have a trailing
  \"/\" are not\n  selectable.\n\n## 0.1.1\n\n- Tighten dependencies on download-curl.\n\n##
  0.1.0\n\n- First version!\n"
basic-deps:
  bytestring: -any
  base: ! '>=4.6 && <5'
  text: -any
  curl: ! '>=1.3.4'
  data-default: -any
  containers: -any
  regex-tdfa: -any
  regex-base: -any
  tagsoup: ! '>=0.12.2'
all-versions:
- '0.1.0'
- '0.1.1'
- '0.1.2'
- '0.1.3'
- '0.1.3.1'
- '0.2.0'
- '0.2.0.1'
- '0.2.1'
- '0.2.1.1'
- '0.3.0'
- '0.3.0.1'
- '0.3.1'
author: Will Coster
latest: '0.3.1'
description-type: markdown
description: ! "Scalpel [![Build Status](https://travis-ci.org/fimad/scalpel.svg?branch=master)](https://travis-ci.org/fimad/scalpel)
  [![Hackage](https://img.shields.io/hackage/v/scalpel.svg)](https://hackage.haskell.org/package/scalpel)\n=======\n\nScalpel
  is a web scraping library inspired by libraries like\n[Parsec](http://hackage.haskell.org/package/parsec-3.1.7/docs/Text-Parsec.html)\nand
  Perl's [Web::Scraper](http://search.cpan.org/~miyagawa/Web-Scraper-0.38/).\nScalpel
  builds on top of [TagSoup](http://hackage.haskell.org/package/tagsoup)\nto provide
  a declarative and monadic interface.\n\nThere are two general mechanisms provided
  by this library that are used to build\nweb scrapers: Selectors and Scrapers.\n\nSelectors\n---------\n\nSelectors
  describe a location within an HTML DOM tree. The simplest selector,\nthat can be
  written is a simple string value. For example, the selector\n`\"div\"` matches every
  single div node in a DOM. Selectors can be combined\nusing tag combinators. The
  `//` operator to define nested relationships within a\nDOM tree. For example, the
  selector `\"div\" // \"a\"` matches all anchor tags\nnested arbitrarily deep within
  a div tag.\n\nIn addition to describing the nested relationships between tags, selectors
  can\nalso include predicates on the attributes of a tag. The `@:` operator creates
  a\nselector that matches a tag based on the name and various conditions on the\ntag's
  attributes. An attribute predicate is just a function that takes an\nattribute and
  returns a boolean indicating if the attribute matches a criteria.\nThere are several
  attribute operators that can be used to generate common\npredicates. The `@=` operator
  creates a predicate that matches the name and\nvalue of an attribute exactly. For
  example, the selector `\"div\" @: [\"id\" @=\n\"article\"]` matches div tags where
  the id attribute is equal to `\"article\"`.\n\nScrapers\n--------\n\nScrapers are
  values that are parameterized over a selector and produce a value\nfrom an HTML
  DOM tree. The `Scraper` type takes two type parameters. The first\nis the string
  like type that is used to store the text values within a DOM tree.\nAny string like
  type supported by `Text.StringLike` is valid. The second type\nis the type of value
  that the scraper produces.\n\nThere are several scraper primitives that take selectors
  and extract content\nfrom the DOM. Each primitive defined by this library comes
  in two variants:\nsingular and plural. The singular variants extract the first instance
  matching\nthe given selector, while the plural variants match every instance.\n\nExample\n-------\n\nComplete
  examples can be found in the\n[examples](https://github.com/fimad/scalpel/tree/master/examples)
  folder in the\nscalpel git repository.\n\nThe following is an example that demonstrates
  most of the features provided by\nthis library. Supposed you have the following
  hypothetical HTML located at\n`\"http://example.com/article.html\"` and you would
  like to extract a list of all\nof the comments.\n\n```html\n<html>\n  <body>\n    <div
  class='comments'>\n      <div class='comment container'>\n        <span class='comment
  author'>Sally</span>\n        <div class='comment text'>Woo hoo!</div>\n      </div>\n
  \     <div class='comment container'>\n        <span class='comment author'>Bill</span>\n
  \       <img class='comment image' src='http://example.com/cat.gif' />\n      </div>\n
  \     <div class='comment container'>\n        <span class='comment author'>Susan</span>\n
  \       <div class='comment text'>WTF!?!</div>\n      </div>\n    </div>\n  </body>\n</html>\n```\n\nThe
  following snippet defines a function, `allComments`, that will download\nthe web
  page, and extract all of the comments into a list:\n\n```haskell\ntype Author =
  String\n\ndata Comment\n    = TextComment Author String\n    | ImageComment Author
  URL\n    deriving (Show, Eq)\n\nallComments :: IO (Maybe [Comment])\nallComments
  = scrapeURL \"http://example.com/article.html\" comments\n   where\n       comments
  :: Scraper String [Comment]\n       comments = chroots (\"div\" @: [hasClass \"container\"])
  comment\n\n       comment :: Scraper String Comment\n       comment = textComment
  <|> imageComment\n\n       textComment :: Scraper String Comment\n       textComment
  = do\n           author      <- text $ \"span\" @: [hasClass \"author\"]\n           commentText
  <- text $ \"div\"  @: [hasClass \"text\"]\n           return $ TextComment author
  commentText\n\n       imageComment :: Scraper String Comment\n       imageComment
  = do\n           author   <- text       $ \"span\" @: [hasClass \"author\"]\n           imageURL
  <- attr \"src\" $ \"img\"  @: [hasClass \"image\"]\n           return $ ImageComment
  author imageURL\n```\n"
license-name: Apache-2.0
