homepage: https://wrengr.org/software/hackage.html
changelog-type: text
hash: ac28c6672bd0cae7a3ec5c6e7b2eeae4953d410762e94f0607c0893ae18256ec
test-bench-deps:
  bytestring: '>=0.9.2.1 && <0.13'
  tasty-smallcheck: '>=0.8.0.1 && <0.9'
  base: '>=4.5 && <5'
  tasty-quickcheck: '>=0.8.3.2 && <0.11'
  tasty: '>=0.10.1.2 && <1.6'
  bytestring-lexing: -any
maintainer: wren@cpan.org
synopsis: Efficiently parse and produce common integral and fractional numbers.
changelog: "0.5.0.11 (2023-11-15):\n    - Updated version bounds for base-4.19, bytestring-0.12,
  tasty-1.5\n0.5.0.10 (2023-03-19):\n    - Updated version bounds for GHC 9.6\n0.5.0.9
  (2021-08-28):\n    - Updated version bounds for GHC 9.4\n0.5.0.8 (2021-11-02):\n
  \   - Updated version bounds for GHC 9.2.1\n0.5.0.7 (2021-10-16):\n    - Switching
  from TravisCI to GithubActions\n\t- Linting Haddock warnings\n\t- Remove some trailing
  whitespaces\n0.5.0.6 (2019-04-13):\n    - Nudging everything to the correct urls,
  emails, etc\n0.5.0.2 (2015-05-06):\n    - Fixed the benchmarking url\n0.5.0.1 (2015-05-06):\n
  \   - Cleaned up the README file\n0.5.0 (2015-05-06):\n    - Corrected the License
  field in the .cabal file to say BSD2 (instead of BSD3)\n    - Data.ByteString.Lex.{Double,.Lazy.Double}:
  removed\n    - Data.ByteString.Lex.Fractional: added based on the inefficiency of
  the old Alex-based parsers, as demonstrated by Hirotomo Moriwaki's bytestring-read
  (v0.3.0).\n\n0.4.3.3 (2015-05-30):\n    - Moved VERSION to CHANGELOG\n0.4.3.1 (2014-03-07):\n
  \   - Updated the .cabal file to require newer alex for newer ghc.\n0.4.3 (2013-03-21):\n
  \   - Data.ByteString.Lex.Integral: Corrected a segmentation fault in packDecimal.\n0.4.2
  (2013-03-20):\n    - Data.ByteString.Lex.Integral: Improved packDecimal.\n0.4.1
  (2012-00-00):\n    - Data.ByteString.Lex.Integral: Added buffer overflow check for
  asHexadecimal\n0.4.0 (2012-02-03):\n    - Data.ByteString.Lex.Integral: added readDecimal_\n\n0.3.0
  (2012-01-28):\n    - Added Data.ByteString.Lex.Integral\n    - Converted repo to
  Darcs-2 hashed format.\n    - wren ng thornton took over maintainership.\n\n0.2.1
  (2010-02-14):\n0.2 (2008-10-15):\n    - Add support for lexing lazy bytestrings.\n\n0.1.2
  (2008-07-23):\n0.1.0.2 (2008-07-23):\n0.1.0.1 (2008-07-19):\n0.1.0 (2008-07-19):\n"
basic-deps:
  bytestring: '>=0.9.2.1 && <0.13'
  base: '>=4.5 && <4.20'
all-versions:
- '0.1'
- 0.1.0.1
- 0.1.0.2
- 0.1.2
- '0.2'
- 0.2.1
- 0.3.0
- 0.4.0
- 0.4.2
- 0.4.3
- 0.4.3.1
- 0.4.3.2
- 0.4.3.3
- 0.5.0
- 0.5.0.1
- 0.5.0.2
- 0.5.0.7
- 0.5.0.8
- 0.5.0.9
- 0.5.0.10
- 0.5.0.11
author: wren gayle romano, Don Stewart
latest: 0.5.0.11
description-type: markdown
description: "bytestring-lexing\n=================\n[![Hackage version](https://img.shields.io/hackage/v/bytestring-lexing.svg?style=flat)](https://hackage.haskell.org/package/bytestring-lexing)
  \n[![Build Status](https://github.com/wrengr/bytestring-lexing/workflows/ci/badge.svg)](https://github.com/wrengr/bytestring-lexing/actions?query=workflow%3Aci)\n[![Dependencies](https://img.shields.io/hackage-deps/v/bytestring-lexing.svg?style=flat)](http://packdeps.haskellers.com/specific?package=bytestring-lexing)\n\nThe
  bytestring-lexing package offers extremely efficient `ByteString`\nparsers for some
  common lexemes: namely integral and fractional\nnumbers. In addition, it provides
  efficient serializers for (some\nof) the formats it parses.\n\nAs of version 0.3.0,
  bytestring-lexing offers the best-in-show\nparsers for integral values. And as of
  version 0.5.0 it offers (to\nmy knowledge) the best-in-show parser for fractional/floating\nnumbers.
  A record of these benchmarks can be found\n[here](https://github.com/wrengr/bytestring-lexing/tree/master/bench/html)\n\n\n##
  Install\n\nThis is a simple package and should be easy to install. You should\nbe
  able to use the standard:\n\n    $> cabal install bytestring-lexing\n\n\n### Testing\n\nTo
  run the test suite (without coverage information), you can use\nthe standard method
  (with `runhaskell Setup.hs` in lieu of `cabal`,\nif necessary):\n\n    $> cd bytestring-lexing\n
  \   $> cabal configure --enable-tests\n    $> cabal build\n    $> cabal test\n\nIf
  you want coverage information as well, there are a few options\ndepending on your
  version of Cabal.  For modern cabal with v2/nix-style\nbuilds, add `--enable-coverage`
  to the configure step, and the\nresults will be located at\n`./dist-newstyle/build/$ARCH/$GHC/bytestring-lexing-$VERSION/opt/hpc/vanilla/html/bytestring-lexing-$VERSION/hpc_index.html`.\nFor
  v1/classic builds, add `--enable-coverage` to the configure\nstep and also add `--keep-tix-files`
  to the test step, and the\nresults are instead located at\n`./dist/hpc/vanilla/html/bytestring-lexing-$VERSION/hpc_index.html`.\nFor
  very old versions of Cabal, you must use `--enable-library-coverage`\nin lieu of
  `--enable-coverage`.\n\n\n### Benchmarks\n\nIf you want to run the benchmarking
  code, then do:\n\n    $> cd bytestring-lexing/bench\n    $> cabal configure\n    $>
  cabal build\n    $> for b in isSpace numDigits packDecimal readDecimal readExponential
  ceilEightThirds; do\n           ./dist/build/bench-${b}/bench-${b} -o ${b}.html;\n
  \      done && open *.html\n\nOf course, you needn't run all the benchmarking programs
  if you\ndon't want. Notably, these benchmarks are artefacts of the development\nof
  the library. They are not necessarily the most up-to-date\nreflection of the library
  itself, nor of other Haskell libraries\nwe've compared against in the past.\n\n\n##
  Portability\n\nAn attempt has been made to keep this library portable. However,\nwe
  do make use of two simple language extensions. Both of these\nwould be easy enough
  to remove, but they should not pose a significant\nportability burden. If they do
  in fact pose a burden for your\ncompiler, contact the maintainer.\n\n* ScopedTypeVariables
  - the `decimalPrecision` function in\n    `Data.ByteString.Lex.Fractional` uses
  ScopedTypeVariables for\n    efficiency; namely to ensure that the constant function\n
  \   `decimalPrecision` need only compute its result once (per type),\n    and that
  its result has no data dependency on the proxy argument.\n* BangPatterns - are used
  to make the code prettier and to \"improve\"\n    code coverage over the equivalent
  semantics via the following\n    idiom:\n    \n        foo x ... z\n            |
  x `seq` ... `seq` z `seq` False = error \"impossible\"\n            | otherwise
  = ...\n    \n    BangPatterns are supported in GHC as far back as [version\n    6.6.1][ghc-bangpatterns],
  and are also supported by\n    [JHC][jhc-bangpatterns] and [UHC][uhc-bangpatterns].
  As of 2010,\n    they were [not supported by Hugs][hugs-bangpatterns]; but alas\n
  \   Hugs is pretty much dead now.\n\n[ghc-bangpatterns]: \n    https://downloads.haskell.org/~ghc/6.6.1/docs/html/users_guide/sec-bang-patterns.html\n[jhc-bangpatterns]:\n
  \   http://repetae.net/computer/jhc/manual.html#code-options\n[uhc-bangpatterns]:\n
  \   https://github.com/UU-ComputerScience/uhc-js/issues/1\n[hugs-bangpatterns]:
  \n    https://mail.haskell.org/pipermail/haskell-cafe/2010-July/079946.html\n\n\n##
  Changes: Version 0.5.0 (2015-05-06) vs 0.4.3 (2013-03-21)\n\nI've completely overhauled
  the parsers for fractional numbers.\n\nThe old `Data.ByteString.Lex.Double` and
  `Data.ByteString.Lex.Lazy.Double`\nmodules have been removed, as has their reliance
  on Alex as a build\ntool. I know some users were reluctant to use bytestring-lexing\nbecause
  of that dependency, and forked their own version of\nbytestring-lexing-0.3.0's integral
  parsers. This is no longer an\nissue, and those users are requested to switch over
  to using\nbytestring-lexing.\n\nThe old modules are replaced by the new `Data.ByteString.Lex.Fractional`\nmodule.
  This module provides two variants of the primary parsers.\nThe `readDecimal` and
  `readExponential` functions are very simple\nand should suffice for most users'
  needs. The `readDecimalLimited`\nand `readExponentialLimited` are variants which
  take an argument\nspecifying the desired precision limit (in decimal digits). With\ncare,
  the limited-precision parsers can perform far more efficiently\nthan the unlimited-precision
  parsers. Performance aside, they can\nalso be used to intentionally restrict the
  precision of your program's\ninputs.\n\n\n## Benchmarks: Version 0.5.0 (2015-05-06)\n\nThe
  Criterion output of the benchmark discussed below, [is available\nhere](https://github.com/wrengr/bytestring-lexing/blob/master/bench/html/readExponential-0.5.0_ereshkigal.html).\nThe
  main competitors we compare against are the previous version\nof bytestring-lexing
  (which already surpassed text and\nattoparsec/scientific) and bytestring-read which
  was the previous\nbest-in-show.\n\nThe unlimited-precision parsers provide 3.3x
  to 3.9x speedup over\nthe `readDouble` function from bytestring-lexing-0.4.3.3,
  as well\nas being polymorphic over all `Fractional` values. For `Float`/`Double`:\nthese
  functions have essentially the same performance as bytestring-read\non reasonable
  inputs (1.07x to 0.89x), but for inputs which have\nfar more precision than `Float`/`Double`
  can handle these functions\nare much slower than bytestring-read (0.30x 'speedup').
  However,\nfor `Rational`: these functions provide 1.26x to 1.96x speedup\ncompared
  to bytestring-read.\n\nThe limited-precision parsers do even better, but require
  some care\nto use properly. For types with infinite precision (e.g., `Rational`)\nwe
  can pass in an 'infinite' limit by passing the length of the\ninput string plus
  one. For `Rational`: doing so provides 1.5x speedup\nover the unlimited-precision
  parsers (and 1.9x to 3x speedup over\nbytestring-read), because we can avoid intermediate
  renormalizations.\nWhether other unlimited precision types would see the same benefit\nremains
  an open question.\n\nFor types with inherently limited precision (e.g., `Float`/`Double`),\nwe
  could either pass in an 'infinite' limit or we could pass in the\nactual inherent
  limit. For types with inherently limited precision,\npassing in an 'infinite' limit
  degrades performance compared to the\nunlimited-precision parsers (0.51x to 0.8x
  'speedup'). Whereas,\npassing in the actual inherent limit gives 1.3x to 4.5x speedup\nover
  the unlimited-precision parsers. They also provide 1.2x to\n1.4x speedup over bytestring-read;
  for a total of 5.1x to 14.4x\nspeedup over bytestring-lexing-0.4.3.3!\n\n\n## Links\n\n*
  [Website](https://wrengr.org/)\n* [Blog](http://winterkoninkje.dreamwidth.org/)\n*
  [Twitter](https://twitter.com/wrengr)\n* [Hackage](http://hackage.haskell.org/package/bytestring-lexing)\n*
  [GitHub](https://github.com/wrengr/bytestring-lexing)\n"
license-name: BSD-3-Clause
