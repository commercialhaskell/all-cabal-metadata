all-versions:
- 0.1.0.0
- 0.1.0.1
- 0.1.0.2
- 0.1.0.3
- 0.2.0.0
- 0.2.0.1
- 0.2.0.2
author: Michael Chavinda
basic-deps:
  array: '>=0.5 && <0.6'
  attoparsec: '>=0.12 && <=0.14.4'
  base: '>=4.17.2.0 && <4.22'
  bytestring: '>=0.11 && <=0.12.2.0'
  containers: '>=0.6.7 && <0.8'
  directory: '>=1.3.0.0 && <=1.3.9.0'
  filepath: '>=1.0.0.0 && <=1.5.4.0'
  hashable: '>=1.2 && <=1.5.0.0'
  snappy: '>=0.2.0.0 && <=0.2.0.4'
  statistics: '>=0.16.2.1 && <=0.16.3.0'
  text: '>=2.0 && <=2.1.2'
  time: '>=1.12 && <=1.14'
  vector: '>=0.13 && <0.14'
  vector-algorithms: '>=0.9 && <0.10'
  zstd: '>=0.1.2.0 && <=0.1.3.0'
changelog: "# Revision history for dataframe\n\n## 0.2.0.1\n* Fix bug with new comparison
  expressions. gt and geq were actually implemented as lt and leq.\n* Changes to make
  library work with ghc 9.10.1 and 9.12.2\n\n## 0.2.0.0\n### Replace `Function` adt
  with a column expression syntax.\n\nPreviously, we tried to stay as close to Haskell
  as possible. We used the explicit\nordering of the column names in the first part
  of the tuple to determine the function\narguments and the a regular Haskell function
  that we evaluated piece-wise on each row.\n\n```haskell\nlet multiply (a :: Int)
  (b :: Double) = fromIntegral a * b\nlet withTotalPrice = D.deriveFrom ([\"quantity\",
  \"item_price\"], D.func multiply) \"total_price\" df\n```\n\nNow, we have a column
  expression syntax that mirrors Pyspark and Polars.\n\n```haskell\nlet withTotalPrice
  = D.derive \"total_price\" (D.lift fromIntegral (D.col @Int \"quantity\") * (D.col
  @Double\"item_price\")) df\n```\n\n### Adds a coverage report to the repository
  (thanks to @oforero)\nWe don't have good test coverage right now. This will help
  us determine where to invest.\n@oforero provided a script to make an HPC HTML report
  for coverage.\n\n### Convenience functions for comparisons \nInstead of lifting
  all bool operations we provide `eq`, `leq` etc.\n\n## 0.1.0.3\n* Use older version
  of correlation for ihaskell itegration\n\n## 0.1.0.2\n* Change namespace from `Data.DataFrame`
  to `DataFrame`\n* Add `toVector` function for converting columns to vectors.\n*
  Add `impute` function for replacing `Nothing` values in optional columns.\n* Add
  `filterAllJust` to filter out all rows with missing data.\n* Add `distinct` function
  that returns a dataframe with distict rows.\n\n## 0.1.0.1\n* Fixed parse failure
  on nested, escaped quotation.\n* Fixed column info when field name isn't found.\n\n##
  0.1.0.0\n* Initial release\n"
changelog-type: markdown
description: "# DataFrame\n\nAn intuitive, dynamically-typed DataFrame library.\n\nA
  tool for exploratory data analysis.\n\n## Installing\n\n### CLI\n* Install Haskell
  (ghc + cabal) via [ghcup](https://www.haskell.org/ghcup/install/) selecting all
  the default options.\n* To install dataframe run `cabal update && cabal install
  dataframe`\n* Open a Haskell repl with dataframe loaded by running `cabal repl --build-depends
  dataframe`.\n* Follow along any one of the tutorials below.\n\n### Jupyter notebook\n*
  Use the Dockerfile in the [ihaskell-dataframe](https://github.com/mchav/ihaskell-dataframe)
  to build and run an image with dataframe integration.\n* For a preview check out
  the [California Housing](https://github.com/mchav/dataframe/blob/main/docs/California%20Housing.ipynb)
  notebook.\n\n## What is exploratory data analysis?\nWe provide a primer [here](https://github.com/mchav/dataframe/blob/main/docs/exploratory_data_analysis_primer.md)
  and show how to do some common analyses.\n\n## Coming from other dataframe libraries\nFamiliar
  with another dataframe library? Get started:\n* [Coming from Pandas](https://github.com/mchav/dataframe/blob/main/docs/coming_from_pandas.md)\n*
  [Coming from Polars](https://github.com/mchav/dataframe/blob/main/docs/coming_from_polars.md)\n*
  [Coming from dplyr](https://github.com/mchav/dataframe/blob/main/docs/coming_from_dplyr.md)\n\n##
  Example usage\n\n### Code example\n```haskell\nimport qualified DataFrame as D\n\nimport
  DataFrame ((|>))\n\nmain :: IO ()\n    df <- D.readTsv \"./data/chipotle.tsv\"\n
  \   print $ df\n      |> D.select [\"item_name\", \"quantity\"]\n      |> D.groupBy
  [\"item_name\"]\n      |> D.aggregate (zip (repeat \"quantity\") [D.Maximum, D.Mean,
  D.Sum])\n      |> D.sortBy D.Descending [\"Sum_quantity\"]\n```\n\nOutput:\n\n```\n----------------------------------------------------------------------------------------------------\nindex
  |               item_name               | Sum_quantity |   Mean_quantity    | Maximum_quantity\n------|---------------------------------------|--------------|--------------------|-----------------\n
  Int  |                 Text                  |     Int      |       Double       |
  \      Int       \n------|---------------------------------------|--------------|--------------------|-----------------\n0
  \    | Chips and Fresh Tomato Salsa          | 130          | 1.1818181818181819
  | 15              \n1     | Izze                                  | 22           |
  1.1                | 3               \n2     | Nantucket Nectar                      |
  31           | 1.1481481481481481 | 3               \n3     | Chips and Tomatillo-Green
  Chili Salsa | 35           | 1.1290322580645162 | 3               \n4     | Chicken
  Bowl                          | 761          | 1.0482093663911847 | 3               \n5
  \    | Side of Chips                         | 110          | 1.0891089108910892
  | 8               \n6     | Steak Burrito                         | 386          |
  1.048913043478261  | 3               \n7     | Steak Soft Tacos                      |
  56           | 1.018181818181818  | 2               \n8     | Chips and Guacamole
  \                  | 506          | 1.0563674321503131 | 4               \n9     |
  Chicken Crispy Tacos                  | 50           | 1.0638297872340425 | 2\n```\n\nFull
  example in `./app` folder using many of the constructs in the API.\n\n### Visual
  example\n![Screencast of usage in GHCI](./static/example.gif)\n\n## Supported input
  formats\n* CSV\n* Apache Parquet (still buggy and experimental)\n\n## Future work\n*
  Apache arrow compatability\n* Integration with common data formats (currently only
  supports CSV)\n* Support windowed plotting (currently only supports ASCII plots)\n\n##
  Contributing\n* Please first submit an issue and we can discuss there.\n"
description-type: markdown
hash: feb3dd770dc7fd8e551f51b8e162da074514081cca42e88b470ad2ca1fe673c1
homepage: ''
latest: 0.2.0.2
license-name: GPL-3.0-or-later
maintainer: mschavinda@gmail.com
synopsis: An intuitive, dynamically-typed DataFrame library.
test-bench-deps:
  HUnit: ^>=1.6
  base: '>=4.17.2.0 && <4.22'
  criterion: '>=1 && <=1.6.4.0'
  dataframe: '>=0'
  random: '>=1 && <=1.3.1'
  random-shuffle: '>=0.0.4'
  text: '>=2.0 && <=2.1.2'
  time: '>=1.12'
  vector: '>=0.13 && <0.14'
