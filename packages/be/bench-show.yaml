homepage: https://github.com/composewell/bench-show
changelog-type: markdown
hash: ecc994d4f3ba79468bd56b281671996f94a8103e381d5e9ad371ca1a75405a37
test-bench-deps:
  bench-show: -any
  split: ==0.2.*
  base: ! '>=4.8 && <5'
  text: ! '>=1.1.1 && <1.3'
maintainer: harendra.kumar@gmail.com
synopsis: Show, plot and compare benchmark results
changelog: ! "## 0.2.2\n\n* Allow additional annotations to title to be controlled
  via config\n* Better error handling\n\n## 0.2.1\n\n* Use new version of `statistics`
  package.\n\n## 0.2.0\n\n### Release Notes\n\n* Due to a bug in the `statistics`
  package, reporting may crash on certain\n  inputs with a `vector index out of bounds`
  message. The bug has been fixed\n  and will be available in an upcoming release.\n\n###
  Breaking Changes\n\n* The package `bench-graph` has been renamed to `bench-show`
  to reflect the\n  fact that it now includes text reports as well. This includes
  the change of\n  module name `BenchGraph` to `BenchShow`.\n* The `bgraph` API has
  been removed and replaced by `graph`\n* The way output file is generated has changed.
  Now field name or group name\n  being plotted or both may be suffixed to the output
  file name automatically.\n  The estimator type (e.g. mean or median) is also suffixed
  to the filename.\n* Changes to `Config` record:\n    * `chartTitle` field has been
  renamed to `title`.\n    * The type of `outputDir` is now a `Maybe`.\n    * `comparisonStyle`
  has been replaced by `presentation`\n    * `ComparisonStyle` has been replaced by
  `Presentation`\n    * `sortBenchmarks` has been replaced by `selectBenchmarks`.
  The new\n      function can be defined as follows in terms of an older definition:\n
  \       `selectBenchmarks = \\g ->\n            sortBenchmarks $ either error (map
  fst) $ f (ColumnIndex 0)`\n    * `sortBenchGroups` has been replaced by `selectGroups`\n
  \   * `setYScale` field has been broken down into two fields `fieldRanges` and\n
  \     `fieldTicks`. Now you also need to specify which fields' scale\n      you
  want to set.\n\n### Enhancements\n\n* A `report` API has been added to generate
  textual reports\n* More ways to compare groups have been added, including percent
  and percent\n  difference\n* Now we can show multiple fields as columns in a single
  benchmark group report\n* Field units are now automatically selected based on the
  range of values\n* Additions to `Config` record type:\n  * `selectFields` added
  to select the fields to be plotted and to change\n    their presentation order.\n
  \ * `selectBenchmarks` can now sort the results based on values corresponding to\n
  \   any field or benchmark group.\n  * new fields added: `diffStrategy`, `verbose`,
  `estimator`, `threshold`\n\n## 0.1.4\n\n* Fix a bug resulting in a bogus error,
  something like \"Field [time] found at\n  different indexes..\" even though the
  field has exactly the same index at all\n  places.\n\n## 0.1.3\n\n* Add maxrss plotting
  support\n\n## 0.1.2\n\n* Fixed a bug that caused missing graphs in some cases when
  multiple iterations\n  of a benchmark are present in the bechmark results file.\n\n*
  Better error reporting to pinpoint errors when a problem occurs.\n\n## 0.1.1\n\n*
  Support GHC 8.4\n\n## 0.1.0\n\n* Initial release\n"
basic-deps:
  Chart: ! '>=1.6 && <2'
  mwc-random: ! '>=0.13 && <0.15'
  Chart-diagrams: ! '>=1.6 && <2'
  ansi-wl-pprint: ! '>=0.6 && <0.7'
  split: ! '>=0.2 && <0.3'
  base: ! '>=4.8 && <5'
  filepath: ! '>=1.3 && <1.5'
  csv: ! '>=0.1 && <0.2'
  statistics: ! '>=0.15 && <0.16'
  transformers: ! '>=0.4 && <0.6'
  vector: ! '>=0.10 && <0.13'
  directory: ! '>=1.2 && <1.4'
all-versions:
- '0.2.0'
- '0.2.1'
- '0.2.2'
author: Harendra Kumar
latest: '0.2.2'
description-type: markdown
description: ! "# bench-show\n\nGenerate text reports and graphical charts from the
  benchmark results\ngenerated by `gauge` or `criterion`, showing or comparing benchmarks
  in\nmany useful ways. In a few lines of code, we can report time taken, peak\nmemory
  usage, allocations, among many other fields; we can group benchmarks\nand compare
  the groups; we can compare benchmarks before and after a change;\nwe can show absolute
  or percentage difference from the baseline; we can sort\nthe results to get the
  worst affected benchmarks by percentage change.\n\nIt can help us in answering questions
  like the following, visually or\ntextually:\n\n* Across two benchmark runs, show
  all the operations that resulted in a\n  regression of more than 10%, so that we
  can quickly identify and fix\n  performance problems in our application.\n* Across
  two (or more) packages providing similar functionality, show all the\n  operations
  where the performance differs by more than 10%, so that we can\n  critically analyze
  the packages and choose the right one.\n\n## Quick Start\n\nUse `gauge` or `criterion`
  to generate a `results.csv` file, and then use the\nfollowing code to generate a
  textual report or a graph:\n\n```\nreport \"results.csv\"  Nothing defaultConfig\ngraph
  \ \"results.csv\" \"output\" defaultConfig\n```\n\nFor advanced usage, control the
  generated report by modifying the\n`defaultConfig`.\n\n## Reports and Charts\n\n`report`
  with `Fields` presentation style generates a multi-column report.  We\ncan select
  many fields from a `gauge` raw report.  Units of the fields are\nautomatically determined
  based on the range of values:\n\n```haskell\nreport \"results.csv\" Nothing defaultConfig
  { presentation = Fields }\n```\n\n```\nBenchmark     time(μs) maxrss(MiB)\n-------------
  -------- -----------\nvector/fold     641.62        2.75\nstreamly/fold   639.96
  \       2.75\nvector/map      638.89        2.72\nstreamly/map    653.36        2.66\nvector/zip
  \     651.42        2.58\nstreamly/zip    644.33        2.59\n```\n\n`graph` generates
  one bar chart per field:\n\n```\ngraph \"results.csv\" \"output\" defaultConfig\n```\n\nWhen
  the input file contains results from a single benchmark run, by default\nall the
  benchmarks are placed in a single benchmark group named \"default\".\n\n[![Median
  Time Grouped](https://github.com/composewell/bench-show/blob/master/docs/full-median-time.svg)](https://github.com/composewell/bench-show/blob/master/docs/full-median-time.svg)\n\n##
  Grouping\n\nLet's write a benchmark classifier to put the `streamly` and `vector`\nbenchmarks
  in their own groups:\n\n```haskell\n   classifier name =\n       case splitOn \"/\"
  name of\n           grp : bench -> Just (grp, concat bench)\n           _          ->
  Nothing\n```\n\nNow we can show the two benchmark groups as separate columns. We
  can\ngenerate reports comparing different benchmark fields (e.g. `time` and\n`maxrss`)
  for all the groups:\n\n```haskell\n   report \"results.csv\" Nothing\n     defaultConfig
  { classifyBenchmark = classifier }\n```\n\n```\n(time)(Median)\nBenchmark streamly(μs)
  vector(μs)\n--------- ------------ ----------\nfold            639.96     641.62\nmap
  \            653.36     638.89\nzip             644.33     651.42\n```\n\nWe can
  do the same graphically as well, just replace `report` with `graph`\nin the code
  above.  Each group is placed as a cluster on the graph. Multiple\nclusters are placed
  side by side (i.e. on the same scale) for easy\ncomparison. For example:\n\n[![Median
  Time Grouped](https://github.com/composewell/bench-show/blob/master/docs/grouped-median-time.svg)](https://github.com/composewell/bench-show/blob/master/docs/grouped-median-time.svg)\n\n##
  Regression, Percentage Difference and Sorting\n\nWe can append benchmarks results
  from multiple runs to the same file. These\nruns can then be compared. We can run
  benchmarks before and after a change\nand then report the regressions by percentage
  change in a sorted order:\n\nGiven a results file with two runs, this code generates
  the report that\nfollows:\n\n```haskell\n   report \"results.csv\" Nothing\n     defaultConfig\n
  \        { classifyBenchmark = classifier\n         , presentation = Groups PercentDiff\n
  \        , selectBenchmarks = \\f ->\n              reverse\n              $ map
  fst\n              $ sortBy (comparing snd)\n              $ either error id $ f
  $ ColumnIndex 1\n         }\n```\n\n```\n(time)(Median)(Diff using min estimator)\nBenchmark
  streamly(0)(μs)(base) streamly(1)(%)(-base)\n--------- --------------------- ---------------------\nzip
  \                     644.33                +23.28\nmap                      653.36
  \                +7.65\nfold                     639.96                -15.63\n```\n\nIt
  tells us that in the second run the worst affected benchmark is zip\ntaking 23.28
  percent more time compared to the baseline.\n\nGraphically:\n\n[![Median Time Regression](https://github.com/composewell/bench-show/blob/master/docs/regression-percent-descending-median-time.svg)](https://github.com/composewell/bench-show/blob/master/docs/regression-percent-descending-median-time.svg)\n\n##
  Full Documentation and examples\n\n* See the [haddock documentation](http://hackage.haskell.org/package/bench-show)
  on Hackage\n* See the [comprehensive tutorial](http://hackage.haskell.org/package/bench-show)
  module in the haddock docs\n* For examples see the [test directory](https://github.com/composewell/bench-show/tree/master/test)
  in the package\n\n## Contributions and Feedback\n\nContributions are welcome! Please
  see the [TODO.md](TODO.md) file or the\nexisting [issues](https://github.com/composewell/bench-show/issues)
  if you want\nto pick up something to work on.\n\nAny feedback on improvements or
  the direction of the package is welcome. You\ncan always send an email to the\n[maintainer](https://github.com/composewell/bench-show/blob/master/bench-show.cabal)\nor
  [raise an issue](https://github.com/composewell/bench-show/issues/new) for\nanything
  you want to suggest or discuss, or send a PR for any change that you\nwould like
  to make.\n"
license-name: BSD3
