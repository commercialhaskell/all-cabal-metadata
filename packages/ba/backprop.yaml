homepage: https://backprop.jle.im
changelog-type: markdown
hash: 4daa553cc56f00cb2919dc3285a09c587225ca951bb3c13d5c7f8648b57e40dd
test-bench-deps:
  mwc-random: -any
  base: ! '>=4.7 && <5'
  time: -any
  criterion: -any
  lens: -any
  deepseq: -any
  hmatrix: ! '>=0.18'
  backprop: -any
  vector: -any
  directory: -any
maintainer: justin@jle.im
synopsis: Heterogeneous automatic differentation
changelog: ! "Changelog\n=========\n\nVersion 0.2.4.0\n---------------\n\n*May 28,
  2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.2.4.0>\n\n**NOTE**
  Major breaking changes to *Explicit* modules, and some re-shuffling of\ntypeclass
  constraints on various non-explicit functions that should only affect\npolymorphic
  usage.\n\n*   *Huge improvements in performance!*  Around 20-40% reduction in\n
  \   runtimes/overheads, with savings higher for large matrix situations or\n    situations
  with expensive `add`.\n*   However, this restructuring required *major* reshuffling
  of constraints on\n    `Backprop`/`Num` for most functions.  These are potentially
  **breaking\n    changes** for polymorphic code, but monomorphic code should remain\n
  \   unchanged.  However, code using the *Explicit* interfaces is most likely\n    broken
  unfortunately.  Fixes just include adding or dropping `zeroFunc`s to\n    the appropriate
  functions.\n*   Added warnings to *Explicit* modules that the API is \"semi-stable\".\n*
  \  `overVar` and `%~~`, for modifying fields.  Essentially a wrapper over a\n    `viewVar`
  and `setVar`.\n*   Argument order in the `backpropWith` family of functions changed
  again;\n    **breaking change** for those using any `backpropWith` function.  However,\n
  \   the new order is much more usable.\n*   Changes to the argument order in the
  `backprop` family of functions in the\n    *Explicit* interfaces now reverted back
  to previous order, from v0.2.0 and\n    before.  Should be an \"un-breaking\" change,
  but will break code written in\n    v0.2.3 style.\n*   Bechmarks now include HKD
  access and a \"hybrid\" approach.  Documentation\n    updated to reflect results.\n*
  \  Documentation updated to include a new \"unital\" law for `one`, namely `one\n
  \   = gradBP id`.\n*   Fixity declarations for `^^?`, `^^?!`, and `<$>`.\n*   Added
  `fmap . const` and `<$` to *Prelude* modules.\n*   `Backprop` instances for `Expr`
  from *simple-reflect*\n*   Added `zeroVecNum` and `oneVecNum` to *Numeric.Backprop.Class*,
  which is\n    potentially more efficient than `zeroVec` and `oneVec` if the items
  are\n    instances of `Num` and the vectors are larger.  Also added `NumVec` newtype\n
  \   wrapper giving `Backprop` instances to vectors using `zeroVecNum` and\n    `oneVecNum`
  instead of `zeroVec` and `oneVec`.\n*   `Build.hs` build script now also builds
  profiling results\n\nVersion 0.2.3.0\n---------------\n\n*May 25, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.2.3.0>\n\n*
  \  Argument order in `backpropWith` family of functions switched around to\n    allow
  for final gradient to be given after-the-fact.  **Breaking change**\n    for anyone
  using any `backpropWith` function.\n*   As a consequence of the previous change,
  `backprop` family of functions in\n    *Explicit* interfaces also all changed argument
  order.  **Breaking change**\n    only for those using the *Explicit* interfaces.\n*
  \  Explicit `collectVar` no longer needs a `ZeroFunc` for the container, and\n    so
  all versions of `collectVar` and functions that use it (`fmap`,\n    `liftA2`, `liftA3`,
  `traverse`, `mapAccumL`, `mapAccumR`) no longer require\n    `Backprop` or `Num`
  instances for the final returned container type.  This\n    enables a lot more flexibility
  in container types.  **Breaking change**\n    only for those using the *Explicit*
  interfaces.\n*   `BV` pattern synonym added to *Numeric.Backprop*, abstracting over\n
  \   application of `splitBV` and `joinBV`.\n*   `foldr` and `foldl'` added to Prelude
  modules, for convenience.\n*   `round` and `fromIntegral'` (\"unround\") added to
  Prelude modules.\n\nVersion 0.2.2.0\n---------------\n\n*May 12, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.2.2.0>\n\n*
  \  `evalBP0` added, for convenience for no-argument values that need to be\n    evaluated
  without backpropagation.\n*   `splitBV` and `joinBV` for \"higher-kinded data\"
  style `BVar` manipulation,\n    via the `BVGroup` helper typeclass.\n*   `toList`,
  `mapAccumL`, and `mapAccumR` for *Prelude.Backprop* modules\n*   `Backprop` instance
  for `BVar`\n*   *COMPLETE* pragmas for `T2` and `T3`\n*   Un-exported `gzero`, `gadd`,
  and `gone` from *Numeric.Backprop.Class*\n*   Many, many more instances of `Backprop`\n*
  \  `Backprop` instance for `Proxy` made non-strict for `add`\n*   Swapped type variable
  order for a few library functions, which might\n    potentially be breaking changes.\n\n*Internal*\n\n*
  \  Fixed documentation for Num and Explicit Prelude modules, and rewrote\n    normal
  and Num Prelude modules in terms of canonical Prelude definitions\n*   Switched
  to `errorWithoutStackTrace` wherever appropriate (in *Internal*\n    module)\n\nVersion
  0.2.1.0\n---------------\n\n*May 8, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.2.1.0>\n\n*
  \  Added `ABP` newtype wrapper to *Numeric.Backprop.Class* (re-exported from\n    *Numeric.Backprop*
  and *Numeric.Backprop.Explicit*) to give free `Backprop`\n    instances for Applicative
  actions.\n*   Added `NumBP` newtype wrapper to *Numeric.Backprop.Class* (re-exported
  in\n    the same places as `ABP`) to give free `Backprop` instances for `Num`\n
  \   instances.\n*   Added `^^?!` (unsafe access) to *Numeric.Backprop* and\n    *Numeric.Backprop.Num*.\n*
  \  `Backprop` instance for `Natural` from *Numeric.Natural*.  Should actually\n
  \   be safe, unlike its `Num` instance!\n*   `zfFunctor` and `ofFunctor` for instances
  of `Functor` for\n    *Numeric.Backprop.Explicit*.\n*   `realToFrac` and `fromIntegral`
  to *Prelude* modules\n*   `T2` and `T3` patterns for *Numeric.Backprop*, for conveniently\n
  \   constructing and deconstructing tuples.\n\nVersion 0.2.0.0\n---------------\n\n*May
  1, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.2.0.0>\n\n*   Added
  `Backprop` class in *Numeric.Backprop.Class*, which is a typeclass\n    specifically
  for \"backpropagatable\" values.  This will replace `Num`.\n*   API of *Numeric.Backprop*
  completely re-written to require values be\n    instances of `Backprop` instead
  of `Num`.  This closes some outstanding\n    issues with the reliance of `Num`,
  and allows backpropagation to work with\n    non-Num instances like variable-length
  vectors, matrices, lists, tuples,\n    etc. (including types from *accelerate*)\n*
  \  *Numeric.Backprop.Num* and *Prelude.Backprop.Num* modules added, providing\n
  \   the old interface that uses `Num` instances instead of `Backprop`\n    instances,
  for those who wish to avoid writing orphan instances when\n    working with external
  types.\n*   *Numeric.Backprop.Explicit* and *Prelude.Backprop.Explicit* modules
  added,\n    providing an interface that allows users to manually specify how zeroing,\n
  \   addition, and one-ing works on a per-value basis.  Useful for those who\n    wish
  to avoid writing orphan instances of `Backprop` for types with no\n    `Num` instances,
  or if you are mixing and matching styles.\n*   `backpropWith` variants added, allowing
  you to specify a \"final gradient\",\n    instead of assuming it to be 1.\n*   Added
  `auto`, a shorter alias for `constVar` inspired by the *ad* library.\n*   *Numeric.Backprop.Tuple*
  module removed.  I couldn't find a significant\n    reason to keep it now that `Num`
  is no longer required for backpropagation.\n\n\nVersion 0.1.5.2\n---------------\n\n*Apr
  26, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.5.2>\n\n*   Added
  `coerceVar` to *Numeric.Backprop*\n*   Added `Random` instaces for all tuple types.
  \ Same as for `Binary`, this\n    does incur a *random* and *time* dependency only
  from the tuple types.\n    Again, because these packages are a part of GHC's boot
  libraries, this\n    is hopefully not too bad.\n\nVersion 0.1.5.1\n---------------\n\n*Apr
  9, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.5.1>\n\n*   Fixed
  `NFData` instance for `T`; before, was shallow.\n*   Added `Typeable` instances
  for all tuple types, and for `BVar`.\n*   Added `Eq`, `Ord`, `Show`, etc. instances
  for `T`.\n*   Added `Binary` instances for all tuple types.  Note that this does
  incur a\n    *binary* dependency only because of the tuple types; however, this
  will\n    hopefully be not too much of an issue because *binary* is a GHC library\n
  \   anyway.\n\nVersion 0.1.5.0\n---------------\n\n*Mar 30, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.5.0>\n\n*
  \  `T` added to *Numeric.Backprop.Tuple*: basically an `HList` with a `Num`\n    instance.\n*
  \  `Eq` and `Ord` instances for `BVar`.  Is this sound?\n\n*Internal*\n\n*   Refactored
  `Monoid` instances in *Numeric.Backprop.Tuple*\n\nVersion 0.1.4.0\n---------------\n\n*Mar
  25, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.4.0>\n\n*   `isoVar`,
  `isoVar2`, `isoVar3`, and `isoVarN`: convenient aliases for\n    applying isomorphisms
  to `BVar`s.  Helpful for use with constructors and\n    deconstructors.\n*   `opIso2`
  and `opIso3` added to *Numeric.Backprop.Op*, for convenience.\n*   `T0` (Unit with
  numeric instances) added to *Numeric.Backprop.Tuple*.\n\n*Internal*\n\n*   Completely
  decoupled the internal implementation from `Num`, which showed\n    some performance
  benefits.  Mostly just to make the code slightly cleaner,\n    and to prepare for
  some day potentially decoupling the external API from\n    `Num` as well.\n\nVersion
  0.1.3.0\n---------------\n\n*Feb 12, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.3.0>\n\n*
  \  *Preulude.Backprop* module added with lifted versions of several *Prelude*\n
  \   and base functions.\n*   `liftOpX` family of operators now have a more logical
  ordering for type\n    variables.  This change breaks backwards-compatibility.\n*
  \  `opIsoN` added to export list of *Numeric.Backprop*\n*   `noGrad` and `noGrad1`
  added to *Numeric.Backprop.Op*, for functions with\n    no defined gradient.\n\n*Internal*\n\n*
  \  Completely decoupled the internal implementation from `Num`, which showed\n    some
  performance benefits.\n\nVersion 0.1.2.0\n---------------\n\n*Feb 7, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.2.0>\n\n*
  \  Added currying and uncurrying functions for tuples in\n    *Numeric.Backprop.Tuple*.\n*
  \  `opIsoN`, for isomorphisms between a tuple of values and a value.\n*   (Internal)
  AD engine now using `Any` from *ghc-prim* instead of `Some I`\n    from *type-combinators*\n\nVersion
  0.1.1.0\n---------------\n\n*Feb 6, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.1.0>\n\n*
  \  Added canonical strict tuple types with `Num` instances, in the module\n    *Numeric.Backprop.Tuple*.
  \ This is meant to be a band-aid for the problem\n    of orphan instances and potential
  mismatched tuple types.\n*   Fixed bug in `collectVar` that occurs if container
  sizes change\n\n*Internal*\n\n*   Internal tweaks to the underlying automatic differentiation
  types that\n    decouple backpropagation from `Num`, internally.  `Num` is now just
  used\n    externally as a part of the API, which might someday be made optional.\n\nVersion
  0.1.0.0\n---------------\n\n*Feb 5, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.0.0>\n\n*
  \  First non-alpha release.\n*   More or less complete redesign of library.  The
  entire API is completely\n    changed, and there is no backwards compatibility!\n
  \   *   Everything is now \"implicit\" style, and there is no more `BP` monad.\n
  \   *   Accessing items in `BVar`s is now lens-, prism-, and traversal- based,\n
  \       instead of iso- and generics-based.\n    *   `Op` is no longer monadic\n
  \   *   *Mono* modules are removed.\n    *   *Implicit* modules are removed, since
  they are the default\n    *   *Iso* module is removed, since `Iso`s no longer play
  major role in the\n        implementation of the library.\n*   Removed dependency
  on *ad* and *ad*-based ops, which had been pulling in\n    the vast majority of
  dependencies.\n*   Moved from *.cabal* file to *hpack* system.\n\nVersion 0.0.3.0\n---------------\n\n*Alpha*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.0.3.0>\n\n*
  \  Removed samples as registered executables in the cabal file, to reduce\n    dependences
  to a bare minimum.  For convenience, build script now also\n    compiles the samples
  into the local directory if *stack* is installed.\n\n*   Added experimental (unsafe)
  combinators for working with GADTs with\n    existential types, `withGADT`, to *Numeric.Backprop*
  module.\n\n*   Fixed broken links in changelog.\n\nVersion 0.0.2.0\n---------------\n\n*Alpha*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.0.2.0>\n\n*
  \  Added optimized numeric `Op`s, and re-write `Num`/`Fractional`/`Floating`\n    instances
  in terms of them.\n\n*   Removed all traces of `Summer`/`Unity` from the library,
  eliminating a\n    whole swath of \"explicit-Summer\"/\"explicit-Unity\" versions
  of functions.\n    As a consequence, the library now only works with `Num` instances.
  \ The\n    API, however, is now much more simple.\n\n*   Benchmark suite added for
  MNIST example.\n\nVersion 0.0.1.0\n---------------\n\n*Alpha*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.0.1.0>\n\n*
  \  Initial pre-release, as a request for comments.  API is in a usable form\n    and
  everything is fully documented, but there are definitely some things\n    left to
  be done. (See [README.md][readme-0.0.1.0])\n\n    [readme-0.0.1.0]: https://github.com/mstksg/backprop/tree/v0.0.1.0#readme\n\n"
basic-deps:
  reflection: -any
  type-combinators: -any
  base: ! '>=4.7 && <5'
  containers: -any
  simple-reflect: -any
  transformers: -any
  deepseq: -any
  microlens: -any
  primitive: -any
  vector: -any
all-versions:
- '0.0.1.0'
- '0.0.2.0'
- '0.0.3.0'
- '0.1.0.0'
- '0.1.1.0'
- '0.1.2.0'
- '0.1.3.0'
- '0.1.4.0'
- '0.1.5.0'
- '0.1.5.1'
- '0.1.5.2'
- '0.2.0.0'
- '0.2.1.0'
- '0.2.2.0'
- '0.2.3.0'
- '0.2.4.0'
author: Justin Le
latest: '0.2.4.0'
description-type: markdown
description: ! "[backprop][docs]\n================\n\n[![backprop on Hackage](https://img.shields.io/hackage/v/backprop.svg?maxAge=86400)](https://hackage.haskell.org/package/backprop)\n[![backprop
  on Stackage LTS 11](http://stackage.org/package/backprop/badge/lts-11)](http://stackage.org/lts-11/package/backprop)\n[![backprop
  on Stackage Nightly](http://stackage.org/package/backprop/badge/nightly)](http://stackage.org/nightly/package/backprop)\n[![Build
  Status](https://travis-ci.org/mstksg/backprop.svg?branch=master)](https://travis-ci.org/mstksg/backprop)\n\n[![Join
  the chat at https://gitter.im/haskell-backprop/Lobby](https://badges.gitter.im/haskell-backprop/Lobby.svg)](https://gitter.im/haskell-backprop/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Beerpay](https://beerpay.io/mstksg/backprop/badge.svg?style=beer-square)](https://beerpay.io/mstksg/backprop)\n\n[**Documentation
  and Walkthrough**][docs]\n\n[docs]: https://backprop.jle.im\n\nAutomatic *heterogeneous*
  back-propagation.\n\nWrite your functions to compute your result, and the library
  will automatically\ngenerate functions to compute your gradient.\n\nDiffers from
  [ad][] by offering full heterogeneity -- each intermediate step\nand the resulting
  value can have different types (matrices, vectors, scalars,\nlists, etc.).\n\n[ad]:
  http://hackage.haskell.org/package/ad\n\nUseful for applications in [differentiable
  programming][dp] and deep learning\nfor creating and training numerical models,
  especially as described in this\nblog post on [a purely functional typed approach
  to trainable models][models].\nOverall, intended for the implementation of gradient
  descent and other numeric\noptimization techniques.  Comparable to the python library
  [autograd][].\n\n[dp]: https://www.facebook.com/yann.lecun/posts/10155003011462143\n[models]:
  https://blog.jle.im/entry/purely-functional-typed-models-1.html\n[autograd]: https://github.com/HIPS/autograd\n\nCurrently
  up on [hackage][], with haddock documentation!  However, a proper\nlibrary introduction
  and usage tutorial [is available here][docs].  See also my\n[introductory blog post][blog].
  \ You can also find help or support on the\n[gitter channel][gitter].\n\n[hackage]:
  http://hackage.haskell.org/package/backprop\n[blog]: https://blog.jle.im/entry/introducing-the-backprop-library.html\n[gitter]:
  https://gitter.im/haskell-backprop/Lobby\n\nIf you want to provide *backprop* for
  users of your library, see this **[guide\nto equipping your library with backprop][library]**.\n\n[library]:
  https://backprop.jle.im/08-equipping-your-library.html\n\n\nMNIST Digit Classifier
  Example\n------------------------------\n\nMy [blog post][blog] introduces the concepts
  in this library in the context of\ntraining a handwritten digit classifier.  I recommend
  reading that first.\n\nThere are some [literate haskell examples][mnist-lhs] in
  the source, though\n([rendered as pdf here][mnist-pdf]), which can be built (if
  [stack][] is\ninstalled) using:\n\n[mnist-lhs]: https://github.com/mstksg/backprop/blob/master/samples/backprop-mnist.lhs\n[mnist-pdf]:
  https://github.com/mstksg/backprop/blob/master/renders/backprop-mnist.pdf\n[stack]:
  http://haskellstack.org/\n\n```bash\n$ ./Build.hs exe\n```\n\nThere is a follow-up
  tutorial on using the library with more advanced types,\nwith extensible neural
  networks a la [this blog post][blog], [available as\nliterate haskell][neural-lhs]
  and also [rendered as a PDF][neural-pdf].\n\n[blog]: https://blog.jle.im/entries/series/+practical-dependent-types-in-haskell.html\n[neural-lhs]:
  https://github.com/mstksg/backprop/blob/master/samples/extensible-neural.lhs\n[neural-pdf]:
  https://github.com/mstksg/backprop/blob/master/renders/extensible-neural.pdf\n\nBrief
  example\n-------------\n\n(This is a really brief version of [the documentation
  walkthrough][docs] and my\n[blog post][blog])\n\nThe quick example below describes
  the running of a neural network with one\nhidden layer to calculate its squared
  error with respect to target `targ`,\nwhich is parameterized by two weight matrices
  and two bias vectors.\nVector/matrix types are from the *hmatrix* package.\n\nLet's
  make a data type to store our parameters, with convenient accessors using\n*[lens][]*:\n\n[lens]:
  http://hackage.haskell.org/package/lens\n\n```haskell\nimport Numeric.LinearAlgebra.Static.Backprop\n\ndata
  Network = Net { _weight1 :: L 20 100\n                   , _bias1   :: R 20\n                   ,
  _weight2 :: L  5  20\n                   , _bias2   :: R  5\n                   }\n\nmakeLenses
  ''Network\n```\n\n(`R n` is an n-length vector, `L m n` is an m-by-n matrix, etc.,
  `#>` is\nmatrix-vector multiplication)\n\n\"Running\" a network on an input vector
  might look like this:\n\n```haskell\nrunNet net x = z\n  where\n    y = logistic
  $ (net ^^. weight1) #> x + (net ^^. bias1)\n    z = logistic $ (net ^^. weight2)
  #> y + (net ^^. bias2)\n\nlogistic :: Floating a => a -> a\nlogistic x = 1 / (1
  + exp (-x))\n```\n\nAnd that's it!  `neuralNet` is now backpropagatable!\n\nWe can
  \"run\" it using `evalBP`:\n\n```haskell\nevalBP2 runNet :: Network -> R 100 ->
  R 5\n```\n\nIf we write a function to compute errors:\n\n```haskell\nsquaredError
  target output = error `dot` error\n  where\n    error = target - output\n```\n\nwe
  can \"test\" our networks:\n\n```haskell\nnetError target input net = squaredError
  (auto target)\n                                         (runNet net (auto input))\n```\n\nThis
  can be run, again:\n\n```haskell\nevalBP (netError myTarget myVector) :: Network
  -> Network\n```\n\nNow, we just wrote a *normal function to compute the error of
  our network*.\nWith the *backprop* library, we now also have a way to *compute the
  gradient*,\nas well!\n\n```haskell\ngradBP (netError myTarget myVector) :: Network
  -> Network\n```\n\nNow, we can perform gradient descent!\n\n```haskell\ngradDescent\n
  \   :: R 100\n    -> R 5\n    -> Network\n    -> Network\ngradDescent x targ n0
  = n0 - 0.1 * gradient\n  where\n    gradient = gradBP (netError targ x) n0\n```\n\nTa
  dah!  We were able to compute the gradient of our error function, just by\nonly
  saying how to compute *the error itself*.\n\nFor a more fleshed out example, see
  [the documentaiton][docs], my [blog\npost][blog] and the [MNIST tutorial][mnist-lhs]
  (also [rendered as a\npdf][mnist-pdf])\n\nBenchmarks and Performance\n--------------------------\n\nHere
  are some basic benchmarks comparing the library's automatic\ndifferentiation process
  to \"manual\" differentiation by hand.  When using the\n[MNIST tutorial][bench]
  as an example:\n\n[bench]: https://github.com/mstksg/backprop/blob/master/bench/bench.hs\n\n![benchmarks](https://i.imgur.com/7L5NV4P.png)\n\nHere
  we compare:\n\n1.  \"Manual\" differentiation of a 784 x 300 x 100 x 10 fully-connected\n
  \   feed-forward ANN.\n2.  Automatic differentiation using *backprop* and the lens-based
  accessor\n    interface\n3.  Automatic differentiation using *backprop* and the
  \"higher-kinded\n    data\"-based pattern matching interface\n4.  A hybrid approach
  that manually provides gradients for individual layers\n    but uses automatic differentiation
  for chaining the layers together.\n\nWe can see that simply *running* the network
  and functions (using `evalBP`)\nincurs virtually zero overhead.  This means that
  library authors could actually\nexport *only* backprop-lifted functions, and users
  would be able to use them\nwithout losing any performance.\n\nAs for computing gradients,
  there exists some associated overhead, from three\nmain sources.  Of these, the
  building of the computational graph and the\nWengert Tape wind up being negligible.
  \ For more information, see [a detailed\nlook at performance, overhead, and optimization
  techniques][performance] in the\ndocumentation.\n\n[performance]: https://backprop.jle.im/07-performance.html\n\nNote
  that the manual and hybrid modes almost overlap in the range of their\nrandom variances.\n\nComparisons\n-----------\n\n*backprop*
  can be compared and contrasted to many other similar libraries with\nsome overlap:\n\n1.
  \ The *[ad][]* library (and variants like *[diffhask][]*) support automatic\n    differentiation,
  but only for *homogeneous*/*monomorphic* situations.  All\n    values in a computation
  must be of the same type --- so, your computation\n    might be the manipulation
  of `Double`s through a `Double -> Double`\n    function.\n\n    *backprop* allows
  you to mix matrices, vectors, doubles, integers, and even\n    key-value maps as
  a part of your computation, and they will all be\n    backpropagated properly with
  the help of the `Backprop` typeclass.\n\n2.  The *[autograd][]* library is a very
  close equivalent to *backprop*,\n    implemented in Python for Python applications.
  \ The difference between\n    *backprop* and *autograd* is mostly the difference
  between Haskell and\n    Python --- static types with type inference, purity, etc.\n\n3.
  \ There is a link between *backprop* and deep learning/neural network\n    libraries
  like *[tensorflow][]*, *[caffe][]*, and *[theano][]*, which all\n    all support
  some form of heterogeneous automatic differentiation.  Haskell\n    libraries doing
  similar things include *[grenade][]*.\n\n    These are all frameworks for working
  with neural networks or other\n    gradient-based optimizations --- they include
  things like built-in\n    optimizers, methods to automate training data, built-in
  models to use out\n    of the box.  *backprop* could be used as a *part* of such
  a framework, like\n    I described in my [A Purely Functional Typed Approach to
  Trainable\n    Models][models] blog series; however, the *backprop* library itself
  does\n    not provide any built in models or optimizers or automated data processing\n
  \   pipelines.\n\n[diffhask]: https://hackage.haskell.org/package/diffhask\n[tensorflow]:
  https://www.tensorflow.org/\n[caffe]: http://caffe.berkeleyvision.org/\n[theano]:
  http://www.deeplearning.net/software/theano/\n[grenade]: http://hackage.haskell.org/package/grenade\n\nSee
  [documentation][comparisons] for a more detailed look.\n\n[comparisons]: https://backprop.jle.im/09-comparisons.html\n\nTodo\n----\n\n1.
  \ Benchmark against competing back-propagation libraries like *ad*, and\n    auto-differentiating
  tensor libraries like *[grenade][]*\n\n    [grenade]: https://github.com/HuwCampbell/grenade\n\n2.
  \ Write tests!\n\n3.  Explore opportunities for parallelization.  There are some
  naive ways of\n    directly parallelizing right now, but potential overhead should
  be\n    investigated.\n\n4.  Some open questions:\n\n    a.  Is it possible to support
  constructors with existential types?\n\n    b.  How to support \"monadic\" operations
  that depend on results of previous\n        operations? (`ApBP` already exists for
  situations that don't)\n\n    c.  What needs to be done to allow us to automatically
  do second,\n        third-order differentiation, as well?  This might be useful
  for certain\n        ODE solvers which rely on second order gradients and hessians.\n"
license-name: BSD3
